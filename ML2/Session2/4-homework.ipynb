{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"oThNdpgKPpLp"},"source":["For the homework, we re-use a great dataset to practice your skills in Keras. We are going to compare classification and regression approaches. As you remember from the previous homework, our wine dataset contains 10 rating levels. Hence, we can also solve this problem as classification task (instead of a regression task). You can find the relevant files under the `wine` subdirectory in the session's folder, but the code below also reloads them for you. This dataset has been used in the following publication:\n","\n","> P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. 'Modeling wine preferences by data mining from physicochemical properties.' *Decision Support Systems* 47(4):547-553.\n","\n","You can find two datasets (encoded as csv-files): one for red wines, and one for white wines. You can load them using `pandas`:"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd gdrive/MyDrive/ML2022/session-2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XvM2AeRxfxzj","executionInfo":{"status":"ok","timestamp":1670250695454,"user_tz":-60,"elapsed":2138,"user":{"displayName":"Nicolae Banari","userId":"02726718402918962079"}},"outputId":"fbb24309-d70e-45a8-bfca-3dc48b79a254"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","[Errno 2] No such file or directory: 'gdrive/MyDrive/ML2022/session-2'\n","/content/gdrive/MyDrive/ML2022/session-2\n"]}]},{"cell_type":"code","metadata":{"id":"FlbpriUjfj8J"},"source":["import utils\n","import numpy as np\n","import torch\n","from torch import nn\n","\n","\n","# to get reproducible results:\n","torch.manual_seed(1234)\n","np.random.seed(1234)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VcO93GhDQYHW"},"source":["## Preprocessing (recap)\n","The longer version of this part can be found in the previous homework. Load the dataset for the white wines (it's also included in the session's folder):"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kDh1VWhPPqqQ","executionInfo":{"status":"ok","timestamp":1670250697876,"user_tz":-60,"elapsed":776,"user":{"displayName":"Nicolae Banari","userId":"02726718402918962079"}},"outputId":"be8745b9-8126-49a8-b866-1641ffbb7c8b"},"source":["!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-12-05 14:31:36--  https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n","Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n","Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 264426 (258K) [application/x-httpd-php]\n","Saving to: ‘winequality-white.csv.1’\n","\n","winequality-white.c 100%[===================>] 258.23K  1005KB/s    in 0.3s    \n","\n","2022-12-05 14:31:37 (1005 KB/s) - ‘winequality-white.csv.1’ saved [264426/264426]\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"J76mfmcyQqy9","executionInfo":{"status":"ok","timestamp":1670250698113,"user_tz":-60,"elapsed":3,"user":{"displayName":"Nicolae Banari","userId":"02726718402918962079"}},"outputId":"7f0bf659-7bdb-4a6a-efa5-3ddfcfbc66b1"},"source":["import pandas as pd\n","white = pd.read_csv('winequality-white.csv', sep=';')\n","white.sample(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n","1431            6.1              0.22         0.49             1.5      0.051   \n","445             7.1              0.32         0.32            11.0      0.038   \n","2816            7.2              0.17         0.41             1.6      0.052   \n","4049            6.8              0.16         0.36             1.3      0.034   \n","4779            6.0              0.59         0.00             0.8      0.037   \n","142             7.9              0.21         0.40             1.2      0.039   \n","2703            6.5              0.23         0.36            16.3      0.038   \n","3252            7.1              0.26         0.37             5.5      0.025   \n","4282            5.7              0.26         0.24            17.8      0.059   \n","46              6.2              0.45         0.26             4.4      0.063   \n","\n","      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n","1431                 18.0                  87.0  0.99280  3.30       0.46   \n","445                  16.0                  66.0  0.99370  3.24       0.40   \n","2816                 24.0                 126.0  0.99228  3.19       0.49   \n","4049                 32.0                  98.0  0.99058  3.02       0.58   \n","4779                 30.0                  95.0  0.99032  3.10       0.40   \n","142                  38.0                 107.0  0.99200  3.21       0.54   \n","2703                 43.0                 133.0  0.99924  3.26       0.41   \n","3252                 31.0                 105.0  0.99082  3.06       0.33   \n","4282                 23.0                 124.0  0.99773  3.30       0.50   \n","46                   63.0                 206.0  0.99400  3.27       0.52   \n","\n","      alcohol  quality  \n","1431      9.6        5  \n","445      11.5        3  \n","2816     10.8        5  \n","4049     11.3        6  \n","4779     10.9        4  \n","142      10.8        6  \n","2703      8.8        5  \n","3252     12.6        8  \n","4282     10.1        5  \n","46        9.8        4  "],"text/html":["\n","  <div id=\"df-24a2eadd-e222-43ee-8d88-be84a0226e17\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fixed acidity</th>\n","      <th>volatile acidity</th>\n","      <th>citric acid</th>\n","      <th>residual sugar</th>\n","      <th>chlorides</th>\n","      <th>free sulfur dioxide</th>\n","      <th>total sulfur dioxide</th>\n","      <th>density</th>\n","      <th>pH</th>\n","      <th>sulphates</th>\n","      <th>alcohol</th>\n","      <th>quality</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1431</th>\n","      <td>6.1</td>\n","      <td>0.22</td>\n","      <td>0.49</td>\n","      <td>1.5</td>\n","      <td>0.051</td>\n","      <td>18.0</td>\n","      <td>87.0</td>\n","      <td>0.99280</td>\n","      <td>3.30</td>\n","      <td>0.46</td>\n","      <td>9.6</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>445</th>\n","      <td>7.1</td>\n","      <td>0.32</td>\n","      <td>0.32</td>\n","      <td>11.0</td>\n","      <td>0.038</td>\n","      <td>16.0</td>\n","      <td>66.0</td>\n","      <td>0.99370</td>\n","      <td>3.24</td>\n","      <td>0.40</td>\n","      <td>11.5</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2816</th>\n","      <td>7.2</td>\n","      <td>0.17</td>\n","      <td>0.41</td>\n","      <td>1.6</td>\n","      <td>0.052</td>\n","      <td>24.0</td>\n","      <td>126.0</td>\n","      <td>0.99228</td>\n","      <td>3.19</td>\n","      <td>0.49</td>\n","      <td>10.8</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>4049</th>\n","      <td>6.8</td>\n","      <td>0.16</td>\n","      <td>0.36</td>\n","      <td>1.3</td>\n","      <td>0.034</td>\n","      <td>32.0</td>\n","      <td>98.0</td>\n","      <td>0.99058</td>\n","      <td>3.02</td>\n","      <td>0.58</td>\n","      <td>11.3</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4779</th>\n","      <td>6.0</td>\n","      <td>0.59</td>\n","      <td>0.00</td>\n","      <td>0.8</td>\n","      <td>0.037</td>\n","      <td>30.0</td>\n","      <td>95.0</td>\n","      <td>0.99032</td>\n","      <td>3.10</td>\n","      <td>0.40</td>\n","      <td>10.9</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>142</th>\n","      <td>7.9</td>\n","      <td>0.21</td>\n","      <td>0.40</td>\n","      <td>1.2</td>\n","      <td>0.039</td>\n","      <td>38.0</td>\n","      <td>107.0</td>\n","      <td>0.99200</td>\n","      <td>3.21</td>\n","      <td>0.54</td>\n","      <td>10.8</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2703</th>\n","      <td>6.5</td>\n","      <td>0.23</td>\n","      <td>0.36</td>\n","      <td>16.3</td>\n","      <td>0.038</td>\n","      <td>43.0</td>\n","      <td>133.0</td>\n","      <td>0.99924</td>\n","      <td>3.26</td>\n","      <td>0.41</td>\n","      <td>8.8</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>3252</th>\n","      <td>7.1</td>\n","      <td>0.26</td>\n","      <td>0.37</td>\n","      <td>5.5</td>\n","      <td>0.025</td>\n","      <td>31.0</td>\n","      <td>105.0</td>\n","      <td>0.99082</td>\n","      <td>3.06</td>\n","      <td>0.33</td>\n","      <td>12.6</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>4282</th>\n","      <td>5.7</td>\n","      <td>0.26</td>\n","      <td>0.24</td>\n","      <td>17.8</td>\n","      <td>0.059</td>\n","      <td>23.0</td>\n","      <td>124.0</td>\n","      <td>0.99773</td>\n","      <td>3.30</td>\n","      <td>0.50</td>\n","      <td>10.1</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>6.2</td>\n","      <td>0.45</td>\n","      <td>0.26</td>\n","      <td>4.4</td>\n","      <td>0.063</td>\n","      <td>63.0</td>\n","      <td>206.0</td>\n","      <td>0.99400</td>\n","      <td>3.27</td>\n","      <td>0.52</td>\n","      <td>9.8</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24a2eadd-e222-43ee-8d88-be84a0226e17')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-24a2eadd-e222-43ee-8d88-be84a0226e17 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-24a2eadd-e222-43ee-8d88-be84a0226e17');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":60}]},{"cell_type":"markdown","metadata":{"id":"O647JjUaRqSw"},"source":["This dataset records, for a large number of (Portugese) wines, a number of objective \"physiochemical\" properties, such as various kinds of acidity or the level of chlorides. In the very last column (\"quality\"), it contains a subjective rating for that wine, as an integer  score, ranging from 1 (\"very bad\") to 10 (\"excellent\"). This score is a median rating given by at least 3 evaluations made by wine experts."]},{"cell_type":"markdown","metadata":{"id":"1htk5z7OSB2s"},"source":["We'll extract the \"quality\" column as our $y$ (target value) and features as $X$:"]},{"cell_type":"code","metadata":{"id":"riUTdhPfRqvX"},"source":["y_label = white['quality'].values\n","white = white.drop(['quality'], axis=1)\n","X = white.values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p3EEzmqxieKD"},"source":["Note that not all ratings are in fact present in the data -- apparently there were no really bad wines! We ignore this below, for teaching purposes, but normally you'd have to remove the empty classes of course."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sQf5NTMgmOUt","executionInfo":{"status":"ok","timestamp":1670250701457,"user_tz":-60,"elapsed":196,"user":{"displayName":"Nicolae Banari","userId":"02726718402918962079"}},"outputId":"c0dbef7f-03ff-4782-f727-505cc6b6be9a"},"source":["np.unique(y_label)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([3, 4, 5, 6, 7, 8, 9])"]},"metadata":{},"execution_count":62}]},{"cell_type":"markdown","metadata":{"id":"t3MNUHUoSm4m"},"source":["**Task 1.** Divide the available data into a train set (60 %), dev set (20 %) and test set (20 %) using the train_test_split function from Scikit-learn (2 times). It's important to **stratify** these splits in terms of ratings, in order to make sure that we have a similar distribution of ratings in train and test."]},{"cell_type":"code","metadata":{"id":"V09KMn9XSXBA"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MIInE9syUKgG"},"source":["Note that the stratify argument is crucial. Verify the shapes:"]},{"cell_type":"code","metadata":{"id":"21NvJqBJUBLC"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mvDe1b3ys4lQ"},"source":["**Task 2** There's one final bit we need to take care of and that is the normalization of our data: if you inspect the feature values in the dataframe above, you'll notice that the features cover very different ranges. To account for that it's best to normalize our data. Again: use `sklearn` to do it!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KREnLsb9UYHq","executionInfo":{"status":"ok","timestamp":1670250704409,"user_tz":-60,"elapsed":2,"user":{"displayName":"Nicolae Banari","userId":"02726718402918962079"}},"outputId":"e5689c30-c058-4f54-dba6-d85b28f7d161"},"source":["from sklearn.preprocessing import StandardScaler\n","\n","x_train =\n","x_dev =\n","x_test =\n","\n","\n","\n","\n","\n","y_train_sc =\n","y_dev_sc =\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(2938, 11)\n","(980, 11)\n"]}]},{"cell_type":"markdown","metadata":{"id":"p7eNb_MPtIJm"},"source":["# Regression"]},{"cell_type":"code","source":["def np2set_reg(x, y, shuffle):\n","  x = torch.tensor(x, dtype=torch.float)\n","  y =  torch.tensor(y, dtype=torch.float)\n","\n","  dataset = torch.utils.data.TensorDataset(x, y)\n","  iterator = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=shuffle)\n","  return iterator\n","\n","\n","\n","r_train_iter =  np2set_reg(x_train, y_train_sc, shuffle=True)\n","r_dev_iter =  np2set_reg(x_dev, y_dev_sc, shuffle=False)\n","r_test_iter = np2set_reg(x_test, y_test, shuffle=False)"],"metadata":{"id":"tvCRYaaGj9Et"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XIVEmwys1wdt"},"source":["**Task 3.** Base yourself on the code from the notebook for the previous homework and train a linear regression that aims to predict a wine appreciation using all columns. Make sure that your results are **reproducible** by correctly 'seeding'. Use the SGD-optimizer with a learning rate of \"0.01\" and train for 200 epochs with early stopping (with a \"patience\" of 5). Thus, early stopping should halt the training process if the validation loss does not improve for 5 (consecutive) epochs. Use MAE, MSE and accuracy to evaluate the results on the test set."]},{"cell_type":"code","metadata":{"id":"fvO_76hLUPl4"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import mean_squared_error as mse\n","from sklearn.metrics import mean_absolute_error as mae\n","from sklearn.metrics import accuracy_score\n","\n","\n","def accuracy_from_floats(y_true, y_pred):\n","  y_pred = np.around(y_pred).astype(int)\n","  return accuracy_score(y_true, y_pred)\n","\n"],"metadata":{"id":"P0dJCRUzl9xo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pDLfQ5hx3bb7"},"source":["**Task 4.** Add an additional, \"hidden\" dense layer with the ReLU activation and 30 input/output neurons. Is the result better?"]},{"cell_type":"code","metadata":{"id":"L6SsPYpBuFcM"},"source":["\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SBoVfs45nQng"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RFPwnZcM49NS"},"source":["**Task 5.** Add an additional (*second*) hidden layer with the ReLU activation and input/output 30 neurons. Is the result better?"]},{"cell_type":"code","metadata":{"id":"_PzOKXALzyT-"},"source":["\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T0TvmNxnz6u8"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ADUcSjeYt4kE"},"source":["# Classification\n","\n","**Task 6.** Let's approach this dataset with a logistic regression now. This time, you will have to predict the rating levels as **classes** (10), instead of treating the output as a scalar. Make sure that your results are **reproducible** by correctly 'seeding'. Use the SGD-optimizer with a learning rate of \"0.01\" and train for 200 epochs with early stopping with patience of 5 (and restore the best weights). Early stopping should interrupt the training if the loss on the validation data  does not improve for 5 epochs. Use accuracy to evaluate the intermediary results, during training."]},{"cell_type":"code","source":["def np2iter_class(x, y, shuffle=True):\n","  x = torch.tensor(x, dtype=torch.float)\n","  y = torch.tensor(y, dtype=torch.long)\n","\n","  ds = torch.utils.data.TensorDataset(x, y)\n","  return torch.utils.data.DataLoader(ds, batch_size=64, shuffle=shuffle)\n","\n","c_train_iter = np2iter_class(x_train, y_train, shuffle=True)\n","c_dev_iter =  np2iter_class(x_dev, y_dev, shuffle=False)\n","c_test_iter =  np2iter_class(x_test, y_test, shuffle=False)\n"],"metadata":{"id":"SeYV2oCNojix"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-SgsikNJbs1Q"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uu4cG23cqMPx"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xAuaediCd9nJ"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2HboX34m8641"},"source":["**Task 7.** Add an additional dense layer with the Relu activation and inputoutput 30 neurons. Is the result better?"]},{"cell_type":"code","metadata":{"id":"Bs31S3JZj-UL"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ikh_tmzAkT80"},"source":["\n","# class_predictions = utils.test(classification_model, c_test_iter)\n","# class_predictions = np.argmax(class_predictions, axis=1)\n","\n","# print(\"Accuracy: \",  accuracy_score(y_test, class_predictions),\n","#       \", MSE: \",mse(y_test, reg_predictions),\n","#       \", MAE\", mae(y_test, reg_predictions))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pVacqcxW9ClC"},"source":["**Task 8.** Add an additional dense layer (a second hidden layer) with the ReLU activation and input/output 30 neurons. Is the result better?"]},{"cell_type":"code","metadata":{"id":"XaSkyNu1mtIc"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O-C1qzxF7tIQ"},"source":["\n","# class_predictions = utils.test(classification_model, c_test_iter)\n","# class_predictions = np.argmax(class_predictions, axis=1)\n","\n","# print(\"Accuracy: \",  accuracy_score(y_test, class_predictions),\n","#       \", MSE: \",mse(y_test, reg_predictions),\n","#       \", MAE\", mae(y_test, reg_predictions))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"967Kh67vzi8K"},"source":["**Task 9** Write a short report about your findings and answer the following questions:\n","\n","\n","1. Is the wine problem a classification task (or a regression task)?\n","2. What is the most suitable metric for our task (MSE, MAE, accuracy)?\n","3. Which model is the best performer overall?\n","4. Is 'deeper' always better?\n","5. Add other your findings you want to share.\n","\n"]},{"cell_type":"code","metadata":{"id":"fTnvBW_MlSis"},"source":[],"execution_count":null,"outputs":[]}]}