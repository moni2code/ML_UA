{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oThNdpgKPpLp"
      },
      "source": [
        "For the homework, we re-use a great dataset to practice your skills in Keras. We are going to compare classification and regression approaches. As you remember from the previous homework, our wine dataset contains 10 rating levels. Hence, we can also solve this problem as classification task (instead of a regression task). You can find the relevant files under the `wine` subdirectory in the session's folder, but the code below also reloads them for you. This dataset has been used in the following publication:\n",
        "\n",
        "> P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. 'Modeling wine preferences by data mining from physicochemical properties.' *Decision Support Systems* 47(4):547-553.\n",
        "\n",
        "You can find two datasets (encoded as csv-files): one for red wines, and one for white wines. You can load them using `pandas`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# %cd gdrive/MyDrive/my_ml_2023/session-7"
      ],
      "metadata": {
        "id": "XvM2AeRxfxzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlbpriUjfj8J"
      },
      "source": [
        "import utils\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "# to get reproducible results:\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcO93GhDQYHW"
      },
      "source": [
        "## Preprocessing (recap)\n",
        "The longer version of this part can be found in the previous homework. Load the dataset for the white wines (it's also included in the session's folder):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDh1VWhPPqqQ",
        "outputId": "f07c9979-68dc-471a-ab12-a8e47c15fdf2"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-10 12:20:59--  https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘winequality-white.csv’\n",
            "\n",
            "winequality-white.c     [   <=>              ] 258.23K   401KB/s    in 0.6s    \n",
            "\n",
            "2024-12-10 12:21:01 (401 KB/s) - ‘winequality-white.csv’ saved [264426]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "J76mfmcyQqy9",
        "outputId": "e10459e4-25a9-4300-bc15-e4ace14701fd"
      },
      "source": [
        "import pandas as pd\n",
        "white = pd.read_csv('winequality-white.csv', sep=';')\n",
        "white.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
              "1431            6.1              0.22         0.49             1.5      0.051   \n",
              "445             7.1              0.32         0.32            11.0      0.038   \n",
              "2816            7.2              0.17         0.41             1.6      0.052   \n",
              "4049            6.8              0.16         0.36             1.3      0.034   \n",
              "4779            6.0              0.59         0.00             0.8      0.037   \n",
              "142             7.9              0.21         0.40             1.2      0.039   \n",
              "2703            6.5              0.23         0.36            16.3      0.038   \n",
              "3252            7.1              0.26         0.37             5.5      0.025   \n",
              "4282            5.7              0.26         0.24            17.8      0.059   \n",
              "46              6.2              0.45         0.26             4.4      0.063   \n",
              "\n",
              "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
              "1431                 18.0                  87.0  0.99280  3.30       0.46   \n",
              "445                  16.0                  66.0  0.99370  3.24       0.40   \n",
              "2816                 24.0                 126.0  0.99228  3.19       0.49   \n",
              "4049                 32.0                  98.0  0.99058  3.02       0.58   \n",
              "4779                 30.0                  95.0  0.99032  3.10       0.40   \n",
              "142                  38.0                 107.0  0.99200  3.21       0.54   \n",
              "2703                 43.0                 133.0  0.99924  3.26       0.41   \n",
              "3252                 31.0                 105.0  0.99082  3.06       0.33   \n",
              "4282                 23.0                 124.0  0.99773  3.30       0.50   \n",
              "46                   63.0                 206.0  0.99400  3.27       0.52   \n",
              "\n",
              "      alcohol  quality  \n",
              "1431      9.6        5  \n",
              "445      11.5        3  \n",
              "2816     10.8        5  \n",
              "4049     11.3        6  \n",
              "4779     10.9        4  \n",
              "142      10.8        6  \n",
              "2703      8.8        5  \n",
              "3252     12.6        8  \n",
              "4282     10.1        5  \n",
              "46        9.8        4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d101a09-5c78-43c3-b366-e5e3cf406093\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1431</th>\n",
              "      <td>6.1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.49</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.051</td>\n",
              "      <td>18.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.99280</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.46</td>\n",
              "      <td>9.6</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>7.1</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.32</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.038</td>\n",
              "      <td>16.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>0.99370</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.40</td>\n",
              "      <td>11.5</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2816</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.41</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.052</td>\n",
              "      <td>24.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>0.99228</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.49</td>\n",
              "      <td>10.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4049</th>\n",
              "      <td>6.8</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.36</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.034</td>\n",
              "      <td>32.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.99058</td>\n",
              "      <td>3.02</td>\n",
              "      <td>0.58</td>\n",
              "      <td>11.3</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4779</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.037</td>\n",
              "      <td>30.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>0.99032</td>\n",
              "      <td>3.10</td>\n",
              "      <td>0.40</td>\n",
              "      <td>10.9</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>7.9</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.40</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.039</td>\n",
              "      <td>38.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>0.99200</td>\n",
              "      <td>3.21</td>\n",
              "      <td>0.54</td>\n",
              "      <td>10.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2703</th>\n",
              "      <td>6.5</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.36</td>\n",
              "      <td>16.3</td>\n",
              "      <td>0.038</td>\n",
              "      <td>43.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>0.99924</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.41</td>\n",
              "      <td>8.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3252</th>\n",
              "      <td>7.1</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.37</td>\n",
              "      <td>5.5</td>\n",
              "      <td>0.025</td>\n",
              "      <td>31.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>0.99082</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.33</td>\n",
              "      <td>12.6</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4282</th>\n",
              "      <td>5.7</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.24</td>\n",
              "      <td>17.8</td>\n",
              "      <td>0.059</td>\n",
              "      <td>23.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>0.99773</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.50</td>\n",
              "      <td>10.1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>6.2</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.26</td>\n",
              "      <td>4.4</td>\n",
              "      <td>0.063</td>\n",
              "      <td>63.0</td>\n",
              "      <td>206.0</td>\n",
              "      <td>0.99400</td>\n",
              "      <td>3.27</td>\n",
              "      <td>0.52</td>\n",
              "      <td>9.8</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d101a09-5c78-43c3-b366-e5e3cf406093')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2d101a09-5c78-43c3-b366-e5e3cf406093 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2d101a09-5c78-43c3-b366-e5e3cf406093');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b2e2d37f-5e7f-412f-bf59-a04f5069f03d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b2e2d37f-5e7f-412f-bf59-a04f5069f03d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b2e2d37f-5e7f-412f-bf59-a04f5069f03d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"white\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"fixed acidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6785605679999719,\n        \"min\": 5.7,\n        \"max\": 7.9,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          5.7,\n          7.1,\n          7.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"volatile acidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13515834335244634,\n        \"min\": 0.16,\n        \"max\": 0.59,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.26,\n          0.32,\n          0.21\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"citric acid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1339527279801846,\n        \"min\": 0.0,\n        \"max\": 0.49,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.24,\n          0.32,\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"residual sugar\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.538127492723824,\n        \"min\": 0.8,\n        \"max\": 17.8,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          17.8,\n          11.0,\n          1.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chlorides\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.012038826077875422,\n        \"min\": 0.025,\n        \"max\": 0.063,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.059,\n          0.038,\n          0.039\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"free sulfur dioxide\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13.822686666009133,\n        \"min\": 16.0,\n        \"max\": 63.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          23.0,\n          16.0,\n          38.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total sulfur dioxide\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37.777271238434125,\n        \"min\": 66.0,\n        \"max\": 206.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          124.0,\n          66.0,\n          107.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"density\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0029990000185246943,\n        \"min\": 0.99032,\n        \"max\": 0.99924,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.99773,\n          0.9937,\n          0.992\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pH\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10113247637518703,\n        \"min\": 3.02,\n        \"max\": 3.3,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          3.06,\n          3.24,\n          3.21\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sulphates\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07703534542302283,\n        \"min\": 0.33,\n        \"max\": 0.58,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.5,\n          0.4,\n          0.41\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"alcohol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0850499220466001,\n        \"min\": 8.8,\n        \"max\": 12.6,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          10.1,\n          11.5,\n          8.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quality\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 3,\n        \"max\": 8,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3,\n          8,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O647JjUaRqSw"
      },
      "source": [
        "This dataset records, for a large number of (Portugese) wines, a number of objective \"physiochemical\" properties, such as various kinds of acidity or the level of chlorides. In the very last column (\"quality\"), it contains a subjective rating for that wine, as an integer  score, ranging from 1 (\"very bad\") to 10 (\"excellent\"). This score is a median rating given by at least 3 evaluations made by wine experts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1htk5z7OSB2s"
      },
      "source": [
        "We'll extract the \"quality\" column as our $y$ (target value) and features as $X$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riUTdhPfRqvX"
      },
      "source": [
        "y_label = white['quality'].values\n",
        "white = white.drop(['quality'], axis=1)\n",
        "X = white.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3EEzmqxieKD"
      },
      "source": [
        "Note that not all ratings are in fact present in the data -- apparently there were no really bad wines! We ignore this below, for teaching purposes, but normally you'd have to remove the empty classes of course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQf5NTMgmOUt",
        "outputId": "d7048265-3e21-485e-e7ed-1ce04a97789e"
      },
      "source": [
        "np.unique(y_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3MNUHUoSm4m"
      },
      "source": [
        "**Task 1.** Divide the available data into a train set (60 %), dev set (20 %) and test set (20 %) using the train_test_split function from Scikit-learn (2 times). It's important to **stratify** these splits in terms of ratings, in order to make sure that we have a similar distribution of ratings in train and test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V09KMn9XSXBA"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_dev_test, y_train, y_dev_test = train_test_split(X, y_label, test_size=0.4,\n",
        "                                                    random_state=1234, stratify=y_label)\n",
        "\n",
        "x_dev, x_test, y_dev, y_test = train_test_split(x_dev_test, y_dev_test, test_size=0.5,\n",
        "                                                    random_state=1234, stratify=y_dev_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIInE9syUKgG"
      },
      "source": [
        "Note that the stratify argument is crucial. Verify the shapes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21NvJqBJUBLC",
        "outputId": "01767c66-c006-4142-cb18-008ce511877b"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_dev.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_dev.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2938, 11)\n",
            "(980, 11)\n",
            "(980, 11)\n",
            "(2938,)\n",
            "(980,)\n",
            "(980,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvDe1b3ys4lQ"
      },
      "source": [
        "**Task 2** There's one final bit we need to take care of and that is the normalization of our data: if you inspect the feature values in the dataframe above, you'll notice that the features cover very different ranges. To account for that it's best to normalize our data. Again: use `sklearn` to do it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KREnLsb9UYHq",
        "outputId": "007d4a49-cb0f-4371-92a9-237665fe7468"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# conventional standardization: remove mean and divide by std\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_dev = scaler.transform(x_dev)\n",
        "x_test = scaler.transform(x_test)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "y_scaler = StandardScaler()\n",
        "\n",
        "# we add a new dimension to y's (it had just one) to fulfil requirements of StandardScaler (2 dimensions)\n",
        "\n",
        "y_train_sc = np.expand_dims(y_train, axis=1)\n",
        "y_dev_sc = np.expand_dims(y_dev, axis=1)\n",
        "\n",
        "y_train_sc = y_scaler.fit_transform(y_train_sc)\n",
        "y_dev_sc = y_scaler.transform(y_dev_sc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2938, 11)\n",
            "(980, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7eNb_MPtIJm"
      },
      "source": [
        "# Regression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def np2set_reg(x, y, shuffle):\n",
        "  x = torch.tensor(x, dtype=torch.float)\n",
        "  y =  torch.tensor(y, dtype=torch.float)\n",
        "\n",
        "  dataset = torch.utils.data.TensorDataset(x, y)\n",
        "  iterator = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=shuffle)\n",
        "  return iterator\n",
        "\n",
        "\n",
        "\n",
        "r_train_iter =  np2set_reg(x_train, y_train_sc, shuffle=True)\n",
        "r_dev_iter =  np2set_reg(x_dev, y_dev_sc, shuffle=False)\n",
        "r_test_iter = np2set_reg(x_test, y_test, shuffle=False)"
      ],
      "metadata": {
        "id": "tvCRYaaGj9Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIVEmwys1wdt"
      },
      "source": [
        "**Task 3.** Base yourself on the code from the notebook for the previous homework and train a linear regression that aims to predict a wine appreciation using all columns. Make sure that your results are **reproducible** by correctly 'seeding'. Use the SGD-optimizer with a learning rate of \"0.01\" and train for 200 epochs with early stopping (with a \"patience\" of 5). Thus, early stopping should halt the training process if the validation loss does not improve for 5 (consecutive) epochs. Use MAE, MSE and accuracy to evaluate the results on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvO_76hLUPl4",
        "outputId": "682d44c7-daa6-4a94-bc36-8903535129e0"
      },
      "source": [
        "class RegModel1(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super(RegModel1, self).__init__()\n",
        "    self.layer = nn.Linear(in_features=11, out_features=1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "reg_model = RegModel1(in_features=11, out_features=1)\n",
        "reg_model = reg_model.cuda()\n",
        "\n",
        "history = utils.train(model=reg_model,\n",
        "              loss=nn.MSELoss(),\n",
        "              val_metrics={\"mse\": nn.MSELoss()},\n",
        "              optimizer=torch.optim.SGD(reg_model.parameters(), lr=0.01),\n",
        "              train_ds=r_train_iter,\n",
        "              dev_ds=r_dev_iter,\n",
        "              num_epochs=200,\n",
        "              early_stopper=utils.EarlyStopper(metric_name=\"mse\", patience=5))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========\n",
            "epoch 1 train loss: 0.8671 val_mse: 0.8035\n",
            "tensor(0.8035) None\n",
            "=========\n",
            "epoch 2 train loss: 0.7675 val_mse: 0.7610\n",
            "tensor(0.7610) tensor(0.8035)\n",
            "=========\n",
            "epoch 3 train loss: 0.7446 val_mse: 0.7443\n",
            "tensor(0.7443) tensor(0.7610)\n",
            "=========\n",
            "epoch 4 train loss: 0.7360 val_mse: 0.7400\n",
            "tensor(0.7400) tensor(0.7443)\n",
            "=========\n",
            "epoch 5 train loss: 0.7312 val_mse: 0.7335\n",
            "tensor(0.7335) tensor(0.7400)\n",
            "=========\n",
            "epoch 6 train loss: 0.7288 val_mse: 0.7327\n",
            "tensor(0.7327) tensor(0.7335)\n",
            "=========\n",
            "epoch 7 train loss: 0.7277 val_mse: 0.7325\n",
            "tensor(0.7325) tensor(0.7327)\n",
            "=========\n",
            "epoch 8 train loss: 0.7268 val_mse: 0.7295\n",
            "tensor(0.7295) tensor(0.7325)\n",
            "=========\n",
            "epoch 9 train loss: 0.7258 val_mse: 0.7318\n",
            "tensor(0.7318) tensor(0.7295)\n",
            "=========\n",
            "epoch 10 train loss: 0.7253 val_mse: 0.7292\n",
            "tensor(0.7292) tensor(0.7295)\n",
            "=========\n",
            "epoch 11 train loss: 0.7252 val_mse: 0.7270\n",
            "tensor(0.7270) tensor(0.7292)\n",
            "=========\n",
            "epoch 12 train loss: 0.7249 val_mse: 0.7282\n",
            "tensor(0.7282) tensor(0.7270)\n",
            "=========\n",
            "epoch 13 train loss: 0.7243 val_mse: 0.7264\n",
            "tensor(0.7264) tensor(0.7270)\n",
            "=========\n",
            "epoch 14 train loss: 0.7240 val_mse: 0.7290\n",
            "tensor(0.7290) tensor(0.7264)\n",
            "=========\n",
            "epoch 15 train loss: 0.7237 val_mse: 0.7280\n",
            "tensor(0.7280) tensor(0.7264)\n",
            "=========\n",
            "epoch 16 train loss: 0.7239 val_mse: 0.7265\n",
            "tensor(0.7265) tensor(0.7264)\n",
            "=========\n",
            "epoch 17 train loss: 0.7234 val_mse: 0.7264\n",
            "tensor(0.7264) tensor(0.7264)\n",
            "=========\n",
            "epoch 18 train loss: 0.7230 val_mse: 0.7296\n",
            "tensor(0.7296) tensor(0.7264)\n",
            "EARLY STOPPING \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/utils.py:353: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history[\"train_loss\"], label='train')\n",
        "plt.plot(history[\"val_mse\"], label='val')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE');\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "VegPTPhUkPVu",
        "outputId": "9bfee2eb-3103-4755-a290-2570695d7816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ef0f1956350>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkYAAAHHCAYAAABa2ZeMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABeuElEQVR4nO3deXwTZeI/8M/k7n2fUCggImg55KhFVFYqp11AV0EQEEVWBFasuIJyePyg6rosu4qgWMQb1q+grCAKVVAQKFKQw1oFCq2lB+Xo3SRN5vfHJGlD7yOZpP28X695JZl5ZvJMSNsPz/PMM4IoiiKIiIiICAq5K0BERETkKhiMiIiIiCwYjIiIiIgsGIyIiIiILBiMiIiIiCwYjIiIiIgsGIyIiIiILBiMiIiIiCwYjIiIiIgsGIyIiIiILBiMiMitbNy4EYIg4KeffpK7KkTUDjEYEREREVkwGBERERFZMBgRUbtz9OhRjBkzBr6+vvD29saIESNw8OBBuzJGoxEvvPACevbsCZ1Oh6CgIAwbNgy7du2ylcnLy8PMmTPRuXNnaLVaREREYPz48Th37pyTz4iInEUldwWIiNrSqVOncNttt8HX1xd///vfoVar8dZbb2H48OHYu3cvYmNjAQDPP/88kpKSMGvWLAwZMgTFxcX46aefkJaWhrvuugsAcO+99+LUqVOYP38+oqOjUVBQgF27diErKwvR0dEyniUROYogiqIodyWIiJpq48aNmDlzJg4fPoxBgwbV2j5x4kTs2LED6enp6N69OwAgNzcXvXr1woABA7B3714AQP/+/dG5c2d8+eWXdb7P1atXERAQgH/84x9YuHCh406IiFwKu9KIqN0wmUz45ptvMGHCBFsoAoCIiAhMmTIF+/btQ3FxMQDA398fp06dwu+//17nsTw8PKDRaLBnzx5cuXLFKfUnIvkxGBFRu3Hx4kWUl5ejV69etbb17t0bZrMZ2dnZAIAXX3wRV69exfXXX4+YmBg8/fTTOH78uK28VqvFK6+8gq+++gphYWG4/fbb8eqrryIvL89p50NEzsdgREQd0u23344zZ85gw4YNuOmmm/DOO+/g5ptvxjvvvGMrs2DBAvz2229ISkqCTqfD0qVL0bt3bxw9elTGmhORIzEYEVG7ERISAk9PT2RkZNTa9uuvv0KhUCAqKsq2LjAwEDNnzsQnn3yC7Oxs9O3bF88//7zdfj169MBTTz2Fb775BidPnoTBYMA///lPR58KEcmEwYiI2g2lUomRI0fiiy++sLukPj8/Hx9//DGGDRsGX19fAMClS5fs9vX29sZ1110HvV4PACgvL0dlZaVdmR49esDHx8dWhojaH16uT0RuacOGDdi5c2et9c8//zx27dqFYcOG4fHHH4dKpcJbb70FvV6PV1991VauT58+GD58OAYOHIjAwED89NNP+L//+z/MmzcPAPDbb79hxIgRuP/++9GnTx+oVCps3boV+fn5mDx5stPOk4ici5frE5FbsV6uX5/s7GxcvHgRixcvxv79+2E2mxEbG4sVK1YgLi7OVm7FihXYtm0bfvvtN+j1enTt2hXTpk3D008/DbVajUuXLmH58uVISUlBdnY2VCoVbrjhBjz11FO47777nHGqRCQDBiMiIiIiC44xIiIiIrJgMCIiIiKyYDAiIiIismAwIiIiIrJgMCIiIiKyYDAiIiIisuAEj3Uwm824cOECfHx8IAiC3NUhIiKiJhBFESUlJYiMjIRC0bK2HwajOly4cMHufkpERETkPrKzs9G5c+cW7ctgVAcfHx8A0gdrva8SERERubbi4mJERUXZ/o63BINRHazdZ76+vgxGREREbqY1w2A4+JqIiIjIgsGIiIiIyILBiIiIiMiCY4yIiIicyGw2w2AwyF0Nt6RWq6FUKh36HgxGRERETmIwGJCZmQmz2Sx3VdyWv78/wsPDHTbPIIMRERGRE4iiiNzcXCiVSkRFRbV4AsKOShRFlJeXo6CgAAAQERHhkPdhMCIiInKCqqoqlJeXIzIyEp6ennJXxy15eHgAAAoKChAaGuqQbjXGVSIiIicwmUwAAI1GI3NN3Js1VBqNRoccn8GIiIjIiXgPztZx9OfHYERERERkwWBEREREThMdHY3Vq1fLXY16cfA1ERERNWj48OHo379/mwSaw4cPw8vLq/WVchAGIycymUUUlFSiyiQiKpBXJBARUfsgiiJMJhNUqsZjRUhIiBNq1HLsSnOizYezEZf0LV743ym5q0JERNQkDz30EPbu3Yt///vfEAQBgiBg48aNEAQBX331FQYOHAitVot9+/bhzJkzGD9+PMLCwuDt7Y3Bgwdj9+7ddse7titNEAS88847mDhxIjw9PdGzZ09s27bNyWdZjcHIiSL8dQCAC1crZa4JERHJTRRFlBuqZFlEUWxyPf/9738jLi4Ojz76KHJzc5Gbm4uoqCgAwKJFi/Dyyy8jPT0dffv2RWlpKcaOHYuUlBQcPXoUo0ePRkJCArKyshp8jxdeeAH3338/jh8/jrFjx2Lq1Km4fPlyqz7flmJXmhNF+kkTU10oqpC5JkREJLcKowl9ln0ty3v/8uIoeGqaFgH8/Pyg0Wjg6emJ8PBwAMCvv/4KAHjxxRdx11132coGBgaiX79+ttcvvfQStm7dim3btmHevHn1vsdDDz2EBx54AACwcuVK/Oc//0FqaipGjx7d7HNrLbYYOZG1xehquREVBpPMtSEiImqdQYMG2b0uLS3FwoUL0bt3b/j7+8Pb2xvp6emNthj17dvX9tzLywu+vr62W384G1uMnMhXp4aPVoUSfRUuFFWgR4i33FUiIiKZeKiV+OXFUbK9d1u49uqyhQsXYteuXXjttddw3XXXwcPDA3/5y19gMBgaPI5arbZ7LQiCbDfaZTBysgh/HUryS3HhKoMREVFHJghCk7uz5KbRaGy3NGnI/v378dBDD2HixIkApBakc+fOObh2bYtdaU4WYRlnlMsB2ERE5Caio6Nx6NAhnDt3DoWFhfW25vTs2RNbtmzBsWPH8PPPP2PKlCmytfy0FIORk0Var0zjAGwiInITCxcuhFKpRJ8+fRASElLvmKFVq1YhICAAQ4cORUJCAkaNGoWbb77ZybVtHfdow2tHItliREREbub666/HgQMH7NY99NBDtcpFR0fj22+/tVs3d+5cu9fXdq3VNXXA1atXW1TPtsAWIyeL8Ocl+0RERK6KwcjJIv2skzwyGBEREbkaBiMns7YY5RZVNmvmUSIiInI8BiMni7C0GJUbTCiuqJK5NkRERFST7MFozZo1iI6Ohk6nQ2xsLFJTUxssv3r1avTq1QseHh6IiorCk08+icpK+4HMOTk5ePDBBxEUFAQPDw/ExMTgp59+cuRpNJlOrUSQlwYAkMPuNCIiIpciazDavHkzEhMTsXz5cqSlpaFfv34YNWpUvdOAf/zxx1i0aBGWL1+O9PR0JCcnY/PmzXj22WdtZa5cuYJbb70VarUaX331FX755Rf885//REBAgLNOq1HWW4PkcgA2ERGRS5H1cv1Vq1bh0UcfxcyZMwEA69atw/bt27FhwwYsWrSoVvkff/wRt956K6ZMmQJAuizwgQcewKFDh2xlXnnlFURFReHdd9+1revWrZuDz6R5Ivw8cDKnGBeKeMk+ERGRK5GtxchgMODIkSOIj4+vroxCgfj4+FpzJVgNHToUR44csXW3nT17Fjt27MDYsWNtZbZt24ZBgwbhvvvuQ2hoKAYMGID169c3WBe9Xo/i4mK7xZE6WQdgsyuNiIjIpcgWjAoLC2EymRAWFma3PiwsDHl5eXXuM2XKFLz44osYNmwY1Go1evTogeHDh9t1pZ09exZr165Fz5498fXXX2POnDn429/+hvfee6/euiQlJcHPz8+2REVFtc1J1iOCl+wTERG5JNkHXzfHnj17sHLlSrz55ptIS0vDli1bsH37drz00ku2MmazGTfffDNWrlyJAQMGYPbs2Xj00Uexbt26eo+7ePFiFBUV2Zbs7GyHnkf1JI/sSiMiovYvOjoaq1evlrsaTSLbGKPg4GAolUrk5+fbrc/Pz0d4eHid+yxduhTTpk3DrFmzAAAxMTEoKyvD7Nmz8dxzz0GhUCAiIgJ9+vSx269379747LPP6q2LVquFVqtt5Rk1nXWSRw6+JiIici2ytRhpNBoMHDgQKSkptnVmsxkpKSmIi4urc5/y8nIoFPZVViqVAKrvtXLrrbciIyPDrsxvv/2Grl27tmX1WyXS0mKUV1QJs5mTPBIREbkKWbvSEhMTsX79erz33ntIT0/HnDlzUFZWZrtKbfr06Vi8eLGtfEJCAtauXYtNmzYhMzMTu3btwtKlS5GQkGALSE8++SQOHjyIlStX4vTp0/j444/x9ttv17qJnZxCfbRQCIDRJKKwVC93dYiIiOr19ttvIzIyEmaz2W79+PHj8fDDD+PMmTMYP348wsLC4O3tjcGDB2P37t0y1bb1ZL1cf9KkSbh48SKWLVuGvLw89O/fHzt37rQNyM7KyrJrIVqyZAkEQcCSJUuQk5ODkJAQJCQkYMWKFbYygwcPxtatW7F48WK8+OKL6NatG1avXo2pU6c6/fzqo1IqEOarQ25RJS4UVSLUVyd3lYiIyNlEETCWy/Peak9AEJpU9L777sP8+fPx3XffYcSIEQCAy5cvY+fOndixYwdKS0sxduxYrFixAlqtFu+//z4SEhKQkZGBLl26OPIsHEIQecOuWoqLi+Hn54eioiL4+vo65D3ueXM/0rKuYu3UmzEmJsIh70FERK6jsrISmZmZ6NatG3Q6HWAoA1ZGylOZZy8AGq8mF58wYQKCgoKQnJwMQGpFeuGFF5CdnV1riAsA3HTTTXjssccwb948ANLg6wULFmDBggWtrnqtz7GGtvj77VZXpbUnkbwyjYiI3MTUqVPx2WefQa+Xhn989NFHmDx5MhQKBUpLS7Fw4UL07t0b/v7+8Pb2Rnp6OrKysmSudcvI2pXWkdmCEecyIiLqmNSeUsuNXO/dDAkJCRBFEdu3b8fgwYPxww8/4F//+hcAYOHChdi1axdee+01XHfddfDw8MBf/vIXGAwGR9Tc4RiMZBLBS/aJiDo2QWhWd5acdDod7rnnHnz00Uc4ffo0evXqhZtvvhkAsH//fjz00EOYOHEiAKC0tBTnzp2Tsbatw2Akkwg/a4sRu9KIiMj1TZ06FXfffTdOnTqFBx980La+Z8+e2LJlCxISEiAIApYuXVrrCjZ3wjFGMrHdL40tRkRE5AbuvPNOBAYGIiMjw3Yzd0C6IXxAQACGDh2KhIQEjBo1ytaa5I7YYiSTCH+pK62gRA9DlRkaFTMqERG5LoVCgQsXao+Jio6Oxrfffmu37tq5A92pa41/jWUS5KWBRqWAKAL5xexOIyIicgUMRjIRBKHGAGwGIyIiIlfAYCSjSD+OMyIiInIlDEYyso4zyuFcRkRERC6BwUhGthYjXrJPRNRh8E5crePoz4/BSEbWFiN2pRERtX9KpRIA3HZGaFdRXi7deFetVjvk+LxcX0bVtwVhixERUXunUqng6emJixcvQq1W13nzVaqfKIooLy9HQUEB/P39bUGzrTEYycjalXaBLUZERO2eIAiIiIhAZmYmzp8/L3d13Ja/vz/Cw8MddnwGIxlZu9KulhtRYTDBQ+OY9EtERK5Bo9GgZ8+e7E5rIbVa7bCWIisGIxn56tTw1qpQqq/ChaIK9AjxlrtKRETkYAqFAjqdTu5qUD3YwSmzSOsAbI4zIiIikh2DkcwirOOMOJcRERGR7BiMZGZtMeIAbCIiIvkxGMksgpM8EhERuQwGI5nZ5jJiixEREZHsGIxkFuln6UrjGCMiIiLZMRjJLMLSYpRbVMn75xAREcmMwUhmEZYWo3KDCcUVVTLXhoiIqGNjMJKZTq1EkJcGAJDD7jQiIiJZMRi5AOutQXI5AJuIiEhWDEYuwDbJYxEv2SciIpITg5ELsF6ZlsuuNCIiIlkxGLkA21xGDEZERESyYjByARH+7EojIiJyBQxGLsDWlcbB10RERLJiMHIB1hajvKJKmM2c5JGIiEguDEYuIMxHC4UAGE0iCkv1cleHiIiow2IwcgEqpQJhvpZ7pnGcERERkWwYjFxEBC/ZJyIikh2DkYvglWlERETyc4lgtGbNGkRHR0On0yE2NhapqakNll+9ejV69eoFDw8PREVF4cknn0RlZd2B4uWXX4YgCFiwYIEDat52OnEuIyIiItnJHow2b96MxMRELF++HGlpaejXrx9GjRqFgoKCOst//PHHWLRoEZYvX4709HQkJydj8+bNePbZZ2uVPXz4MN566y307dvX0afRahG8ZJ+IiEh2sgejVatW4dFHH8XMmTPRp08frFu3Dp6entiwYUOd5X/88UfceuutmDJlCqKjozFy5Eg88MADtVqZSktLMXXqVKxfvx4BAQHOOJVWsd0v7Sq70oiIiOQiazAyGAw4cuQI4uPjbesUCgXi4+Nx4MCBOvcZOnQojhw5YgtCZ8+exY4dOzB27Fi7cnPnzsW4cePsjl0fvV6P4uJiu8XZIv3ZYkRERCQ3lZxvXlhYCJPJhLCwMLv1YWFh+PXXX+vcZ8qUKSgsLMSwYcMgiiKqqqrw2GOP2XWlbdq0CWlpaTh8+HCT6pGUlIQXXnih5SfSBqz3Syso0cNQZYZGJXtjHhERUYfjdn999+zZg5UrV+LNN99EWloatmzZgu3bt+Oll14CAGRnZ+OJJ57ARx99BJ1O16RjLl68GEVFRbYlOzvbkadQpyAvDTQqBUQRyC9mdxoREZEcZG0xCg4OhlKpRH5+vt36/Px8hIeH17nP0qVLMW3aNMyaNQsAEBMTg7KyMsyePRvPPfccjhw5goKCAtx88822fUwmE77//nu88cYb0Ov1UCqVdsfUarXQarVtfHbNIwgCIvx0OH+pHLlFlYgK9JS1PkRERB2RrC1GGo0GAwcOREpKim2d2WxGSkoK4uLi6tynvLwcCoV9ta1BRxRFjBgxAidOnMCxY8dsy6BBgzB16lQcO3asVihyJbwyjYiISF6ythgBQGJiImbMmIFBgwZhyJAhWL16NcrKyjBz5kwAwPTp09GpUyckJSUBABISErBq1SoMGDAAsbGxOH36NJYuXYqEhAQolUr4+PjgpptusnsPLy8vBAUF1VrvaqzjjHI4lxEREZEsZA9GkyZNwsWLF7Fs2TLk5eWhf//+2Llzp21AdlZWll0L0ZIlSyAIApYsWYKcnByEhIQgISEBK1askOsU2kyk5ZL9XF6yT0REJAtBFEVR7kq4muLiYvj5+aGoqAi+vr5Oe9+PDp3Hc1tPIr53KN6ZMdhp70tERNQetMXfb7e7Kq09i+Qkj0RERLJiMHIhkbYbyXKMERERkRwYjFxIhGX266vlRlQYTDLXhoiIqONhMHIhvjo1vLXSeHi2GhERETkfg5GLsc1lxHFGRERETsdg5GJs44w4lxEREZHTMRi5mEjLOCN2pRERETkfg5GLieAkj0RERLJhMHIx1jFGbDEiIiJyPgYjF9OJY4yIiIhkw2DkYiIswSi3qBK8WwsREZFzMRi5GGtXWrnBhOKKKplrQ0RE1LEwGLkYnVqJQC8NAI4zIiIicjYGIxdku2Sf44yIiIicisHIBVkv2b9QxEv2iYiInInByAVF2m4LwhYjIiIiZ2IwckE1r0wjIiIi52EwckHW+6XlsMWIiIjIqRiMXJCtK41XpRERETkVg5ELsnal5RVVwmzmJI9ERETOwmDkgsJ8tFAIgNEkorBUL3d1iIiIOgwGIxekUioQ5mu9mSwHYBMRETkLg5GLiuAl+0RERE7HYOSirOOM2GJERETkPAxGLsp6ZRpvC0JEROQ8DEYuKtI2ySODERERkbMwGLko2/3SrrIrjYiIyFkYjFxUpD8neSQiInI2BiMXZW0xKijRw1Bllrk2REREHQODkYsK8tJAo1JAFIH8YnanEREROQODkYtSKITquYx4yT4REZFTMBi5sAjeTJaIiMipGIxcWKRlnFEO5zIiIiJyCgYjF2aby4iX7BMRETkFg5ELi+Al+0RERE7lEsFozZo1iI6Ohk6nQ2xsLFJTUxssv3r1avTq1QseHh6IiorCk08+icrK6laVpKQkDB48GD4+PggNDcWECROQkZHh6NNoc5Gc5JGIiMipZA9GmzdvRmJiIpYvX460tDT069cPo0aNQkFBQZ3lP/74YyxatAjLly9Heno6kpOTsXnzZjz77LO2Mnv37sXcuXNx8OBB7Nq1C0ajESNHjkRZWZmzTqtNWFuMLrDFiIiIyCkEURRFOSsQGxuLwYMH44033gAAmM1mREVFYf78+Vi0aFGt8vPmzUN6ejpSUlJs65566ikcOnQI+/btq/M9Ll68iNDQUOzduxe33357o3UqLi6Gn58fioqK4Ovr28Iza73iSiP6Pv8NACD9xdHw0ChlqwsREZGra4u/37K2GBkMBhw5cgTx8fG2dQqFAvHx8Thw4ECd+wwdOhRHjhyxdbedPXsWO3bswNixY+t9n6KiIgBAYGBgG9be8Xx1anhrVQDYakREROQMKjnfvLCwECaTCWFhYXbrw8LC8Ouvv9a5z5QpU1BYWIhhw4ZBFEVUVVXhscces+tKq8lsNmPBggW49dZbcdNNN9VZRq/XQ6/X214XFxe38IzaXoSfDr8XlCL3aiV6hHjLXR0iIqJ2TfYxRs21Z88erFy5Em+++SbS0tKwZcsWbN++HS+99FKd5efOnYuTJ09i06ZN9R4zKSkJfn5+tiUqKspR1W+2CH/rAGy2GBERETmarC1GwcHBUCqVyM/Pt1ufn5+P8PDwOvdZunQppk2bhlmzZgEAYmJiUFZWhtmzZ+O5556DQlGd9ebNm4cvv/wS33//PTp37lxvPRYvXozExETb6+LiYpcJR504AJuIiMhpZG0x0mg0GDhwoN1AarPZjJSUFMTFxdW5T3l5uV34AQClUhqUbB1HLooi5s2bh61bt+Lbb79Ft27dGqyHVquFr6+v3eIqIvw4ySMREZGzyNpiBACJiYmYMWMGBg0ahCFDhmD16tUoKyvDzJkzAQDTp09Hp06dkJSUBABISEjAqlWrMGDAAMTGxuL06dNYunQpEhISbAFp7ty5+Pjjj/HFF1/Ax8cHeXl5AAA/Pz94eHjIc6ItZL1fGluMiIiIHE/2YDRp0iRcvHgRy5YtQ15eHvr374+dO3faBmRnZWXZtRAtWbIEgiBgyZIlyMnJQUhICBISErBixQpbmbVr1wIAhg8fbvde7777Lh566CGHn1NbiuQYIyIiIqeRfR4jV+Qq8xgBQGZhGf702h54apQ49cIoCIIga32IiIhcldvPY0SNs3allRtMKK6okrk2RERE7RuDkYvTqZUI9NIA4DgjIiIiR2MwcgO2AdgcZ0RERORQDEZuwDYAu4iX7BMRETkSg5EbiLS0GOWyxYiIiMihGIzcgPW2ILlsMSIiInIoBiM3YB1jlMMWIyIiIodiMHIDnWwtRgxGREREjsRg5AasXWl5RZUwmzkfJxERkaMwGLmBMB8tFAJgNIkoLNPLXR0iIqJ2i8HIDaiUCoT6WOcy4gBsIiIiR2EwchOR/rxkn4iIyNEYjNxEBCd5JCIicjgGIzfBSR6JiIgcj8HITUT4WVuMGIyIiIgchcHITdjul8bB10RERA7DYOQmbIOv2WJERETkMAxGbsLalVZQooehyixzbYiIiNonBiM3EeSlgUapgCgC+cXsTiMiInIEBiM3oVAIiLB1pzEYEREROQKDkRuJ8OM4IyIiIkdiMHIjkZZxRjmcy4iIiMghGIzciK0rjZfsExEROQSDkRuxzmXErjQiIiLHYDByI9auNE7ySERE5BgMRm7E2pXG24IQERE5BoORG7FO8ni13IgKg0nm2hAREbU/DEZuxFengrdWBYCtRkRERI7AYORGBEGonsuI44yIiIjaHIORm4nwtw7AZosRERFRW2MwkoMotnjXSD8OwCYiInIUBiNn+u0bYMNo4JslLT6EbS4jdqURERG1OQYjZ6qqBLIOAL/vavEhIthiRERE5DAMRs7U7TZAUACFGUBRTosOEckxRkRERA7DYORMHgFA5ADpeebeFh3CdlVaUSXEVoxVIiIiotoYjJyt+3Dp8cx3Ldrd2mJUbjChuKKqjSpFREREgIsEozVr1iA6Oho6nQ6xsbFITU1tsPzq1avRq1cveHh4ICoqCk8++SQqK+0HIzf3mE7T/U/S49k9Lbo6TadWItBLA4DjjIiIiNqa7MFo8+bNSExMxPLly5GWloZ+/fph1KhRKCgoqLP8xx9/jEWLFmH58uVIT09HcnIyNm/ejGeffbbFx3SqqCGA2hMoKwAKfmnRIWwDsDnOiIiIqE3JHoxWrVqFRx99FDNnzkSfPn2wbt06eHp6YsOGDXWW//HHH3HrrbdiypQpiI6OxsiRI/HAAw/YtQg195hOpdICXYdKz1vYnWa9Z9qFIl6yT0RE1JZkDUYGgwFHjhxBfHy8bZ1CoUB8fDwOHDhQ5z5Dhw7FkSNHbEHo7Nmz2LFjB8aOHdviY+r1ehQXF9stDmXrTmtZMOrkb70tCFuMiIiI2pJKzjcvLCyEyWRCWFiY3fqwsDD8+uuvde4zZcoUFBYWYtiwYRBFEVVVVXjsscdsXWktOWZSUhJeeOGFNjijJuphCUbnfwSq9FIrUjNYbwuSyxYjIiKiNtWsFqNXX30VFRXVrRT79++HXq+3vS4pKcHjjz/edrWrw549e7By5Uq8+eabSEtLw5YtW7B9+3a89NJLLT7m4sWLUVRUZFuys7PbsMZ1CO0DeIUCxnIgu/mDwq1jjHLYYkRERNSmmhWMFi9ejJKSEtvrMWPGICeneqLC8vJyvPXWW00+XnBwMJRKJfLz8+3W5+fnIzw8vM59li5dimnTpmHWrFmIiYnBxIkTsXLlSiQlJcFsNrfomFqtFr6+vnaLQwlC9WX7Z/c0e3fbbUF4VRoREVGbalYwunZCwdZOMKjRaDBw4ECkpKTY1pnNZqSkpCAuLq7OfcrLy6FQ2FdbqVTa6tOSY8rCFoyaP87IGozyiiphNnOSRyIiorYi6xgjAEhMTMSMGTMwaNAgDBkyBKtXr0ZZWRlmzpwJAJg+fTo6deqEpKQkAEBCQgJWrVqFAQMGIDY2FqdPn8bSpUuRkJBgC0iNHdMlWMcZXTgKVFyRZsVuojAfLRQCYDSJKCzTI9RH56BKEhERdSyyB6NJkybh4sWLWLZsGfLy8tC/f3/s3LnTNng6KyvLroVoyZIlEAQBS5YsQU5ODkJCQpCQkIAVK1Y0+ZguwTcSCO4l3Tct8wegz5+bvKtKqUCojw55xZW4cLWSwYiIiKiNCGIz+sMUCgX+3//7f/D29gYAPPPMM3j66acRHBwMQBp8vWzZMphMJsfU1kmKi4vh5+eHoqIix4432vF3IPUtYNDDwN3/atauE9/cj6NZV7F26s0YExPhoAoSERG5j7b4+92sFqMuXbpg/fr1ttfh4eH44IMPapWhJurxJykYtXAA9tGsq5zkkYiIqA01KxidO3fOQdXooLreCghK4PJZ4Mp5IKBrk3eN9OMkj0RERG1N9luCdGg6X6DzYOl5M69Oq74tCIMRERFRW2lWMDpw4AC+/PJLu3Xvv/8+unXrhtDQUMyePdtuwkdqAuvVac3sTov0t95Ill1pREREbaVZwejFF1/EqVOnbK9PnDiBRx55BPHx8Vi0aBH+97//2S6rpyayzWe0FzCbm7wbJ3kkIiJqe80KRseOHcOIESNsrzdt2oTY2FisX78eiYmJ+M9//oP//ve/bV7Jdq3TQEDjA1RcBvJ+bvJu1q60ghI9jKamByoiIiKqX7OC0ZUrV+zmAtq7dy/GjBljez148GDH32esvVGqgehh0vNmdKcFeWmgUSogitIM2ERERNR6zQpGYWFhyMzMBAAYDAakpaXhlltusW0vKSmBWq1u2xp2BNZxRmeaPgBboRAQbr0yjcGIiIioTTQrGI0dOxaLFi3CDz/8gMWLF8PT0xO33Xabbfvx48fRo0ePNq9ku9fdEoyyDgLGpo8Zsg7A5jgjIiKittGsYPTSSy9BpVLhjjvuwPr16/H2229Do9HYtm/YsAEjR45s80q2e8E9AZ9IwKQHsg40ebdI6yX7vDKNiIioTTRrgsfg4GB8//33KCoqgre3t+2mrVaffvopfHx82rSCHYIgSN1pxz6SutN63Nmk3SJsl+yzxYiIiKgtNCsYPfzww00qt2HDhhZVpkPrPlwKRs0YgG29Mo1daURERG2jWcFo48aN6Nq1KwYMGIBm3HuWmsI6n1HecaCsEPAKbnSXTv7sSiMiImpLzQpGc+bMwSeffILMzEzMnDkTDz74IAIDAx1Vt47FOxQIuwnIPwlk7gVuurfRXSI4+JqIiKhNNWvw9Zo1a5Cbm4u///3v+N///oeoqCjcf//9+Prrr9mC1BasrUZNvGzf2pV2pdyICoPJQZUiIiLqOJp9E1mtVosHHngAu3btwi+//IIbb7wRjz/+OKKjo1FaWuqIOnYc3WvcN60JQdNXp4KXRhoAz5vJEhERtV6zg5HdzgoFBEGAKIowmdhi0Wpd4wClBijKBi6fbbS4IAjV90zjOCMiIqJWa3Yw0uv1+OSTT3DXXXfh+uuvx4kTJ/DGG28gKysL3t7ejqhjx6HxAqJipednvm3SLhG2AdhsMSIiImqtZg2+fvzxx7Fp0yZERUXh4YcfxieffILg4MavnqJm6D4cOPeD1J025NFGi0dabgvCrjQiIqLWa1YwWrduHbp06YLu3btj79692Lt3b53ltmzZ0iaV65C6/wn49iUg8wfAVAUoG/4nss1lxK40IiKiVmtWMJo+fToEQXBUXQgAIvsDOj+gsgi4cBSIGtxwcX+2GBEREbWVZk/wSA6mUALdbgfS/yd1pzUajDjGiIiIqK206qo0chDbZfuNz2cU4Wed5LGSc0kRERG1EoORK+phCUbZqYC+4bmhrGOMyg0mFFdUObpmRERE7RqDkSsK6Ab4dwHMRuD8jw0W9dAoEeilAcBxRkRERK3FYOSKBKFF3WkcZ0RERNQ6DEauqkeN24M0wtqddqGIl+wTERG1BoORq+p2BwABKPgFKMlrsKj1kv1cthgRERG1CoORq/IMBCL6Sc8baTWy3S+NLUZEREStwmDkyroPlx4bCUbWMUY5bDEiIiJqFQYjV2YdZ3TmO6CBOYqqW4wYjIiIiFqDwciVRd0CqHRAaR5w8dd6i1lbjPKKKmE2c5JHIiKilmIwcmVqHdAlTnreQHdamK8OCgEwmkQUlumdUzciIqJ2iMHI1dXsTquHWqlAqI91LiMOwCYiImopBiNXZx2AfW4fYDLWWyyCl+wTERG1GoORqwuLATyDAWMZ8MfheotFcpJHIiKiVnOJYLRmzRpER0dDp9MhNjYWqamp9ZYdPnw4BEGotYwbN85WprS0FPPmzUPnzp3h4eGBPn36YN26dc44lbanUADd75CeN9CdxkkeiYiIWk/2YLR582YkJiZi+fLlSEtLQ79+/TBq1CgUFBTUWX7Lli3Izc21LSdPnoRSqcR9991nK5OYmIidO3fiww8/RHp6OhYsWIB58+Zh27ZtzjqtttWE+YyqbwvCYERERNRSsgejVatW4dFHH8XMmTNtLTuenp7YsGFDneUDAwMRHh5uW3bt2gVPT0+7YPTjjz9ixowZGD58OKKjozF79mz069evwZYol2a9oWzOEaCyqM4i1hYjDr4mIiJqOVmDkcFgwJEjRxAfH29bp1AoEB8fjwMHDjTpGMnJyZg8eTK8vLxs64YOHYpt27YhJycHoijiu+++w2+//YaRI0fWeQy9Xo/i4mK7xaX4RwGBPQDRJA3CroO1xYiTPBIREbWcrMGosLAQJpMJYWFhduvDwsKQl9fwjVMBIDU1FSdPnsSsWbPs1r/++uvo06cPOnfuDI1Gg9GjR2PNmjW4/fbb6zxOUlIS/Pz8bEtUVFTLT8pRGrls3zr7dUGJHkaT2Vm1IiIialdk70prjeTkZMTExGDIkCF2619//XUcPHgQ27Ztw5EjR/DPf/4Tc+fOxe7du+s8zuLFi1FUVGRbsrOznVH95rF2p52tOxgFeWmgUSogitIM2ERERNR8KjnfPDg4GEqlEvn5+Xbr8/PzER4e3uC+ZWVl2LRpE1588UW79RUVFXj22WexdetW25Vqffv2xbFjx/Daa6/ZddtZabVaaLXaVp6Ng0UPAwQFcOk0cDVb6l6rQaEQEO6nQ9blcuQWVSIq0FOmihIREbkvWVuMNBoNBg4ciJSUFNs6s9mMlJQUxMXFNbjvp59+Cr1ejwcffNBuvdFohNFohEJhf2pKpRJmsxt3MXn4A50GSs/ruTrNes80jjMiIiJqGdm70hITE7F+/Xq89957SE9Px5w5c1BWVoaZM2cCAKZPn47FixfX2i85ORkTJkxAUFCQ3XpfX1/ccccdePrpp7Fnzx5kZmZi48aNeP/99zFx4kSnnJPDNHLZfifLOCNemUZERNQysnalAcCkSZNw8eJFLFu2DHl5eejfvz927txpG5CdlZVVq/UnIyMD+/btwzfffFPnMTdt2oTFixdj6tSpuHz5Mrp27YoVK1bgsccec/j5OFT3PwHf/0MKRmazNPljDRG2S/bZYkRERNQSgiiKotyVcDXFxcXw8/NDUVERfH195a5OtSoD8Eq0dHuQv/4ARPS12/zhwfNY8vlJxPcOxTszBstTRyIiIpm0xd9v2bvSqBlUGiD6Vul5Hd1pnOSRiIiodRiM3E0Dl+1b5zLi4GsiIqKWYTByN9YB2Od/BIz2LUPW2a+vlBtRYTA5uWJERETuj8HI3YT2BrzDgapKIPuQ3SZfnQpeGiUA3kyWiIioJRiM3I0g1Lhs/7trNgmIsHancZwRERFRszEYuaMG5jOyjjNiixEREVHzMRi5I2swunAMKL9stynSj3MZERERtRSDkTvyjQBCbgAgApl77TZZB2CzK42IiKj5GIzcle2y/T12q22zX7MrjYiIqNkYjNxVD0swOmM/ALuTbS4jthgRERE1F4ORu+o6FFCogKvngcuZttURNcYY8W4vREREzcNg5K60PkDnIdLzGpftW8cYlRtMKK6okqNmREREbovByJ3V0Z3moVEiwFMNgOOMiIiImovByJ1ZL9vP/B4wV98CxDaXES/ZJyIiahYGI3cWeTOg9QUqrwK5x2yrrd1pFzgAm4iIqFkYjNyZUgVE3yY9r9GdFmm5ZD+XLUZERETNwmDk7nrUns/INskjW4yIiIiahcHI3Vknesw+BBjKAVS3GOWwxYiIiKhZGIzcXVAPwLczYDIAWT8CqB58ncur0oiIiJqFwcjdCQLQY7j03DLOyDrJY15RJcxmTvJIRETUVAxG7cE1900L89VBEACjSURhmV6+ehEREbkZBqP2oNsd0mP+SaC0AGqlAmE+UqtR9mV2pxERETUVg1F74B0ChMdIz8/uBQDc1MkPAPDfw9ly1YqIiMjtMBi1F9ZZsC3daXOGdwcAfJb2B7Ivl8tTJyIiIjfDYNRe2MYZfQeIIgZ2DcSw64JRZRbx5p7T8taNiIjITTAYtRdd4gClBijOAQp/BwA8Ed8TAPDpT3/gjytsNSIiImoMg1F7ofEEutwiPbd0pw2ODsTQHkGWVqMz8tWNiIjITTAYtSc1u9MsnhhhbTXK5kzYREREjWAwak+sA7AzfwBMRgBAbPcg3NI9EEaTiLUca0RERNQgBqP2JKIf4BEAGEqAnDTb6idGXA8A+O/hP3ibECIiogYwGLUnCiXQ7XbpeY3utLgeQRjSLRAGkxlrOdaIiIioXgxG7c01twexWmAZa7QpNRt5RZVOrhQREZF7YDBqb3pYgtEfhwF9iW11XI8gDI4OgMFkxrq9bDUiIiKqC4NRexMQLS3mKuDcPttqQRBsY40+Ts1CfjFbjYiIiK7FYNQe1dOddut1QRjYNQCGKrYaERER1cUlgtGaNWsQHR0NnU6H2NhYpKam1lt2+PDhEASh1jJu3Di7cunp6fjzn/8MPz8/eHl5YfDgwcjKynL0qbgG62X7Z76zWy21GkljjT4+lIUCthoRERHZkT0Ybd68GYmJiVi+fDnS0tLQr18/jBo1CgUFBXWW37JlC3Jzc23LyZMnoVQqcd9999nKnDlzBsOGDcMNN9yAPXv24Pjx41i6dCl0Op2zTkte3W4HIACFGcDVbLtNt/UMxoAu/tBXmfHW92flqR8REZGLEkRRFOWsQGxsLAYPHow33ngDAGA2mxEVFYX58+dj0aJFje6/evVqLFu2DLm5ufDy8gIATJ48GWq1Gh988EGL6lRcXAw/Pz8UFRXB19e3RceQ3YbRQNYBoM8E4L6NgCDYNu3JKMBD7x6GTq3AD3+/EyE+WtmqSURE1Fba4u+3rC1GBoMBR44cQXx8vG2dQqFAfHw8Dhw40KRjJCcnY/LkybZQZDabsX37dlx//fUYNWoUQkNDERsbi88//9wRp+C6RicBChXwy+fAyc/sNt1xfQj6Rfmj0mjG299zrBEREZGVrMGosLAQJpMJYWFhduvDwsKQl5fX6P6pqak4efIkZs2aZVtXUFCA0tJSvPzyyxg9ejS++eYbTJw4Effccw/27t1b53H0ej2Ki4vtFrcXOQC4/Wnp+fangOJc2yZBEGzzGn1w8DwKS/Vy1JCIiMjlyD7GqDWSk5MRExODIUOG2NaZzWYAwPjx4/Hkk0+if//+WLRoEe6++26sW7euzuMkJSXBz8/PtkRFRTml/g5321NARH+g8irwv78BNXpNh/cKQd/Ofqg0mrGeY42IiIgAyByMgoODoVQqkZ+fb7c+Pz8f4eHhDe5bVlaGTZs24ZFHHql1TJVKhT59+tit7927d71XpS1evBhFRUW2JTs7u85ybkepBiauA5Ra4PdvgLT3bZtqXqH2/oHzuMRWIyIiInmDkUajwcCBA5GSkmJbZzabkZKSgri4uAb3/fTTT6HX6/Hggw/WOubgwYORkZFht/63335D165d6zyWVquFr6+v3dJuhPYG7lwiPf/6WeDKedumO28IRUwnP1QYTXhnX6ZMFSQiInIdsnelJSYmYv369XjvvfeQnp6OOXPmoKysDDNnzgQATJ8+HYsXL661X3JyMiZMmICgoKBa255++mls3rwZ69evx+nTp/HGG2/gf//7Hx5//HGHn49LipsLdBkKGEqBzx8HLN2NgiDgb9ZWox/P4UqZQc5aEhERyU72YDRp0iS89tprWLZsGfr3749jx45h586dtgHZWVlZyM3NtdsnIyMD+/btq9WNZjVx4kSsW7cOr776KmJiYvDOO+/gs88+w7Bhwxx+Pi5JoQQmrAHUXsD5fcCh6rFW8b1DcWOkL8oMJryzj2ONiIioY5N9HiNX1C7mMarL4WRgeyKg0gF//QEIke6d9vWpPPz1gyPw1qqw75k/wd9TI3NFiYiIms/t5zEiJxv0MNDjTqCqEvj8McBUBQAY2ScMvSN8UaqvQjLHGhERUQfGYNSRCALw5zcArR+QcwTY/y/LagFPjLgOALBx/zkUlRvlrCUREZFsGIw6Gr9OwNh/SM/3vALkHgcAjOwTjhvCfVCir0LyfrYaERFRx8Rg1BH1vR/onQCYjcDWx4AqPRSK6ivU3t2fiaIKthoREVHHw2DUEQkCcPdqwDMYKDgF7EkCAIy+MRy9wnxQUlmFd9lqREREHRCDUUflFQwk/Ft6vv/fQNYhKBQC5lvGGm3Yl4niSrYaERFRx8Jg1JH1vhvoOxkQzdJVaoYyjL0pAj1DvVFcWYWN+8/JXUMiIiKnYjDq6Ma8AvhEApfPAruft7QaSWONkvdlooStRkRE1IEwGHV0Hv7A+Dek56lvA2f3YFxMBHqEeKGowoj3fjwnZ+2IiIicisGIgOtGAIMst1f5fC6UhmLbFWrv7MtEqb5KxsoRERE5D4MRSe56EQjoBhT/Aex8Fnf3jUT3EC9cLWerERERdRwMRiTRegMT1gIQgGMfQvn7Tsy/U7pC7Z0fzqKMrUZERNQBMBhRta5xwND50vNtf0PCdVp0C/bClXIj3j9wXt66EREROQGDEdn703NASG+grACqr57C3OE9AADr2WpEREQdAIMR2VPrgIlrAYUK+OULTFQfQNcgT1wuM+DDg2w1IiKi9o3BiGqLHADc/jQAQPnV03gqzhcA8Pb3Z1FuYKsRERG1XwxGVLfbngIi+gOVV3H3uZWICtDhUpkBHx3MkrtmREREDsNgRHVTqoGJbwFKLRSnd2PVdccBAG99fwYVBpPMlSMiInIMBiOqX+gNwIilAIBBGa9hsF8xCksN+OgQxxoREVH7xGBEDbvlcaDLUAiGUrzh9Q4EmPHW92dRaWSrERERtT8MRtQwhRKY8Cag9kLY5Z+wwPtbXCzR4+NDHGtERETtD4MRNS6wGzDq/wEA5po/Qg8hB+v2nmGrERERtTsMRtQ0A2cCPUZAZdbjdd1buFRSjk2pbDUiIqL2hcGImkYQgPFvADo/9BFPY45yG9ay1YiIiNoZBiNqOt9IYOxrAIAF6i0IKsnApz9ly1wpIiKitsNgRM0Tcx/QOwEqmLBKvRbrv/sV+iq2GhERUfvAYETNIwjA3ashegbjBkU2Hij/EJ/+9IfctSIiImoTDEbUfF7BEBL+DQCYrfwSP3y7HYYqs8yVIiIiaj0GI2qZ3nfDFDMZSkHEosrV+Oxghtw1IiIiajUGI2ox5dhXUKoNQzdFPrp/8xC++Ww9xCq93NUiIiJqMQYjajkPf+j+sg5VUCFW8StGnliIkpXXQ799MVCQLnftiIiImk0QRVGUuxKupri4GH5+figqKoKvr6/c1XF5YuHvOLX9TYSe3YJQ4Wr1hk6DgAEPAjfdA+j8ZKsfERF1DG3x95vBqA4MRi1zMvsSPvjgHdxZ8Q3uVByFWrBcxq/yAPqMl0JS11sBBRsqiYio7TEYOQiDUcsVVRjx9Kc/I+2X3zBBuQ+PeO1DhOF8dYGAaCkg9ZsC+HWSrZ5ERNT+MBg5CINR64iiiA37zyFpRzqqzGaMDcjBiq4/I+DsNsBQIhUSFECPO6WQ1GssoNLKW2kiInJ7DEYOwmDUNo6cv4J5H6cht6gSWpUCK8Z1w70eaRCOfgSc31dd0CMA6DtJCknhMfJVmIiI3Fpb/P12icEea9asQXR0NHQ6HWJjY5Gamlpv2eHDh0MQhFrLuHHj6iz/2GOPQRAErF692kG1p/oM7BqA7X+7DcN7hUBfZcbCL87gqd/6oHzqF8D8NOC2hYBPJFBxBTi0Dlg3DHjrDiB1vbSOiIjIyWQPRps3b0ZiYiKWL1+OtLQ09OvXD6NGjUJBQUGd5bds2YLc3FzbcvLkSSiVStx33321ym7duhUHDx5EZGSko0+D6hHopcGGGYPx9KheUAjAlrQcjH9jP06bQoERS4EnTwJTPwP6TAAUaiD3GLBjIfBaL+D/HgHOfAeYOas2ERE5h+xdabGxsRg8eDDeeOMNAIDZbEZUVBTmz5+PRYsWNbr/6tWrsWzZMuTm5sLLy8u2PicnB7Gxsfj6668xbtw4LFiwAAsWLGhSndiV5hgHz17C/E+O4mKJHh5qJZLuicGEATUGYJddAk78F0j7ACg4Vb3erwvQf4q0BHR1fsWJiMgtuH1XmsFgwJEjRxAfH29bp1AoEB8fjwMHDjTpGMnJyZg8ebJdKDKbzZg2bRqefvpp3HjjjY0eQ6/Xo7i42G6htndL9yDs+NttGNojCBVGExZsPobFW06g0mi5rN8rCLhlDjBnPzB7DzB4FqD1A4qygL0vA//uC7w/AfjjiJynQURE7ZiswaiwsBAmkwlhYWF268PCwpCXl9fo/qmpqTh58iRmzZplt/6VV16BSqXC3/72tybVIykpCX5+frYlKiqq6SdBzRLio8UHj8TibyN6QhCAT1KzcM+bP+JcYVl1IUEAIgcA4/4JLMwA7nkH6HaHtO3sd8A7dwKfPQoU/SHPSRARUbsl+xij1khOTkZMTAyGDBliW3fkyBH8+9//xsaNGyEIQpOOs3jxYhQVFdmW7OxsR1WZACgVAhLvuh7vzRyCQC8Nfsktxt2v78NXJ3JrF1Z7AH3vA2ZsA574WZr/CJC63F4fBHy7AtCXOvcEiIio3ZI1GAUHB0OpVCI/P99ufX5+PsLDwxvct6ysDJs2bcIjjzxit/6HH35AQUEBunTpApVKBZVKhfPnz+Opp55CdHR0ncfSarXw9fW1W8jxbr8+BDv+dhsGdQ1Aqb4Kcz5Kwwv/OwVDVT2DrQOigYlrpW62LkOBqgrg+1eB1wcCRz/iIG0iImo1WYORRqPBwIEDkZKSYltnNpuRkpKCuLi4Bvf99NNPodfr8eCDD9qtnzZtGo4fP45jx47ZlsjISDz99NP4+uuvHXIe1HLhfjp8MvsW/PWO7gCAd/efw31vHcAfV8rr3ylyADBzB3D/+1JYKs0DvngcePsO4Ny++vcjIiJqhOxdaYmJiVi/fj3ee+89pKenY86cOSgrK8PMmTMBANOnT8fixYtr7ZecnIwJEyYgKCjIbn1QUBBuuukmu0WtViM8PBy9evVyyjlR86iVCiwe0xvvTB8EPw81fs6+inH/2YeU9Pz6dxIE6f5rc1OBu14EtL5A3nFg4zhg01Tg8lnnnQAREbUbsgejSZMm4bXXXsOyZcvQv39/HDt2DDt37rQNyM7KykJurv3Yk4yMDOzbt69WNxq5t/g+Yfhy/jD06+yHogojHnnvJ7z81a+oMjXQRabSArc+IU0YOehh6VYjv34JvDEE+Po5oOKq0+pPRETuT/Z5jFwR5zGSl6HKjJU70rHxx3MAgCHRgfjPAwMQ7qdrfOf8X4BvngPOfCu99gwChi8GBs4ElCrHVZqIiGTHe6U5CIORa9h+PBfPfHYcpfoqBHlpsHpyf9zWM6TxHUUROL1bajEqzJDWhdwAjFwB9IxveF8iInJbDEYOwmDkOjILy/D4R2lIzy2GIADz/3Qd/npHD3hpm9D6YzICRzYC360EKi5L666LlwJS6A0OrTcRETkfg5GDMBi5lkqjCS/87xd8kpoFAPDRqnDvwM548JauuC7Uu/EDVFwBvn8NOPQWYDYCghIYNBMY/qw02zYREbULDEYOwmDkmr44loPVu39HZo1Zsof2CML0uK6I7x0GlbKRawkunQF2LZMGZwPS7UbueBoYMlsaxE1ERG6NwchBGIxcl9ksYv+ZQrx/4DxS0vNhtnx7w311mBLbBZMHRyHUt5FB2pnfA18/C+SdkF4HdANGvgTccLc0DQAREbklBiMHYTByDzlXK/DxofPYlJqNS2UGAIBKIWDUTeGYfktXDOkWWP9tYcwm4OdPgJQXgVLLfEldhwGjVgCR/Z1zAkRE1KYYjByEwci96KtM2HkyDx8cOI+fzl+xre8V5oMH47pi4oBO8K5vsLa+FNi/GvjxdaCqEoAA9J8C3LkU8I1wSv2JiKhtMBg5CIOR+zp1oQgfHszC50dzUGE0AQC8tSrcc3MnTLulK3qG+dS949VsIOUF4MSn0mu1F9D9DsAjEPDwlxadP+ARYHluefQIAHR+gELp+JMjIqIGMRg5CIOR+yuqMGJL2h/44OB5nL1YPVj7lu6BmHZLNEbeGAZ1XYO1sw9L44/+SG3eG2r9AA8/S1Dyrw5Q176+dpvGm+OaiIjaCIORgzAYtR+iKOLHM5fwwYHz2JWeD5NltHaojxYPDOmCKbFdEHbtYG1RBM7uAa5kSpf6V1yVHiuvWp5ftTy/AhhKW1dBhQrQ+gAqHaBUA0oNoNRWP1fVeF5zUWlqr1OqLeUtz5Xaa9argcAeQGC31tWZiMhFMRg5CINR+5RbVIFPDmXh49RsFJbqAQBKhYBRN4Zh2i3RuKV7A4O162My2gelWiHqSj3brgAmQxueXTP4dwG6D5eWbncAXsHy1IOIqI0xGDkIg1H7Zqgy4+tT0mDt1HOXbet7hnpjmmWwto9O7dhKiCJgrJBCUmWxFJJqLlWG2uvs1hsBk776eZX+mvXG2vsYK6RbpJir7OsSFiONp+o+HOgSB2ibMGmmnCquAheOAhfSgNzjUndkl1ip7kHXsWuSqANjMHIQBqOO49e8Ynxw4Dy2Hs1BuUEarO2lUWLizZ0wLiYSfTv7Ne32I+5CXwqc/xHI3Ct1F+aftN+uUAOdB1talO4AOg2UuuDkYiiTws+FNCAnTQpEl8/UX94zCIi6RQpKUbdIUy9w8k6iDoPByEEYjDqekkojtqTl4IOD53G6oHrckEIAeoX7on+UPwZ08cfNXfzRPdgbCkU7aZUovVgdks7uBYqy7LdrvIHoYVKXW/fhQGhvx7XIVBmkoHYhDcixtAhd/BUQzbXLBkQDkQOkpbIIyDoE5PxkmXKhBqUW6HQz0OUWKShFDQE8Ax1TfyKSHYORgzAYdVyiKOLA2UvYfDgbhzMv40JRZa0yPjqVFJSi/DGgSwD6R/kjwEsjQ23bmChKA87PWoJS5vfVN9+18gqt7nbrdgfgH9Wy9zKbgIsZNVqC0oD8U3WPu/KJACJvlkJQpwHS87rCTZUByP0ZyD4IZFmW8sLa5UJuqA5KXW6RQha734jaBQYjB2EwIqu8okocy76Co9lXcTTrKo7/cRWVxtotGNFBnraQNKCLP24I94VG1ci921yd2Qzkn6gOSud/BKoq7MsE9qjudou+re7AIorA5bNSN5g1BOX+DBjLa5f1CKgRgm6Wnrd0ok1RlO6Pl30QyDogtSpd+r12Oe8wIMoyRqlLLBDeV97uQyJqMQYjB2EwovpUmczIyC/B0SwpKB3LvoIzNeZJstKqFLipk5+tVWlAF39E+Omaf9WbK6nSA38cru52yzkCiKYaBQQgop8UlMJuBArSpRB04ajU3XUtjTcQ0d/SCmRpCXJ0601ZIZB9qDooXTgKmI32ZdSe0tgqa1DqPATQ8fcAUZu4fBb4fTdwehfQcyQw5NE2PTyDkYMwGFFzFJUbceyPqziadQXHLC1LRRXGWuVCfbQY0KW6+61vZz94atx4YHdlMXB+f3VQuphef1mlFgiPqW4FihwABPeUf8ZwY4UUjqxBKftg7RAnKIDQG6WQFNFPOo+Q3oC6kZsVk/PpS6UrL7V+gHcIoPVlN6ncjBXAuf1SEPp9l/3FE93uAGZsa9O3YzByEAYjag1RFJFZWGYLSUezryA9t8Q2uaSVUiGgV5gPYjr5IdxPhxAfrW0J9dEi2FsLndqNbjVSkicFpMy9QOHvQOgN1SEotI80KaWrM5ulP6xZB6rHKV09X7ucoARCekkhybqExQBeQc6vc0dWkl9jTNkB6QrGmq2YKh3gHSp1l3qHAV4hluehNR4tz9Ue8p1He3PpDHB6txSEzv1gf1GEQiWN7+sZL7UYhd3Ypm/NYOQgDEbU1ioMJpy8UGTXqpRbx8Dua/nqVDUCkw4h3tXBqWaQCvTUtJ8r5VxNca70xzf7sDTmKu+ENEFnXXw7XROWbgICugEKFxlvVmUASvOkcyrOkebRCoiWWsB8wl27dUUUpcBtC60HpIsFruUVKv0h1hc37/ha34aDk/XRK4Rj0K5lrADO7ZOC0OldUndZTT6RUhC67i6pq92BXdMMRg7CYETOYB3Y/WteCS6W6FFQosdF61Kqh6GqjsvU66FUCAjy0lSHJW8tQn21liBV3RoV5K2Bj1bl3mOd5CaKUqjIs4SkvOPS45VzdZfXeEsBqWZgCu3T9l1xlcVA8QWg5IIl+NR4XnJBel12sf79dX7SFXshvaSgFNJLmp7BJ0KewGS9ytAahLIPAuWXrikkSJ9tl1uqF7/O0iZDOVBWAJQWAKX5lscaz8tqPL92mofGeARKIck3QgqW/l2lxwDLo0dA68/f1V06Ux2Ezu2r3SrUJQ7oeZcUhhw5zcc1GIwchMGI5CaKIoorq+yCUkFxJS6W1ghPJXoUlupxqcyA5vwUqxQCArw0CPLSIMBTg0AvaQnw0iDQU23ZpkWAl1pa76lxry49uVQWS1MO1AxLBenSTOTXEpRA8PX2YSk8pu7bs5hNUqAptoSbktwajzmW4JPb9Pv2KdRS2PGNlP7nfjlT+h++3UD6GrS+lrB0g7SEWh59O7XtH7vKIqlVzhqE6pqXSqUDOg2yhKA4IGqwFOhaQxSl1qVrg1Npfu1gVXax9szxddH6VYck66N/tOUxyj0nHTWUS91iv++Susmuba3z7VyjVegO6R6QMmAwchAGI3InVSYzLpcZarU4XbzmdUFxJcoM9fzxa4SXRikFJ+viqbF7HeCpQZB3ddDy81BDya496dYshb/bh6W8E7Xnh7LyiQTCb5ICgC0A5dUfWq6l9ZNaMXwipOBiex4pLT6R0uzg13btVemBS6elIHcxQxpIfzFDahWo7701PpZWJUtQsrYy+XVuWmAqyrEfy5V/EsA1f448Ai1XB1qCUEQ/eceqmc1SN2ppvrQU50gthVfOWx7PSWGqQYL0b1FXS1NAtNQS5QotuqIofSdsrUL77UO+Qg10jZOCUM+7pO+AC9SbwchBGIyovao0mnCl3IDLZdXLFevzcgOulBmrt5VL26rMzf8VoRBg1xoV7K1FoJcUnoK8NAj00tqeB3lr4e+h7jhjpERRCj3XhqVrx2XUJCikP5jWkGMXdmqsa+v73FXppXBkDUrW4HT5TP0tJxrv2t1xIb2kK8ZqBqFrZ1kHgMDu1UEo6hbpykUX+GPbLIYy4GqWfVi6er46QBlrT+9hR6WzBKau1eHJv4u0XhAsn4cgfScE66Oi9jrbazShjPU1pCBvvYLs2gsP/KKA6+KlINTtdtlahRrCYOQgDEZEElEUUaKvwuXS6qB0yRqmymuEqjIDrpQbcalUj+LKJnQ1XMMapIK8NZYApbUEqOrnQV7W7e00SOlLqrvizCb74OMdBihdaGqHKoMUjmwtTL9Ky6XTTetqAqTuxIi+9kHIJ8yx9ZabKEpzadmCUmZ1gLp6Hij6o+5b4MhFoQa6Dq0eKxTSy+WDKoORgzAYEbWc0WSubpUqNaCwzIDLpXpcLrM+N+BSmTQ26nKZAVfLa8/51BilQkCApzQGylenhodGCQ+1Ep4aJTw0KnhqpOc6yzrr+uoylke19bm0jd1/rWQyWlqYfq1eCiyBSamRxgRZg1CnQW3fwuXuTEYpHF3b0lSUI01EKpqlcCWKlgAl1lhX83WNcrXW1bFfzXKeQcB1I6Qg1O12t/s3YjByEAYjIuexBqlLpVJQKrSEKOm5AZfL9LZtl8oMdU6e2VY0KoUUpCyByUOjhKdaZQteSoUAQZCCmUKwLrCsr35e1zalAjXWS9sUlrL22wC1SgGtSgmtSgGtSgGd2vJcXb1Oq1ZCV2OdyvI+LslUJbU0yD2hJ7V7bfH324XaZomoI1IrFQj10SHUp2mXrxtNZluX3qVSA0r1RpQbTCg3mFBhMKHCaH1eJa03mlBp2V5urF5faS1nNNmu6jNUmWGoMuMqHBe+HEUhQApTagV0lkdtzYBlt15pC1xeWiW8tWr46FTw0angrbUsOhV8tGp4W9a16t5/rtQNSNQIfluJyK2olQqE+uoQ6ts28wCJoohKoxnl1wQmKTRV2QJXpdEEswiYzCLMonWB9GgWbdtEUYSpgW1mETCJlnLmOsqJIqpMZlQazdBXmaCvMkNveS6tq15fc64rswhUGKWgBwcEO41KAR9LYLKGJ2uQ8tFVB6j6wpWHWgmNSgGNSgG1UoBGqXDdFi7q0BiMiKhDEwTB1m3mbjf0MJtFGEzVwckammyh6pogVWm0D1oVRhPK9CaUVFahVG9Eqb4KpZVVKLE8luqlYAhIrWmXqqSWuraiUdYISrbQpIBGKbV22V6rpHVqlQJa6+trttV8VCkFqBUKKBWC9FwpdTWqldI2paL2OpVCel2zvEop1U2pkI7XlAH/tvBrloKvSRRhMkmPVWYzzGagymyu3m4pU2WSwnaVWQrJdo+iWGO8nDSGzksjdfG2qiWP6sRgRETkphQKATqF0jIBp2NuU1FlMqPMYKoOTZVGu+BkH6SkcFVSY5v1dYXRVOt+gQaTGQaTC12F1QiFACksKaSwpFAINUJPddBxJrVSsFxUoIKntu7w5HXNRQmeGhW8tNK4OS+ttYzKdmGCzjJ+TaXsmKGLwYiIiOqlUirg56GAn0frg5fJLMJoklqxjCazbUyXwWT/2PRtouXRZNkm2spUWVpbqkxmVJlEGM2WR5PUWlNlqUuVSWrJkcqKtn3rCjhm0TIOrYXnLwjSzPNKhQClYHlUCFAqFFAqAJVCAYX1UZAerQP+K41Sl26ZpWvXGiiNJhFGU1WLpslojFopSCFJrYROrYCH7bm0eKgVlhAlBSptjTIeln3sy9sfx1enhp+n6913jsGIiIicQgoBSre4xYy1G8tkrg5VVSYzjNawZRkzprR0wSlrLoIApfKa8CMIbTr3ltFktoyFs1xkoLc8N9Z4br3owPZceizTS+PnyvTWsFVle6w0mmu8hxS6ShwQugBgXEwE1ky92SHHbg0GIyIiomsoFAI0liDjAdcLcuo2bMmryWwWbePRKqusFx6YUWE0QW8Z3F9plLZLz62LucZrs219he3RDL3dOjM8NK73uQIMRkRERGShUFRfjNBRdcyRVURERER1cIlgtGbNGkRHR0On0yE2Nhapqan1lh0+fDgEQai1jBs3DgBgNBrxzDPPICYmBl5eXoiMjMT06dNx4cIFZ50OERERuSnZg9HmzZuRmJiI5cuXIy0tDf369cOoUaNQUFBQZ/ktW7YgNzfXtpw8eRJKpRL33XcfAKC8vBxpaWlYunQp0tLSsGXLFmRkZODPf/6zM0+LiIiI3JDs90qLjY3F4MGD8cYbbwAAzGYzoqKiMH/+fCxatKjR/VevXo1ly5YhNzcXXl5edZY5fPgwhgwZgvPnz6NLly6NHpP3SiMiInI/bfH3W9YWI4PBgCNHjiA+Pt62TqFQID4+HgcOHGjSMZKTkzF58uR6QxEAFBUVQRAE+Pv717ldr9ejuLjYbiEiIqKOR9ZgVFhYCJPJhLCwMLv1YWFhyMvLa3T/1NRUnDx5ErNmzaq3TGVlJZ555hk88MAD9abHpKQk+Pn52ZaoqKjmnQgRERG1C7KPMWqN5ORkxMTEYMiQIXVuNxqNuP/++yGKItauXVvvcRYvXoyioiLbkp2d7agqExERkQuTdR6j4OBgKJVK5Ofn263Pz89HeHh4g/uWlZVh06ZNePHFF+vcbg1F58+fx7fffttgX6NWq4VWq23+CRAREVG7ImuLkUajwcCBA5GSkmJbZzabkZKSgri4uAb3/fTTT6HX6/Hggw/W2mYNRb///jt2796NoCB3u2c2ERERyUH2ma8TExMxY8YMDBo0CEOGDMHq1atRVlaGmTNnAgCmT5+OTp06ISkpyW6/5ORkTJgwoVboMRqN+Mtf/oK0tDR8+eWXMJlMtvFKgYGB0Gg0zjkxIiIicjuyB6NJkybh4sWLWLZsGfLy8tC/f3/s3LnTNiA7KysLCoV9w1ZGRgb27duHb775ptbxcnJysG3bNgBA//797bZ99913GD58uEPOg4iIiNyf7PMYuSLOY0REROR+3H4eIyIiIiJXwmBEREREZCH7GCNXZO1d5AzYRERE7sP6d7s1o4QYjOpQUlICAJwBm4iIyA2VlJTAz8+vRfty8HUdzGYzLly4AB8fHwiC0KbHLi4uRlRUFLKzszv0wG5+DhJ+DtX4WUj4OUj4OVTjZyFpyucgiiJKSkoQGRlZ64r2pmKLUR0UCgU6d+7s0Pfw9fXt0F9wK34OEn4O1fhZSPg5SPg5VONnIWnsc2hpS5EVB18TERERWTAYEREREVkwGDmZVqvF8uXLO/xNa/k5SPg5VONnIeHnIOHnUI2fhcRZnwMHXxMRERFZsMWIiIiIyILBiIiIiMiCwYiIiIjIgsGIiIiIyILByAHWrFmD6Oho6HQ6xMbGIjU1tcHyn376KW644QbodDrExMRgx44dTqqpYyQlJWHw4MHw8fFBaGgoJkyYgIyMjAb32bhxIwRBsFt0Op2TauwYzz//fK1zuuGGGxrcp719F6yio6NrfRaCIGDu3Ll1lm8v34fvv/8eCQkJiIyMhCAI+Pzzz+22i6KIZcuWISIiAh4eHoiPj8fvv//e6HGb+ztGbg19DkajEc888wxiYmLg5eWFyMhITJ8+HRcuXGjwmC35+XIFjX0nHnrooVrnNXr06EaP256+EwDq/H0hCAL+8Y9/1HvMtvpOMBi1sc2bNyMxMRHLly9HWloa+vXrh1GjRqGgoKDO8j/++CMeeOABPPLIIzh69CgmTJiACRMm4OTJk06uedvZu3cv5s6di4MHD2LXrl0wGo0YOXIkysrKGtzP19cXubm5tuX8+fNOqrHj3HjjjXbntG/fvnrLtsfvgtXhw4ftPoddu3YBAO67775692kP34eysjL069cPa9asqXP7q6++iv/85z9Yt24dDh06BC8vL4waNQqVlZX1HrO5v2NcQUOfQ3l5OdLS0rB06VKkpaVhy5YtyMjIwJ///OdGj9ucny9X0dh3AgBGjx5td16ffPJJg8dsb98JAHbnn5ubiw0bNkAQBNx7770NHrdNvhMitakhQ4aIc+fOtb02mUxiZGSkmJSUVGf5+++/Xxw3bpzdutjYWPGvf/2rQ+vpTAUFBSIAce/evfWWeffdd0U/Pz/nVcoJli9fLvbr16/J5TvCd8HqiSeeEHv06CGazeY6t7fH7wMAcevWrbbXZrNZDA8PF//xj3/Y1l29elXUarXiJ598Uu9xmvs7xtVc+znUJTU1VQQgnj9/vt4yzf35ckV1fRYzZswQx48f36zjdITvxPjx48U777yzwTJt9Z1gi1EbMhgMOHLkCOLj423rFAoF4uPjceDAgTr3OXDggF15ABg1alS95d1RUVERACAwMLDBcqWlpejatSuioqIwfvx4nDp1yhnVc6jff/8dkZGR6N69O6ZOnYqsrKx6y3aE7wIg/Zx8+OGHePjhhxu8SXN7/D7UlJmZiby8PLt/cz8/P8TGxtb7b96S3zHuqKioCIIgwN/fv8Fyzfn5cid79uxBaGgoevXqhTlz5uDSpUv1lu0I34n8/Hxs374djzzySKNl2+I7wWDUhgoLC2EymRAWFma3PiwsDHl5eXXuk5eX16zy7sZsNmPBggW49dZbcdNNN9VbrlevXtiwYQO++OILfPjhhzCbzRg6dCj++OMPJ9a2bcXGxmLjxo3YuXMn1q5di8zMTNx2220oKSmps3x7/y5Yff7557h69Soeeuihesu0x+/Dtaz/rs35N2/J7xh3U1lZiWeeeQYPPPBAgzcKbe7Pl7sYPXo03n//faSkpOCVV17B3r17MWbMGJhMpjrLd4TvxHvvvQcfHx/cc889DZZrq++EqjWVJWrM3LlzcfLkyUb7eePi4hAXF2d7PXToUPTu3RtvvfUWXnrpJUdX0yHGjBlje963b1/Exsaia9eu+O9//9uk//m0V8nJyRgzZgwiIyPrLdMevw/UOKPRiPvvvx+iKGLt2rUNlm2vP1+TJ0+2PY+JiUHfvn3Ro0cP7NmzByNGjJCxZvLZsGEDpk6d2ugFGG31nWCLURsKDg6GUqlEfn6+3fr8/HyEh4fXuU94eHizyruTefPm4csvv8R3332Hzp07N2tftVqNAQMG4PTp0w6qnfP5+/vj+uuvr/ec2vN3wer8+fPYvXs3Zs2a1az92uP3wfrv2px/85b8jnEX1lB0/vx57Nq1q8HWoro09vPlrrp3747g4OB6z6s9fycA4IcffkBGRkazf2cALf9OMBi1IY1Gg4EDByIlJcW2zmw2IyUlxe5/vzXFxcXZlQeAXbt21VveHYiiiHnz5mHr1q349ttv0a1bt2Yfw2Qy4cSJE4iIiHBADeVRWlqKM2fO1HtO7fG7cK13330XoaGhGDduXLP2a4/fh27duiE8PNzu37y4uBiHDh2q99+8Jb9j3IE1FP3+++/YvXs3goKCmn2Mxn6+3NUff/yBS5cu1Xte7fU7YZWcnIyBAweiX79+zd63xd+JVg/fJjubNm0StVqtuHHjRvGXX34RZ8+eLfr7+4t5eXmiKIritGnTxEWLFtnK79+/X1SpVOJrr70mpqeni8uXLxfVarV44sQJuU6h1ebMmSP6+fmJe/bsEXNzc21LeXm5rcy1n8MLL7wgfv311+KZM2fEI0eOiJMnTxZ1Op146tQpOU6hTTz11FPinj17xMzMTHH//v1ifHy8GBwcLBYUFIii2DG+CzWZTCaxS5cu4jPPPFNrW3v9PpSUlIhHjx4Vjx49KgIQV61aJR49etR2tdXLL78s+vv7i1988YV4/Phxcfz48WK3bt3EiooK2zHuvPNO8fXXX7e9bux3jCtq6HMwGAzin//8Z7Fz587isWPH7H5n6PV62zGu/Rwa+/lyVQ19FiUlJeLChQvFAwcOiJmZmeLu3bvFm2++WezZs6dYWVlpO0Z7/05YFRUViZ6enuLatWvrPIajvhMMRg7w+uuvi126dBE1Go04ZMgQ8eDBg7Ztd9xxhzhjxgy78v/973/F66+/XtRoNOKNN94obt++3ck1blsA6lzeffddW5lrP4cFCxbYPrOwsDBx7NixYlpamvMr34YmTZokRkREiBqNRuzUqZM4adIk8fTp07btHeG7UNPXX38tAhAzMjJqbWuv34fvvvuuzp8F67mazWZx6dKlYlhYmKjVasURI0bU+ny6du0qLl++3G5dQ79jXFFDn0NmZma9vzO+++472zGu/Rwa+/lyVQ19FuXl5eLIkSPFkJAQUa1Wi127dhUfffTRWgGnvX8nrN566y3Rw8NDvHr1ap3HcNR3QhBFUWx2+xQRERFRO8QxRkREREQWDEZEREREFgxGRERERBYMRkREREQWDEZEREREFgxGRERERBYMRkREREQWDEZERPUQBAGff/653NUgIidiMCIil/TQQw9BEIRay+jRo+WuGhG1Yyq5K0BEVJ/Ro0fj3XfftVun1Wplqg0RdQRsMSIil6XVahEeHm63BAQEAJC6udauXYsxY8bAw8MD3bt3x//93//Z7X/ixAnceeed8PDwQFBQEGbPno3S0lK7Mhs2bMCNN94IrVaLiIgIzJs3z257YWEhJk6cCE9PT/Ts2RPbtm2zbbty5QqmTp2KkJAQeHh4oGfPnrWCHBG5FwYjInJbS5cuxb333ouff/4ZU6dOxeTJk5Geng4AKCsrw6hRoxAQEIDDhw/j008/xe7du+2Cz9q1azF37lzMnj0bJ06cwLZt23DdddfZvccLL7yA+++/H8ePH8fYsWMxdepUXL582fb+v/zyC7766iukp6dj7dq1CA4Odt4HQERtr9m3nSUicoIZM2aISqVS9PLysltWrFghiqIoAhAfe+wxu31iY2PFOXPmiKIoim+//bYYEBAglpaW2rZv375dVCgUtruVR0ZGis8991y9dQAgLlmyxPa6tLRUBCB+9dVXoiiKYkJCgjhz5sy2OWEicgkcY0RELutPf/oT1q5da7cuMDDQ9jwuLs5uW1xcHI4dOwYASE9PR79+/eDl5WXbfuutt8JsNiMjIwOCIODChQsYMWJEg3Xo27ev7bmXlxd8fX1RUFAAAJgzZw7uvfdepKWlYeTIkZgwYQKGDh3aonMlItfAYERELsvLy6tW11Zb8fDwaFI5tVpt91oQBJjNZgDAmDFjcP78eezYsQO7du3CiBEjMHfuXLz22mttXl8icg6OMSIit3Xw4MFar3v37g0A6N27N37++WeUlZXZtu/fvx8KhQK9evWCj48PoqOjkZKS0qo6hISEYMaMGfjwww+xevVqvP322606HhHJiy1GROSy9Ho98vLy7NapVCrbAOdPP/0UgwYNwrBhw/DRRx8hNTUVycnJAICpU6di+fLlmDFjBp5//nlcvHgR8+fPx7Rp0xAWFgYAeP755/HYY48hNDQUY8aMQUlJCfbv34/58+c3qX7Lli3DwIEDceONN0Kv1+PLL7+0BTMick8MRkTksnbu3ImIiAi7db169cKvv/4KQLpibNOmTXj88ccRERGBTz75BH369AEAeHp64uuvv8YTTzyBwYMHw9PTE/feey9WrVplO9aMGTNQWVmJf/3rX1i4cCGCg4Pxl7/8pcn102g0WLx4Mc6dOwcPDw/cdttt2LRpUxucORHJRRBFUZS7EkREzSUIArZu3YoJEybIXRUiakc4xoiIiIjIgsGIiIiIyIJjjIjILXEUABE5AluMiIiIiCwYjIiIiIgsGIyIiIiILBiMiIiIiCwYjIiIiIgsGIyIiIiILBiMiIiIiCwYjIiIiIgsGIyIiIiILP4/TMYY7k+7nHwAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "def accuracy_from_floats(y_true, y_pred):\n",
        "  y_pred = np.around(y_pred).astype(int)\n",
        "  return accuracy_score(y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "reg_predictions = utils.test(reg_model, r_test_iter)\n",
        "reg_predictions = y_scaler.inverse_transform(reg_predictions)\n",
        "\n",
        "\n",
        "print(\"Accuracy: \",  accuracy_from_floats(y_true=y_test, y_pred=reg_predictions),\n",
        "      \", MSE: \",mse(y_true=y_test, y_pred=reg_predictions),\n",
        "      \", MAE\", mae(y_true=y_test, y_pred=reg_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0dJCRUzl9xo",
        "outputId": "0ac27ff5-853c-4b0b-a05d-ff8f2c850ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.5275510204081633 , MSE:  0.5734203207325338 , MAE 0.5884605476076957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDLfQ5hx3bb7"
      },
      "source": [
        "**Task 4.** Add an additional, \"hidden\" dense layer with the ReLU activation and 30 input/output neurons. Is the result better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6SsPYpBuFcM",
        "outputId": "4b631de9-0118-4c5e-cfd9-d18c636a5c6f"
      },
      "source": [
        "class RegModel2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(RegModel2, self).__init__()\n",
        "    self.layer = nn.Sequential(nn.Linear(in_features=11, out_features=30),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(in_features=30, out_features=1))\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "reg_model = RegModel2()\n",
        "reg_model = reg_model.cuda()\n",
        "\n",
        "history = utils.train(model=reg_model,\n",
        "              loss=nn.MSELoss(),\n",
        "              val_metrics={\"mse\": nn.MSELoss()},\n",
        "              optimizer=torch.optim.SGD(reg_model.parameters(), lr=0.01),\n",
        "              train_ds=r_train_iter,\n",
        "              dev_ds=r_dev_iter,\n",
        "              num_epochs=200,\n",
        "              early_stopper=utils.EarlyStopper(metric_name=\"mse\", patience=5))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========\n",
            "epoch 1 train loss: 0.9171 val_mse: 0.8401\n",
            "tensor(0.8401) None\n",
            "=========\n",
            "epoch 2 train loss: 0.8237 val_mse: 0.7773\n",
            "tensor(0.7773) tensor(0.8401)\n",
            "=========\n",
            "epoch 3 train loss: 0.7720 val_mse: 0.7394\n",
            "tensor(0.7394) tensor(0.7773)\n",
            "=========\n",
            "epoch 4 train loss: 0.7400 val_mse: 0.7203\n",
            "tensor(0.7203) tensor(0.7394)\n",
            "=========\n",
            "epoch 5 train loss: 0.7190 val_mse: 0.7129\n",
            "tensor(0.7129) tensor(0.7203)\n",
            "=========\n",
            "epoch 6 train loss: 0.7067 val_mse: 0.7038\n",
            "tensor(0.7038) tensor(0.7129)\n",
            "=========\n",
            "epoch 7 train loss: 0.6966 val_mse: 0.6968\n",
            "tensor(0.6968) tensor(0.7038)\n",
            "=========\n",
            "epoch 8 train loss: 0.6895 val_mse: 0.6942\n",
            "tensor(0.6942) tensor(0.6968)\n",
            "=========\n",
            "epoch 9 train loss: 0.6847 val_mse: 0.6887\n",
            "tensor(0.6887) tensor(0.6942)\n",
            "=========\n",
            "epoch 10 train loss: 0.6789 val_mse: 0.6847\n",
            "tensor(0.6847) tensor(0.6887)\n",
            "=========\n",
            "epoch 11 train loss: 0.6752 val_mse: 0.6840\n",
            "tensor(0.6840) tensor(0.6847)\n",
            "=========\n",
            "epoch 12 train loss: 0.6718 val_mse: 0.6787\n",
            "tensor(0.6787) tensor(0.6840)\n",
            "=========\n",
            "epoch 13 train loss: 0.6682 val_mse: 0.6758\n",
            "tensor(0.6758) tensor(0.6787)\n",
            "=========\n",
            "epoch 14 train loss: 0.6652 val_mse: 0.6738\n",
            "tensor(0.6738) tensor(0.6758)\n",
            "=========\n",
            "epoch 15 train loss: 0.6623 val_mse: 0.6734\n",
            "tensor(0.6734) tensor(0.6738)\n",
            "=========\n",
            "epoch 16 train loss: 0.6599 val_mse: 0.6707\n",
            "tensor(0.6707) tensor(0.6734)\n",
            "=========\n",
            "epoch 17 train loss: 0.6573 val_mse: 0.6687\n",
            "tensor(0.6687) tensor(0.6707)\n",
            "=========\n",
            "epoch 18 train loss: 0.6547 val_mse: 0.6671\n",
            "tensor(0.6671) tensor(0.6687)\n",
            "=========\n",
            "epoch 19 train loss: 0.6525 val_mse: 0.6664\n",
            "tensor(0.6664) tensor(0.6671)\n",
            "=========\n",
            "epoch 20 train loss: 0.6500 val_mse: 0.6655\n",
            "tensor(0.6655) tensor(0.6664)\n",
            "=========\n",
            "epoch 21 train loss: 0.6482 val_mse: 0.6643\n",
            "tensor(0.6643) tensor(0.6655)\n",
            "=========\n",
            "epoch 22 train loss: 0.6463 val_mse: 0.6636\n",
            "tensor(0.6636) tensor(0.6643)\n",
            "=========\n",
            "epoch 23 train loss: 0.6446 val_mse: 0.6630\n",
            "tensor(0.6630) tensor(0.6636)\n",
            "=========\n",
            "epoch 24 train loss: 0.6426 val_mse: 0.6603\n",
            "tensor(0.6603) tensor(0.6630)\n",
            "=========\n",
            "epoch 25 train loss: 0.6408 val_mse: 0.6602\n",
            "tensor(0.6602) tensor(0.6603)\n",
            "=========\n",
            "epoch 26 train loss: 0.6395 val_mse: 0.6603\n",
            "tensor(0.6603) tensor(0.6602)\n",
            "=========\n",
            "epoch 27 train loss: 0.6380 val_mse: 0.6601\n",
            "tensor(0.6601) tensor(0.6602)\n",
            "=========\n",
            "epoch 28 train loss: 0.6365 val_mse: 0.6577\n",
            "tensor(0.6577) tensor(0.6601)\n",
            "=========\n",
            "epoch 29 train loss: 0.6352 val_mse: 0.6576\n",
            "tensor(0.6576) tensor(0.6577)\n",
            "=========\n",
            "epoch 30 train loss: 0.6333 val_mse: 0.6577\n",
            "tensor(0.6577) tensor(0.6576)\n",
            "=========\n",
            "epoch 31 train loss: 0.6321 val_mse: 0.6569\n",
            "tensor(0.6569) tensor(0.6576)\n",
            "=========\n",
            "epoch 32 train loss: 0.6315 val_mse: 0.6562\n",
            "tensor(0.6562) tensor(0.6569)\n",
            "=========\n",
            "epoch 33 train loss: 0.6303 val_mse: 0.6555\n",
            "tensor(0.6555) tensor(0.6562)\n",
            "=========\n",
            "epoch 34 train loss: 0.6290 val_mse: 0.6555\n",
            "tensor(0.6555) tensor(0.6555)\n",
            "=========\n",
            "epoch 35 train loss: 0.6278 val_mse: 0.6541\n",
            "tensor(0.6541) tensor(0.6555)\n",
            "=========\n",
            "epoch 36 train loss: 0.6265 val_mse: 0.6561\n",
            "tensor(0.6561) tensor(0.6541)\n",
            "=========\n",
            "epoch 37 train loss: 0.6263 val_mse: 0.6549\n",
            "tensor(0.6549) tensor(0.6541)\n",
            "=========\n",
            "epoch 38 train loss: 0.6249 val_mse: 0.6540\n",
            "tensor(0.6540) tensor(0.6541)\n",
            "=========\n",
            "epoch 39 train loss: 0.6240 val_mse: 0.6531\n",
            "tensor(0.6531) tensor(0.6540)\n",
            "=========\n",
            "epoch 40 train loss: 0.6235 val_mse: 0.6532\n",
            "tensor(0.6532) tensor(0.6531)\n",
            "=========\n",
            "epoch 41 train loss: 0.6229 val_mse: 0.6528\n",
            "tensor(0.6528) tensor(0.6531)\n",
            "=========\n",
            "epoch 42 train loss: 0.6214 val_mse: 0.6528\n",
            "tensor(0.6528) tensor(0.6528)\n",
            "=========\n",
            "epoch 43 train loss: 0.6213 val_mse: 0.6532\n",
            "tensor(0.6532) tensor(0.6528)\n",
            "=========\n",
            "epoch 44 train loss: 0.6206 val_mse: 0.6528\n",
            "tensor(0.6528) tensor(0.6528)\n",
            "=========\n",
            "epoch 45 train loss: 0.6194 val_mse: 0.6524\n",
            "tensor(0.6524) tensor(0.6528)\n",
            "=========\n",
            "epoch 46 train loss: 0.6186 val_mse: 0.6529\n",
            "tensor(0.6529) tensor(0.6524)\n",
            "=========\n",
            "epoch 47 train loss: 0.6181 val_mse: 0.6521\n",
            "tensor(0.6521) tensor(0.6524)\n",
            "=========\n",
            "epoch 48 train loss: 0.6170 val_mse: 0.6524\n",
            "tensor(0.6524) tensor(0.6521)\n",
            "=========\n",
            "epoch 49 train loss: 0.6168 val_mse: 0.6517\n",
            "tensor(0.6517) tensor(0.6521)\n",
            "=========\n",
            "epoch 50 train loss: 0.6160 val_mse: 0.6516\n",
            "tensor(0.6516) tensor(0.6517)\n",
            "=========\n",
            "epoch 51 train loss: 0.6145 val_mse: 0.6505\n",
            "tensor(0.6505) tensor(0.6516)\n",
            "=========\n",
            "epoch 52 train loss: 0.6138 val_mse: 0.6517\n",
            "tensor(0.6517) tensor(0.6505)\n",
            "=========\n",
            "epoch 53 train loss: 0.6140 val_mse: 0.6506\n",
            "tensor(0.6506) tensor(0.6505)\n",
            "=========\n",
            "epoch 54 train loss: 0.6126 val_mse: 0.6503\n",
            "tensor(0.6503) tensor(0.6505)\n",
            "=========\n",
            "epoch 55 train loss: 0.6121 val_mse: 0.6516\n",
            "tensor(0.6516) tensor(0.6503)\n",
            "=========\n",
            "epoch 56 train loss: 0.6114 val_mse: 0.6518\n",
            "tensor(0.6518) tensor(0.6503)\n",
            "=========\n",
            "epoch 57 train loss: 0.6108 val_mse: 0.6506\n",
            "tensor(0.6506) tensor(0.6503)\n",
            "=========\n",
            "epoch 58 train loss: 0.6103 val_mse: 0.6506\n",
            "tensor(0.6506) tensor(0.6503)\n",
            "=========\n",
            "epoch 59 train loss: 0.6097 val_mse: 0.6507\n",
            "tensor(0.6507) tensor(0.6503)\n",
            "EARLY STOPPING \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/utils.py:353: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "cksuuZUmvh3U",
        "outputId": "898fdf8b-bb82-4b3d-a7be-2eecb8254e33"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history[\"train_loss\"], label='train')\n",
        "plt.plot(history[\"val_mse\"], label='val')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE');\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ef0f1bf6050>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_predictions = utils.test(reg_model, r_test_iter)\n",
        "reg_predictions = y_scaler.inverse_transform(reg_predictions)\n",
        "\n",
        "\n",
        "print(\"Accuracy: \",  accuracy_from_floats(y_true=y_test, y_pred=reg_predictions),\n",
        "      \", MSE: \",mse(y_true=y_test, y_pred=reg_predictions),\n",
        "      \", MAE\", mae(y_true=y_test, y_pred=reg_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBoVfs45nQng",
        "outputId": "bdf085c5-74d5-4284-b71c-d0238d29f5fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.5510204081632653 , MSE:  0.5029345852758415 , MAE 0.5516026288710867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFPwnZcM49NS"
      },
      "source": [
        "**Task 5.** Add an additional (*second*) hidden layer with the ReLU activation and input/output 30 neurons. Is the result better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PzOKXALzyT-",
        "outputId": "50431a6f-32a5-40e8-8ae3-7af20f8a7d07"
      },
      "source": [
        "class RegModel3(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(RegModel3, self).__init__()\n",
        "    self.layer = nn.Sequential(nn.Linear(in_features=11, out_features=30),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(in_features=30, out_features=30),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(in_features=30, out_features=1))\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "\n",
        "\n",
        "reg_model = RegModel3()\n",
        "reg_model = reg_model.cuda()\n",
        "\n",
        "history = utils.train(model=reg_model,\n",
        "              loss=nn.MSELoss(),\n",
        "              val_metrics={\"mse\": nn.MSELoss()},\n",
        "              optimizer=torch.optim.SGD(reg_model.parameters(), lr=0.01),\n",
        "              train_ds=r_train_iter,\n",
        "              dev_ds=r_dev_iter,\n",
        "              num_epochs=200,\n",
        "              early_stopper=utils.EarlyStopper(metric_name=\"mse\", patience=5))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========\n",
            "epoch 1 train loss: 0.9885 val_mse: 0.9421\n",
            "tensor(0.9421) None\n",
            "=========\n",
            "epoch 2 train loss: 0.9147 val_mse: 0.8761\n",
            "tensor(0.8761) tensor(0.9421)\n",
            "=========\n",
            "epoch 3 train loss: 0.8587 val_mse: 0.8224\n",
            "tensor(0.8224) tensor(0.8761)\n",
            "=========\n",
            "epoch 4 train loss: 0.8116 val_mse: 0.7834\n",
            "tensor(0.7834) tensor(0.8224)\n",
            "=========\n",
            "epoch 5 train loss: 0.7740 val_mse: 0.7536\n",
            "tensor(0.7536) tensor(0.7834)\n",
            "=========\n",
            "epoch 6 train loss: 0.7441 val_mse: 0.7327\n",
            "tensor(0.7327) tensor(0.7536)\n",
            "=========\n",
            "epoch 7 train loss: 0.7222 val_mse: 0.7189\n",
            "tensor(0.7189) tensor(0.7327)\n",
            "=========\n",
            "epoch 8 train loss: 0.7056 val_mse: 0.7089\n",
            "tensor(0.7089) tensor(0.7189)\n",
            "=========\n",
            "epoch 9 train loss: 0.6942 val_mse: 0.7013\n",
            "tensor(0.7013) tensor(0.7089)\n",
            "=========\n",
            "epoch 10 train loss: 0.6855 val_mse: 0.6968\n",
            "tensor(0.6968) tensor(0.7013)\n",
            "=========\n",
            "epoch 11 train loss: 0.6793 val_mse: 0.6911\n",
            "tensor(0.6911) tensor(0.6968)\n",
            "=========\n",
            "epoch 12 train loss: 0.6732 val_mse: 0.6882\n",
            "tensor(0.6882) tensor(0.6911)\n",
            "=========\n",
            "epoch 13 train loss: 0.6690 val_mse: 0.6848\n",
            "tensor(0.6848) tensor(0.6882)\n",
            "=========\n",
            "epoch 14 train loss: 0.6645 val_mse: 0.6819\n",
            "tensor(0.6819) tensor(0.6848)\n",
            "=========\n",
            "epoch 15 train loss: 0.6617 val_mse: 0.6784\n",
            "tensor(0.6784) tensor(0.6819)\n",
            "=========\n",
            "epoch 16 train loss: 0.6581 val_mse: 0.6767\n",
            "tensor(0.6767) tensor(0.6784)\n",
            "=========\n",
            "epoch 17 train loss: 0.6558 val_mse: 0.6750\n",
            "tensor(0.6750) tensor(0.6767)\n",
            "=========\n",
            "epoch 18 train loss: 0.6524 val_mse: 0.6730\n",
            "tensor(0.6730) tensor(0.6750)\n",
            "=========\n",
            "epoch 19 train loss: 0.6499 val_mse: 0.6710\n",
            "tensor(0.6710) tensor(0.6730)\n",
            "=========\n",
            "epoch 20 train loss: 0.6488 val_mse: 0.6700\n",
            "tensor(0.6700) tensor(0.6710)\n",
            "=========\n",
            "epoch 21 train loss: 0.6459 val_mse: 0.6687\n",
            "tensor(0.6687) tensor(0.6700)\n",
            "=========\n",
            "epoch 22 train loss: 0.6427 val_mse: 0.6732\n",
            "tensor(0.6732) tensor(0.6687)\n",
            "=========\n",
            "epoch 23 train loss: 0.6418 val_mse: 0.6697\n",
            "tensor(0.6697) tensor(0.6687)\n",
            "=========\n",
            "epoch 24 train loss: 0.6390 val_mse: 0.6666\n",
            "tensor(0.6666) tensor(0.6687)\n",
            "=========\n",
            "epoch 25 train loss: 0.6372 val_mse: 0.6668\n",
            "tensor(0.6668) tensor(0.6666)\n",
            "=========\n",
            "epoch 26 train loss: 0.6354 val_mse: 0.6660\n",
            "tensor(0.6660) tensor(0.6666)\n",
            "=========\n",
            "epoch 27 train loss: 0.6330 val_mse: 0.6662\n",
            "tensor(0.6662) tensor(0.6660)\n",
            "=========\n",
            "epoch 28 train loss: 0.6317 val_mse: 0.6667\n",
            "tensor(0.6667) tensor(0.6660)\n",
            "=========\n",
            "epoch 29 train loss: 0.6301 val_mse: 0.6648\n",
            "tensor(0.6648) tensor(0.6660)\n",
            "=========\n",
            "epoch 30 train loss: 0.6274 val_mse: 0.6625\n",
            "tensor(0.6625) tensor(0.6648)\n",
            "=========\n",
            "epoch 31 train loss: 0.6255 val_mse: 0.6627\n",
            "tensor(0.6627) tensor(0.6625)\n",
            "=========\n",
            "epoch 32 train loss: 0.6246 val_mse: 0.6631\n",
            "tensor(0.6631) tensor(0.6625)\n",
            "=========\n",
            "epoch 33 train loss: 0.6224 val_mse: 0.6622\n",
            "tensor(0.6622) tensor(0.6625)\n",
            "=========\n",
            "epoch 34 train loss: 0.6213 val_mse: 0.6620\n",
            "tensor(0.6620) tensor(0.6622)\n",
            "=========\n",
            "epoch 35 train loss: 0.6203 val_mse: 0.6614\n",
            "tensor(0.6614) tensor(0.6620)\n",
            "=========\n",
            "epoch 36 train loss: 0.6182 val_mse: 0.6641\n",
            "tensor(0.6641) tensor(0.6614)\n",
            "=========\n",
            "epoch 37 train loss: 0.6169 val_mse: 0.6628\n",
            "tensor(0.6628) tensor(0.6614)\n",
            "=========\n",
            "epoch 38 train loss: 0.6165 val_mse: 0.6609\n",
            "tensor(0.6609) tensor(0.6614)\n",
            "=========\n",
            "epoch 39 train loss: 0.6138 val_mse: 0.6637\n",
            "tensor(0.6637) tensor(0.6609)\n",
            "=========\n",
            "epoch 40 train loss: 0.6132 val_mse: 0.6627\n",
            "tensor(0.6627) tensor(0.6609)\n",
            "=========\n",
            "epoch 41 train loss: 0.6119 val_mse: 0.6629\n",
            "tensor(0.6629) tensor(0.6609)\n",
            "=========\n",
            "epoch 42 train loss: 0.6108 val_mse: 0.6621\n",
            "tensor(0.6621) tensor(0.6609)\n",
            "=========\n",
            "epoch 43 train loss: 0.6092 val_mse: 0.6616\n",
            "tensor(0.6616) tensor(0.6609)\n",
            "EARLY STOPPING \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/utils.py:353: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0TvmNxnz6u8",
        "outputId": "2c961bc2-1cc7-4b0b-8a9b-0d23fedde9ea"
      },
      "source": [
        "reg_predictions = utils.test(reg_model, r_test_iter)\n",
        "reg_predictions = y_scaler.inverse_transform(reg_predictions)\n",
        "\n",
        "\n",
        "print(\"Accuracy: \",  accuracy_from_floats(y_true=y_test, y_pred=reg_predictions),\n",
        "      \", MSE: \",mse(y_true=y_test, y_pred=reg_predictions),\n",
        "      \", MAE\", mae(y_true=y_test, y_pred=reg_predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.5459183673469388 , MSE:  0.49701865722541705 , MAE 0.5479685860307663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADUcSjeYt4kE"
      },
      "source": [
        "# Classification\n",
        "\n",
        "**Task 6.** Let's approach this dataset with a logistic regression now. This time, you will have to predict the rating levels as **classes** (10), instead of treating the output as a scalar. Make sure that your results are **reproducible** by correctly 'seeding'. Use the SGD-optimizer with a learning rate of \"0.01\" and train for 200 epochs with early stopping with patience of 5 (and restore the best weights). Early stopping should interrupt the training if the loss on the validation data  does not improve for 5 epochs. Use accuracy to evaluate the intermediary results, during training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def np2iter_class(x, y, shuffle=True):\n",
        "  x = torch.tensor(x, dtype=torch.float)\n",
        "  y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "  ds = torch.utils.data.TensorDataset(x, y)\n",
        "  return torch.utils.data.DataLoader(ds, batch_size=64, shuffle=shuffle)\n",
        "\n",
        "c_train_iter = np2iter_class(x_train, y_train, shuffle=True)\n",
        "c_dev_iter =  np2iter_class(x_dev, y_dev, shuffle=False)\n",
        "c_test_iter =  np2iter_class(x_test, y_test, shuffle=False)\n"
      ],
      "metadata": {
        "id": "SeYV2oCNojix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SgsikNJbs1Q",
        "outputId": "3b0edcdb-70cb-477f-93f3-d73dbeb5651c"
      },
      "source": [
        "class ClassModel1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ClassModel1, self).__init__()\n",
        "    self.layer = nn.Linear(in_features=11, out_features=10)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "\n",
        "\n",
        "def val_acc(y_pred, y_test):\n",
        "  y_pred = np.argmax(y_pred, axis=1)\n",
        "  return accuracy_score(y_pred=y_pred, y_true=y_test)\n",
        "\n",
        "classification_model = ClassModel1()\n",
        "classification_model = classification_model.cuda()\n",
        "\n",
        "history = utils.train(model=classification_model,\n",
        "                            loss=nn.CrossEntropyLoss(),\n",
        "                            val_metrics={\"cls\": nn.CrossEntropyLoss(), \"acc\": val_acc},\n",
        "                            optimizer=torch.optim.SGD(classification_model.parameters(), lr=0.01),\n",
        "                            train_ds=c_train_iter,\n",
        "                            dev_ds=c_dev_iter,\n",
        "                            num_epochs=200,\n",
        "                            early_stopper=utils.EarlyStopper(metric_name=\"cls\", patience=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========\n",
            "epoch 1 train loss: 2.3960 val_cls: 2.3075 val_acc: 0.1449\n",
            "tensor(2.3075) None\n",
            "=========\n",
            "epoch 2 train loss: 2.2345 val_cls: 2.1585 val_acc: 0.2204\n",
            "tensor(2.1585) tensor(2.3075)\n",
            "=========\n",
            "epoch 3 train loss: 2.0971 val_cls: 2.0324 val_acc: 0.2857\n",
            "tensor(2.0324) tensor(2.1585)\n",
            "=========\n",
            "epoch 4 train loss: 1.9805 val_cls: 1.9259 val_acc: 0.3531\n",
            "tensor(1.9259) tensor(2.0324)\n",
            "=========\n",
            "epoch 5 train loss: 1.8817 val_cls: 1.8359 val_acc: 0.4061\n",
            "tensor(1.8359) tensor(1.9259)\n",
            "=========\n",
            "epoch 6 train loss: 1.7978 val_cls: 1.7593 val_acc: 0.4378\n",
            "tensor(1.7593) tensor(1.8359)\n",
            "=========\n",
            "epoch 7 train loss: 1.7260 val_cls: 1.6940 val_acc: 0.4633\n",
            "tensor(1.6940) tensor(1.7593)\n",
            "=========\n",
            "epoch 8 train loss: 1.6645 val_cls: 1.6380 val_acc: 0.4827\n",
            "tensor(1.6380) tensor(1.6940)\n",
            "=========\n",
            "epoch 9 train loss: 1.6117 val_cls: 1.5900 val_acc: 0.5010\n",
            "tensor(1.5900) tensor(1.6380)\n",
            "=========\n",
            "epoch 10 train loss: 1.5662 val_cls: 1.5485 val_acc: 0.5143\n",
            "tensor(1.5485) tensor(1.5900)\n",
            "=========\n",
            "epoch 11 train loss: 1.5269 val_cls: 1.5126 val_acc: 0.5102\n",
            "tensor(1.5126) tensor(1.5485)\n",
            "=========\n",
            "epoch 12 train loss: 1.4928 val_cls: 1.4815 val_acc: 0.5041\n",
            "tensor(1.4815) tensor(1.5126)\n",
            "=========\n",
            "epoch 13 train loss: 1.4632 val_cls: 1.4543 val_acc: 0.5061\n",
            "tensor(1.4543) tensor(1.4815)\n",
            "=========\n",
            "epoch 14 train loss: 1.4374 val_cls: 1.4304 val_acc: 0.5071\n",
            "tensor(1.4304) tensor(1.4543)\n",
            "=========\n",
            "epoch 15 train loss: 1.4147 val_cls: 1.4094 val_acc: 0.5041\n",
            "tensor(1.4094) tensor(1.4304)\n",
            "=========\n",
            "epoch 16 train loss: 1.3948 val_cls: 1.3908 val_acc: 0.5010\n",
            "tensor(1.3908) tensor(1.4094)\n",
            "=========\n",
            "epoch 17 train loss: 1.3771 val_cls: 1.3742 val_acc: 0.5041\n",
            "tensor(1.3742) tensor(1.3908)\n",
            "=========\n",
            "epoch 18 train loss: 1.3614 val_cls: 1.3593 val_acc: 0.5020\n",
            "tensor(1.3593) tensor(1.3742)\n",
            "=========\n",
            "epoch 19 train loss: 1.3473 val_cls: 1.3459 val_acc: 0.5031\n",
            "tensor(1.3459) tensor(1.3593)\n",
            "=========\n",
            "epoch 20 train loss: 1.3346 val_cls: 1.3338 val_acc: 0.5082\n",
            "tensor(1.3338) tensor(1.3459)\n",
            "=========\n",
            "epoch 21 train loss: 1.3231 val_cls: 1.3227 val_acc: 0.5122\n",
            "tensor(1.3227) tensor(1.3338)\n",
            "=========\n",
            "epoch 22 train loss: 1.3126 val_cls: 1.3125 val_acc: 0.5102\n",
            "tensor(1.3125) tensor(1.3227)\n",
            "=========\n",
            "epoch 23 train loss: 1.3031 val_cls: 1.3032 val_acc: 0.5082\n",
            "tensor(1.3032) tensor(1.3125)\n",
            "=========\n",
            "epoch 24 train loss: 1.2943 val_cls: 1.2946 val_acc: 0.5092\n",
            "tensor(1.2946) tensor(1.3032)\n",
            "=========\n",
            "epoch 25 train loss: 1.2862 val_cls: 1.2866 val_acc: 0.5102\n",
            "tensor(1.2866) tensor(1.2946)\n",
            "=========\n",
            "epoch 26 train loss: 1.2787 val_cls: 1.2792 val_acc: 0.5112\n",
            "tensor(1.2792) tensor(1.2866)\n",
            "=========\n",
            "epoch 27 train loss: 1.2717 val_cls: 1.2723 val_acc: 0.5102\n",
            "tensor(1.2723) tensor(1.2792)\n",
            "=========\n",
            "epoch 28 train loss: 1.2653 val_cls: 1.2658 val_acc: 0.5133\n",
            "tensor(1.2658) tensor(1.2723)\n",
            "=========\n",
            "epoch 29 train loss: 1.2592 val_cls: 1.2598 val_acc: 0.5133\n",
            "tensor(1.2598) tensor(1.2658)\n",
            "=========\n",
            "epoch 30 train loss: 1.2536 val_cls: 1.2541 val_acc: 0.5112\n",
            "tensor(1.2541) tensor(1.2598)\n",
            "=========\n",
            "epoch 31 train loss: 1.2482 val_cls: 1.2488 val_acc: 0.5112\n",
            "tensor(1.2488) tensor(1.2541)\n",
            "=========\n",
            "epoch 32 train loss: 1.2433 val_cls: 1.2438 val_acc: 0.5112\n",
            "tensor(1.2438) tensor(1.2488)\n",
            "=========\n",
            "epoch 33 train loss: 1.2385 val_cls: 1.2390 val_acc: 0.5112\n",
            "tensor(1.2390) tensor(1.2438)\n",
            "=========\n",
            "epoch 34 train loss: 1.2341 val_cls: 1.2345 val_acc: 0.5143\n",
            "tensor(1.2345) tensor(1.2390)\n",
            "=========\n",
            "epoch 35 train loss: 1.2299 val_cls: 1.2303 val_acc: 0.5153\n",
            "tensor(1.2303) tensor(1.2345)\n",
            "=========\n",
            "epoch 36 train loss: 1.2259 val_cls: 1.2263 val_acc: 0.5153\n",
            "tensor(1.2263) tensor(1.2303)\n",
            "=========\n",
            "epoch 37 train loss: 1.2221 val_cls: 1.2225 val_acc: 0.5163\n",
            "tensor(1.2225) tensor(1.2263)\n",
            "=========\n",
            "epoch 38 train loss: 1.2185 val_cls: 1.2189 val_acc: 0.5173\n",
            "tensor(1.2189) tensor(1.2225)\n",
            "=========\n",
            "epoch 39 train loss: 1.2151 val_cls: 1.2155 val_acc: 0.5153\n",
            "tensor(1.2155) tensor(1.2189)\n",
            "=========\n",
            "epoch 40 train loss: 1.2118 val_cls: 1.2122 val_acc: 0.5184\n",
            "tensor(1.2122) tensor(1.2155)\n",
            "=========\n",
            "epoch 41 train loss: 1.2088 val_cls: 1.2090 val_acc: 0.5184\n",
            "tensor(1.2090) tensor(1.2122)\n",
            "=========\n",
            "epoch 42 train loss: 1.2057 val_cls: 1.2061 val_acc: 0.5173\n",
            "tensor(1.2061) tensor(1.2090)\n",
            "=========\n",
            "epoch 43 train loss: 1.2030 val_cls: 1.2032 val_acc: 0.5173\n",
            "tensor(1.2032) tensor(1.2061)\n",
            "=========\n",
            "epoch 44 train loss: 1.2001 val_cls: 1.2006 val_acc: 0.5143\n",
            "tensor(1.2006) tensor(1.2032)\n",
            "=========\n",
            "epoch 45 train loss: 1.1976 val_cls: 1.1980 val_acc: 0.5163\n",
            "tensor(1.1980) tensor(1.2006)\n",
            "=========\n",
            "epoch 46 train loss: 1.1951 val_cls: 1.1955 val_acc: 0.5163\n",
            "tensor(1.1955) tensor(1.1980)\n",
            "=========\n",
            "epoch 47 train loss: 1.1927 val_cls: 1.1931 val_acc: 0.5163\n",
            "tensor(1.1931) tensor(1.1955)\n",
            "=========\n",
            "epoch 48 train loss: 1.1905 val_cls: 1.1909 val_acc: 0.5173\n",
            "tensor(1.1909) tensor(1.1931)\n",
            "=========\n",
            "epoch 49 train loss: 1.1883 val_cls: 1.1887 val_acc: 0.5173\n",
            "tensor(1.1887) tensor(1.1909)\n",
            "=========\n",
            "epoch 50 train loss: 1.1862 val_cls: 1.1866 val_acc: 0.5173\n",
            "tensor(1.1866) tensor(1.1887)\n",
            "=========\n",
            "epoch 51 train loss: 1.1841 val_cls: 1.1846 val_acc: 0.5184\n",
            "tensor(1.1846) tensor(1.1866)\n",
            "=========\n",
            "epoch 52 train loss: 1.1821 val_cls: 1.1827 val_acc: 0.5173\n",
            "tensor(1.1827) tensor(1.1846)\n",
            "=========\n",
            "epoch 53 train loss: 1.1803 val_cls: 1.1808 val_acc: 0.5184\n",
            "tensor(1.1808) tensor(1.1827)\n",
            "=========\n",
            "epoch 54 train loss: 1.1785 val_cls: 1.1791 val_acc: 0.5194\n",
            "tensor(1.1791) tensor(1.1808)\n",
            "=========\n",
            "epoch 55 train loss: 1.1767 val_cls: 1.1774 val_acc: 0.5194\n",
            "tensor(1.1774) tensor(1.1791)\n",
            "=========\n",
            "epoch 56 train loss: 1.1751 val_cls: 1.1758 val_acc: 0.5194\n",
            "tensor(1.1758) tensor(1.1774)\n",
            "=========\n",
            "epoch 57 train loss: 1.1734 val_cls: 1.1742 val_acc: 0.5204\n",
            "tensor(1.1742) tensor(1.1758)\n",
            "=========\n",
            "epoch 58 train loss: 1.1718 val_cls: 1.1727 val_acc: 0.5204\n",
            "tensor(1.1727) tensor(1.1742)\n",
            "=========\n",
            "epoch 59 train loss: 1.1703 val_cls: 1.1712 val_acc: 0.5184\n",
            "tensor(1.1712) tensor(1.1727)\n",
            "=========\n",
            "epoch 60 train loss: 1.1688 val_cls: 1.1698 val_acc: 0.5184\n",
            "tensor(1.1698) tensor(1.1712)\n",
            "=========\n",
            "epoch 61 train loss: 1.1675 val_cls: 1.1684 val_acc: 0.5184\n",
            "tensor(1.1684) tensor(1.1698)\n",
            "=========\n",
            "epoch 62 train loss: 1.1660 val_cls: 1.1671 val_acc: 0.5194\n",
            "tensor(1.1671) tensor(1.1684)\n",
            "=========\n",
            "epoch 63 train loss: 1.1647 val_cls: 1.1659 val_acc: 0.5194\n",
            "tensor(1.1659) tensor(1.1671)\n",
            "=========\n",
            "epoch 64 train loss: 1.1634 val_cls: 1.1647 val_acc: 0.5224\n",
            "tensor(1.1647) tensor(1.1659)\n",
            "=========\n",
            "epoch 65 train loss: 1.1621 val_cls: 1.1635 val_acc: 0.5184\n",
            "tensor(1.1635) tensor(1.1647)\n",
            "=========\n",
            "epoch 66 train loss: 1.1610 val_cls: 1.1623 val_acc: 0.5194\n",
            "tensor(1.1623) tensor(1.1635)\n",
            "=========\n",
            "epoch 67 train loss: 1.1598 val_cls: 1.1612 val_acc: 0.5204\n",
            "tensor(1.1612) tensor(1.1623)\n",
            "=========\n",
            "epoch 68 train loss: 1.1586 val_cls: 1.1602 val_acc: 0.5214\n",
            "tensor(1.1602) tensor(1.1612)\n",
            "=========\n",
            "epoch 69 train loss: 1.1575 val_cls: 1.1591 val_acc: 0.5214\n",
            "tensor(1.1591) tensor(1.1602)\n",
            "=========\n",
            "epoch 70 train loss: 1.1565 val_cls: 1.1581 val_acc: 0.5224\n",
            "tensor(1.1581) tensor(1.1591)\n",
            "=========\n",
            "epoch 71 train loss: 1.1554 val_cls: 1.1572 val_acc: 0.5224\n",
            "tensor(1.1572) tensor(1.1581)\n",
            "=========\n",
            "epoch 72 train loss: 1.1544 val_cls: 1.1562 val_acc: 0.5224\n",
            "tensor(1.1562) tensor(1.1572)\n",
            "=========\n",
            "epoch 73 train loss: 1.1534 val_cls: 1.1553 val_acc: 0.5235\n",
            "tensor(1.1553) tensor(1.1562)\n",
            "=========\n",
            "epoch 74 train loss: 1.1524 val_cls: 1.1544 val_acc: 0.5235\n",
            "tensor(1.1544) tensor(1.1553)\n",
            "=========\n",
            "epoch 75 train loss: 1.1515 val_cls: 1.1536 val_acc: 0.5224\n",
            "tensor(1.1536) tensor(1.1544)\n",
            "=========\n",
            "epoch 76 train loss: 1.1506 val_cls: 1.1528 val_acc: 0.5245\n",
            "tensor(1.1528) tensor(1.1536)\n",
            "=========\n",
            "epoch 77 train loss: 1.1496 val_cls: 1.1519 val_acc: 0.5245\n",
            "tensor(1.1519) tensor(1.1528)\n",
            "=========\n",
            "epoch 78 train loss: 1.1488 val_cls: 1.1511 val_acc: 0.5224\n",
            "tensor(1.1511) tensor(1.1519)\n",
            "=========\n",
            "epoch 79 train loss: 1.1479 val_cls: 1.1504 val_acc: 0.5224\n",
            "tensor(1.1504) tensor(1.1511)\n",
            "=========\n",
            "epoch 80 train loss: 1.1472 val_cls: 1.1496 val_acc: 0.5214\n",
            "tensor(1.1496) tensor(1.1504)\n",
            "=========\n",
            "epoch 81 train loss: 1.1463 val_cls: 1.1489 val_acc: 0.5214\n",
            "tensor(1.1489) tensor(1.1496)\n",
            "=========\n",
            "epoch 82 train loss: 1.1455 val_cls: 1.1481 val_acc: 0.5214\n",
            "tensor(1.1481) tensor(1.1489)\n",
            "=========\n",
            "epoch 83 train loss: 1.1448 val_cls: 1.1474 val_acc: 0.5214\n",
            "tensor(1.1474) tensor(1.1481)\n",
            "=========\n",
            "epoch 84 train loss: 1.1441 val_cls: 1.1467 val_acc: 0.5214\n",
            "tensor(1.1467) tensor(1.1474)\n",
            "=========\n",
            "epoch 85 train loss: 1.1432 val_cls: 1.1461 val_acc: 0.5214\n",
            "tensor(1.1461) tensor(1.1467)\n",
            "=========\n",
            "epoch 86 train loss: 1.1426 val_cls: 1.1454 val_acc: 0.5214\n",
            "tensor(1.1454) tensor(1.1461)\n",
            "=========\n",
            "epoch 87 train loss: 1.1419 val_cls: 1.1448 val_acc: 0.5214\n",
            "tensor(1.1448) tensor(1.1454)\n",
            "=========\n",
            "epoch 88 train loss: 1.1412 val_cls: 1.1442 val_acc: 0.5214\n",
            "tensor(1.1442) tensor(1.1448)\n",
            "=========\n",
            "epoch 89 train loss: 1.1405 val_cls: 1.1436 val_acc: 0.5214\n",
            "tensor(1.1436) tensor(1.1442)\n",
            "=========\n",
            "epoch 90 train loss: 1.1399 val_cls: 1.1430 val_acc: 0.5214\n",
            "tensor(1.1430) tensor(1.1436)\n",
            "=========\n",
            "epoch 91 train loss: 1.1392 val_cls: 1.1424 val_acc: 0.5214\n",
            "tensor(1.1424) tensor(1.1430)\n",
            "=========\n",
            "epoch 92 train loss: 1.1385 val_cls: 1.1419 val_acc: 0.5214\n",
            "tensor(1.1419) tensor(1.1424)\n",
            "=========\n",
            "epoch 93 train loss: 1.1380 val_cls: 1.1413 val_acc: 0.5214\n",
            "tensor(1.1413) tensor(1.1419)\n",
            "=========\n",
            "epoch 94 train loss: 1.1374 val_cls: 1.1408 val_acc: 0.5214\n",
            "tensor(1.1408) tensor(1.1413)\n",
            "=========\n",
            "epoch 95 train loss: 1.1368 val_cls: 1.1403 val_acc: 0.5214\n",
            "tensor(1.1403) tensor(1.1408)\n",
            "=========\n",
            "epoch 96 train loss: 1.1362 val_cls: 1.1398 val_acc: 0.5204\n",
            "tensor(1.1398) tensor(1.1403)\n",
            "=========\n",
            "epoch 97 train loss: 1.1357 val_cls: 1.1393 val_acc: 0.5214\n",
            "tensor(1.1393) tensor(1.1398)\n",
            "=========\n",
            "epoch 98 train loss: 1.1351 val_cls: 1.1388 val_acc: 0.5224\n",
            "tensor(1.1388) tensor(1.1393)\n",
            "=========\n",
            "epoch 99 train loss: 1.1345 val_cls: 1.1383 val_acc: 0.5224\n",
            "tensor(1.1383) tensor(1.1388)\n",
            "=========\n",
            "epoch 100 train loss: 1.1340 val_cls: 1.1379 val_acc: 0.5224\n",
            "tensor(1.1379) tensor(1.1383)\n",
            "=========\n",
            "epoch 101 train loss: 1.1335 val_cls: 1.1374 val_acc: 0.5224\n",
            "tensor(1.1374) tensor(1.1379)\n",
            "=========\n",
            "epoch 102 train loss: 1.1330 val_cls: 1.1370 val_acc: 0.5224\n",
            "tensor(1.1370) tensor(1.1374)\n",
            "=========\n",
            "epoch 103 train loss: 1.1325 val_cls: 1.1366 val_acc: 0.5214\n",
            "tensor(1.1366) tensor(1.1370)\n",
            "=========\n",
            "epoch 104 train loss: 1.1321 val_cls: 1.1362 val_acc: 0.5214\n",
            "tensor(1.1362) tensor(1.1366)\n",
            "=========\n",
            "epoch 105 train loss: 1.1315 val_cls: 1.1358 val_acc: 0.5214\n",
            "tensor(1.1358) tensor(1.1362)\n",
            "=========\n",
            "epoch 106 train loss: 1.1310 val_cls: 1.1353 val_acc: 0.5224\n",
            "tensor(1.1353) tensor(1.1358)\n",
            "=========\n",
            "epoch 107 train loss: 1.1306 val_cls: 1.1349 val_acc: 0.5224\n",
            "tensor(1.1349) tensor(1.1353)\n",
            "=========\n",
            "epoch 108 train loss: 1.1302 val_cls: 1.1345 val_acc: 0.5235\n",
            "tensor(1.1345) tensor(1.1349)\n",
            "=========\n",
            "epoch 109 train loss: 1.1297 val_cls: 1.1341 val_acc: 0.5214\n",
            "tensor(1.1341) tensor(1.1345)\n",
            "=========\n",
            "epoch 110 train loss: 1.1293 val_cls: 1.1337 val_acc: 0.5214\n",
            "tensor(1.1337) tensor(1.1341)\n",
            "=========\n",
            "epoch 111 train loss: 1.1288 val_cls: 1.1334 val_acc: 0.5224\n",
            "tensor(1.1334) tensor(1.1337)\n",
            "=========\n",
            "epoch 112 train loss: 1.1283 val_cls: 1.1331 val_acc: 0.5245\n",
            "tensor(1.1331) tensor(1.1334)\n",
            "=========\n",
            "epoch 113 train loss: 1.1280 val_cls: 1.1327 val_acc: 0.5224\n",
            "tensor(1.1327) tensor(1.1331)\n",
            "=========\n",
            "epoch 114 train loss: 1.1276 val_cls: 1.1323 val_acc: 0.5204\n",
            "tensor(1.1323) tensor(1.1327)\n",
            "=========\n",
            "epoch 115 train loss: 1.1272 val_cls: 1.1319 val_acc: 0.5214\n",
            "tensor(1.1319) tensor(1.1323)\n",
            "=========\n",
            "epoch 116 train loss: 1.1268 val_cls: 1.1316 val_acc: 0.5214\n",
            "tensor(1.1316) tensor(1.1319)\n",
            "=========\n",
            "epoch 117 train loss: 1.1264 val_cls: 1.1313 val_acc: 0.5224\n",
            "tensor(1.1313) tensor(1.1316)\n",
            "=========\n",
            "epoch 118 train loss: 1.1260 val_cls: 1.1310 val_acc: 0.5214\n",
            "tensor(1.1310) tensor(1.1313)\n",
            "=========\n",
            "epoch 119 train loss: 1.1256 val_cls: 1.1307 val_acc: 0.5214\n",
            "tensor(1.1307) tensor(1.1310)\n",
            "=========\n",
            "epoch 120 train loss: 1.1252 val_cls: 1.1304 val_acc: 0.5214\n",
            "tensor(1.1304) tensor(1.1307)\n",
            "=========\n",
            "epoch 121 train loss: 1.1249 val_cls: 1.1301 val_acc: 0.5214\n",
            "tensor(1.1301) tensor(1.1304)\n",
            "=========\n",
            "epoch 122 train loss: 1.1245 val_cls: 1.1298 val_acc: 0.5214\n",
            "tensor(1.1298) tensor(1.1301)\n",
            "=========\n",
            "epoch 123 train loss: 1.1242 val_cls: 1.1295 val_acc: 0.5214\n",
            "tensor(1.1295) tensor(1.1298)\n",
            "=========\n",
            "epoch 124 train loss: 1.1239 val_cls: 1.1292 val_acc: 0.5214\n",
            "tensor(1.1292) tensor(1.1295)\n",
            "=========\n",
            "epoch 125 train loss: 1.1235 val_cls: 1.1289 val_acc: 0.5214\n",
            "tensor(1.1289) tensor(1.1292)\n",
            "=========\n",
            "epoch 126 train loss: 1.1231 val_cls: 1.1286 val_acc: 0.5214\n",
            "tensor(1.1286) tensor(1.1289)\n",
            "=========\n",
            "epoch 127 train loss: 1.1228 val_cls: 1.1283 val_acc: 0.5204\n",
            "tensor(1.1283) tensor(1.1286)\n",
            "=========\n",
            "epoch 128 train loss: 1.1225 val_cls: 1.1280 val_acc: 0.5204\n",
            "tensor(1.1280) tensor(1.1283)\n",
            "=========\n",
            "epoch 129 train loss: 1.1222 val_cls: 1.1277 val_acc: 0.5204\n",
            "tensor(1.1277) tensor(1.1280)\n",
            "=========\n",
            "epoch 130 train loss: 1.1219 val_cls: 1.1275 val_acc: 0.5204\n",
            "tensor(1.1275) tensor(1.1277)\n",
            "=========\n",
            "epoch 131 train loss: 1.1216 val_cls: 1.1272 val_acc: 0.5204\n",
            "tensor(1.1272) tensor(1.1275)\n",
            "=========\n",
            "epoch 132 train loss: 1.1213 val_cls: 1.1270 val_acc: 0.5214\n",
            "tensor(1.1270) tensor(1.1272)\n",
            "=========\n",
            "epoch 133 train loss: 1.1209 val_cls: 1.1267 val_acc: 0.5204\n",
            "tensor(1.1267) tensor(1.1270)\n",
            "=========\n",
            "epoch 134 train loss: 1.1206 val_cls: 1.1265 val_acc: 0.5204\n",
            "tensor(1.1265) tensor(1.1267)\n",
            "=========\n",
            "epoch 135 train loss: 1.1203 val_cls: 1.1262 val_acc: 0.5214\n",
            "tensor(1.1262) tensor(1.1265)\n",
            "=========\n",
            "epoch 136 train loss: 1.1200 val_cls: 1.1260 val_acc: 0.5214\n",
            "tensor(1.1260) tensor(1.1262)\n",
            "=========\n",
            "epoch 137 train loss: 1.1198 val_cls: 1.1257 val_acc: 0.5224\n",
            "tensor(1.1257) tensor(1.1260)\n",
            "=========\n",
            "epoch 138 train loss: 1.1194 val_cls: 1.1255 val_acc: 0.5214\n",
            "tensor(1.1255) tensor(1.1257)\n",
            "=========\n",
            "epoch 139 train loss: 1.1192 val_cls: 1.1253 val_acc: 0.5204\n",
            "tensor(1.1253) tensor(1.1255)\n",
            "=========\n",
            "epoch 140 train loss: 1.1189 val_cls: 1.1250 val_acc: 0.5245\n",
            "tensor(1.1250) tensor(1.1253)\n",
            "=========\n",
            "epoch 141 train loss: 1.1186 val_cls: 1.1248 val_acc: 0.5204\n",
            "tensor(1.1248) tensor(1.1250)\n",
            "=========\n",
            "epoch 142 train loss: 1.1184 val_cls: 1.1246 val_acc: 0.5214\n",
            "tensor(1.1246) tensor(1.1248)\n",
            "=========\n",
            "epoch 143 train loss: 1.1181 val_cls: 1.1244 val_acc: 0.5214\n",
            "tensor(1.1244) tensor(1.1246)\n",
            "=========\n",
            "epoch 144 train loss: 1.1179 val_cls: 1.1242 val_acc: 0.5235\n",
            "tensor(1.1242) tensor(1.1244)\n",
            "=========\n",
            "epoch 145 train loss: 1.1176 val_cls: 1.1240 val_acc: 0.5224\n",
            "tensor(1.1240) tensor(1.1242)\n",
            "=========\n",
            "epoch 146 train loss: 1.1174 val_cls: 1.1238 val_acc: 0.5224\n",
            "tensor(1.1238) tensor(1.1240)\n",
            "=========\n",
            "epoch 147 train loss: 1.1171 val_cls: 1.1236 val_acc: 0.5224\n",
            "tensor(1.1236) tensor(1.1238)\n",
            "=========\n",
            "epoch 148 train loss: 1.1169 val_cls: 1.1234 val_acc: 0.5214\n",
            "tensor(1.1234) tensor(1.1236)\n",
            "=========\n",
            "epoch 149 train loss: 1.1166 val_cls: 1.1232 val_acc: 0.5214\n",
            "tensor(1.1232) tensor(1.1234)\n",
            "=========\n",
            "epoch 150 train loss: 1.1163 val_cls: 1.1230 val_acc: 0.5224\n",
            "tensor(1.1230) tensor(1.1232)\n",
            "=========\n",
            "epoch 151 train loss: 1.1161 val_cls: 1.1228 val_acc: 0.5214\n",
            "tensor(1.1228) tensor(1.1230)\n",
            "=========\n",
            "epoch 152 train loss: 1.1159 val_cls: 1.1226 val_acc: 0.5214\n",
            "tensor(1.1226) tensor(1.1228)\n",
            "=========\n",
            "epoch 153 train loss: 1.1157 val_cls: 1.1225 val_acc: 0.5214\n",
            "tensor(1.1225) tensor(1.1226)\n",
            "=========\n",
            "epoch 154 train loss: 1.1154 val_cls: 1.1223 val_acc: 0.5214\n",
            "tensor(1.1223) tensor(1.1225)\n",
            "=========\n",
            "epoch 155 train loss: 1.1152 val_cls: 1.1221 val_acc: 0.5214\n",
            "tensor(1.1221) tensor(1.1223)\n",
            "=========\n",
            "epoch 156 train loss: 1.1150 val_cls: 1.1220 val_acc: 0.5224\n",
            "tensor(1.1220) tensor(1.1221)\n",
            "=========\n",
            "epoch 157 train loss: 1.1148 val_cls: 1.1218 val_acc: 0.5224\n",
            "tensor(1.1218) tensor(1.1220)\n",
            "=========\n",
            "epoch 158 train loss: 1.1145 val_cls: 1.1217 val_acc: 0.5224\n",
            "tensor(1.1217) tensor(1.1218)\n",
            "=========\n",
            "epoch 159 train loss: 1.1144 val_cls: 1.1215 val_acc: 0.5224\n",
            "tensor(1.1215) tensor(1.1217)\n",
            "=========\n",
            "epoch 160 train loss: 1.1142 val_cls: 1.1214 val_acc: 0.5235\n",
            "tensor(1.1214) tensor(1.1215)\n",
            "=========\n",
            "epoch 161 train loss: 1.1139 val_cls: 1.1212 val_acc: 0.5235\n",
            "tensor(1.1212) tensor(1.1214)\n",
            "=========\n",
            "epoch 162 train loss: 1.1137 val_cls: 1.1210 val_acc: 0.5235\n",
            "tensor(1.1210) tensor(1.1212)\n",
            "=========\n",
            "epoch 163 train loss: 1.1135 val_cls: 1.1209 val_acc: 0.5235\n",
            "tensor(1.1209) tensor(1.1210)\n",
            "=========\n",
            "epoch 164 train loss: 1.1133 val_cls: 1.1207 val_acc: 0.5224\n",
            "tensor(1.1207) tensor(1.1209)\n",
            "=========\n",
            "epoch 165 train loss: 1.1132 val_cls: 1.1205 val_acc: 0.5224\n",
            "tensor(1.1205) tensor(1.1207)\n",
            "=========\n",
            "epoch 166 train loss: 1.1130 val_cls: 1.1204 val_acc: 0.5235\n",
            "tensor(1.1204) tensor(1.1205)\n",
            "=========\n",
            "epoch 167 train loss: 1.1127 val_cls: 1.1202 val_acc: 0.5224\n",
            "tensor(1.1202) tensor(1.1204)\n",
            "=========\n",
            "epoch 168 train loss: 1.1125 val_cls: 1.1201 val_acc: 0.5235\n",
            "tensor(1.1201) tensor(1.1202)\n",
            "=========\n",
            "epoch 169 train loss: 1.1124 val_cls: 1.1199 val_acc: 0.5224\n",
            "tensor(1.1199) tensor(1.1201)\n",
            "=========\n",
            "epoch 170 train loss: 1.1122 val_cls: 1.1198 val_acc: 0.5214\n",
            "tensor(1.1198) tensor(1.1199)\n",
            "=========\n",
            "epoch 171 train loss: 1.1120 val_cls: 1.1196 val_acc: 0.5214\n",
            "tensor(1.1196) tensor(1.1198)\n",
            "=========\n",
            "epoch 172 train loss: 1.1118 val_cls: 1.1195 val_acc: 0.5214\n",
            "tensor(1.1195) tensor(1.1196)\n",
            "=========\n",
            "epoch 173 train loss: 1.1116 val_cls: 1.1193 val_acc: 0.5224\n",
            "tensor(1.1193) tensor(1.1195)\n",
            "=========\n",
            "epoch 174 train loss: 1.1114 val_cls: 1.1192 val_acc: 0.5224\n",
            "tensor(1.1192) tensor(1.1193)\n",
            "=========\n",
            "epoch 175 train loss: 1.1113 val_cls: 1.1191 val_acc: 0.5235\n",
            "tensor(1.1191) tensor(1.1192)\n",
            "=========\n",
            "epoch 176 train loss: 1.1111 val_cls: 1.1190 val_acc: 0.5235\n",
            "tensor(1.1190) tensor(1.1191)\n",
            "=========\n",
            "epoch 177 train loss: 1.1109 val_cls: 1.1189 val_acc: 0.5235\n",
            "tensor(1.1189) tensor(1.1190)\n",
            "=========\n",
            "epoch 178 train loss: 1.1108 val_cls: 1.1187 val_acc: 0.5235\n",
            "tensor(1.1187) tensor(1.1189)\n",
            "=========\n",
            "epoch 179 train loss: 1.1106 val_cls: 1.1186 val_acc: 0.5235\n",
            "tensor(1.1186) tensor(1.1187)\n",
            "=========\n",
            "epoch 180 train loss: 1.1104 val_cls: 1.1185 val_acc: 0.5214\n",
            "tensor(1.1185) tensor(1.1186)\n",
            "=========\n",
            "epoch 181 train loss: 1.1102 val_cls: 1.1184 val_acc: 0.5224\n",
            "tensor(1.1184) tensor(1.1185)\n",
            "=========\n",
            "epoch 182 train loss: 1.1101 val_cls: 1.1182 val_acc: 0.5224\n",
            "tensor(1.1182) tensor(1.1184)\n",
            "=========\n",
            "epoch 183 train loss: 1.1099 val_cls: 1.1181 val_acc: 0.5214\n",
            "tensor(1.1181) tensor(1.1182)\n",
            "=========\n",
            "epoch 184 train loss: 1.1097 val_cls: 1.1180 val_acc: 0.5214\n",
            "tensor(1.1180) tensor(1.1181)\n",
            "=========\n",
            "epoch 185 train loss: 1.1096 val_cls: 1.1179 val_acc: 0.5224\n",
            "tensor(1.1179) tensor(1.1180)\n",
            "=========\n",
            "epoch 186 train loss: 1.1095 val_cls: 1.1177 val_acc: 0.5224\n",
            "tensor(1.1177) tensor(1.1179)\n",
            "=========\n",
            "epoch 187 train loss: 1.1093 val_cls: 1.1176 val_acc: 0.5235\n",
            "tensor(1.1176) tensor(1.1177)\n",
            "=========\n",
            "epoch 188 train loss: 1.1091 val_cls: 1.1175 val_acc: 0.5235\n",
            "tensor(1.1175) tensor(1.1176)\n",
            "=========\n",
            "epoch 189 train loss: 1.1090 val_cls: 1.1174 val_acc: 0.5235\n",
            "tensor(1.1174) tensor(1.1175)\n",
            "=========\n",
            "epoch 190 train loss: 1.1089 val_cls: 1.1173 val_acc: 0.5224\n",
            "tensor(1.1173) tensor(1.1174)\n",
            "=========\n",
            "epoch 191 train loss: 1.1087 val_cls: 1.1172 val_acc: 0.5224\n",
            "tensor(1.1172) tensor(1.1173)\n",
            "=========\n",
            "epoch 192 train loss: 1.1086 val_cls: 1.1171 val_acc: 0.5224\n",
            "tensor(1.1171) tensor(1.1172)\n",
            "=========\n",
            "epoch 193 train loss: 1.1084 val_cls: 1.1170 val_acc: 0.5214\n",
            "tensor(1.1170) tensor(1.1171)\n",
            "=========\n",
            "epoch 194 train loss: 1.1083 val_cls: 1.1169 val_acc: 0.5224\n",
            "tensor(1.1169) tensor(1.1170)\n",
            "=========\n",
            "epoch 195 train loss: 1.1081 val_cls: 1.1168 val_acc: 0.5224\n",
            "tensor(1.1168) tensor(1.1169)\n",
            "=========\n",
            "epoch 196 train loss: 1.1080 val_cls: 1.1167 val_acc: 0.5224\n",
            "tensor(1.1167) tensor(1.1168)\n",
            "=========\n",
            "epoch 197 train loss: 1.1078 val_cls: 1.1165 val_acc: 0.5214\n",
            "tensor(1.1165) tensor(1.1167)\n",
            "=========\n",
            "epoch 198 train loss: 1.1077 val_cls: 1.1164 val_acc: 0.5214\n",
            "tensor(1.1164) tensor(1.1165)\n",
            "=========\n",
            "epoch 199 train loss: 1.1076 val_cls: 1.1163 val_acc: 0.5224\n",
            "tensor(1.1163) tensor(1.1164)\n",
            "=========\n",
            "epoch 200 train loss: 1.1074 val_cls: 1.1162 val_acc: 0.5224\n",
            "tensor(1.1162) tensor(1.1163)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history['val_cls'],  label='val')\n",
        "plt.plot(history['train_loss'], label='train')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "uu4cG23cqMPx",
        "outputId": "04ea33b7-baef-4de2-f6b9-7ee456daf790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAuaediCd9nJ",
        "outputId": "374ee2bd-dab0-4653-bea9-4f92030cd1e0"
      },
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "\n",
        "class_predictions = utils.test(classification_model, c_test_iter)\n",
        "class_predictions = np.argmax(class_predictions, axis=1)\n",
        "\n",
        "print(\"Accuracy: \",  accuracy_score(y_true=y_test, y_pred=class_predictions),\n",
        "      \", MSE: \",mse(y_true=y_test, y_pred=class_predictions),\n",
        "      \", MAE\", mae(y_true=y_test, y_pred=class_predictions))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.5479591836734694 , MSE:  0.673469387755102 , MAE 0.5224489795918368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HboX34m8641"
      },
      "source": [
        "**Task 7.** Add an additional dense layer with the Relu activation and inputoutput 30 neurons. Is the result better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs31S3JZj-UL",
        "outputId": "8f3e545c-80e3-4b5c-e1c9-7108850e0c2f"
      },
      "source": [
        "class ClassModel2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ClassModel2, self).__init__()\n",
        "    self.layer =nn.Sequential(nn.Linear(in_features=11, out_features=30),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(in_features=30, out_features=10))\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "classification_model = ClassModel2()\n",
        "classification_model = classification_model.cuda()\n",
        "\n",
        "history = utils.train(model=classification_model,\n",
        "                            loss=nn.CrossEntropyLoss(),\n",
        "                            val_metrics={\"cls\": nn.CrossEntropyLoss(), \"acc\": val_acc},\n",
        "                            optimizer=torch.optim.SGD(classification_model.parameters(), lr=0.01),\n",
        "                            train_ds=c_train_iter,\n",
        "                            dev_ds=c_dev_iter,\n",
        "                            num_epochs=200,\n",
        "                            early_stopper=utils.EarlyStopper(metric_name=\"cls\", patience=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========\n",
            "epoch 1 train loss: 2.1629 val_cls: 2.0415 val_acc: 0.4041\n",
            "tensor(2.0415) None\n",
            "=========\n",
            "epoch 2 train loss: 1.9493 val_cls: 1.8561 val_acc: 0.4480\n",
            "tensor(1.8561) tensor(2.0415)\n",
            "=========\n",
            "epoch 3 train loss: 1.7854 val_cls: 1.7151 val_acc: 0.4459\n",
            "tensor(1.7151) tensor(1.8561)\n",
            "=========\n",
            "epoch 4 train loss: 1.6600 val_cls: 1.6082 val_acc: 0.4541\n",
            "tensor(1.6082) tensor(1.7151)\n",
            "=========\n",
            "epoch 5 train loss: 1.5639 val_cls: 1.5266 val_acc: 0.4684\n",
            "tensor(1.5266) tensor(1.6082)\n",
            "=========\n",
            "epoch 6 train loss: 1.4896 val_cls: 1.4631 val_acc: 0.4888\n",
            "tensor(1.4631) tensor(1.5266)\n",
            "=========\n",
            "epoch 7 train loss: 1.4312 val_cls: 1.4130 val_acc: 0.4908\n",
            "tensor(1.4130) tensor(1.4631)\n",
            "=========\n",
            "epoch 8 train loss: 1.3849 val_cls: 1.3731 val_acc: 0.5020\n",
            "tensor(1.3731) tensor(1.4130)\n",
            "=========\n",
            "epoch 9 train loss: 1.3478 val_cls: 1.3412 val_acc: 0.5010\n",
            "tensor(1.3412) tensor(1.3731)\n",
            "=========\n",
            "epoch 10 train loss: 1.3181 val_cls: 1.3155 val_acc: 0.4990\n",
            "tensor(1.3155) tensor(1.3412)\n",
            "=========\n",
            "epoch 11 train loss: 1.2939 val_cls: 1.2947 val_acc: 0.4990\n",
            "tensor(1.2947) tensor(1.3155)\n",
            "=========\n",
            "epoch 12 train loss: 1.2742 val_cls: 1.2778 val_acc: 0.5061\n",
            "tensor(1.2778) tensor(1.2947)\n",
            "=========\n",
            "epoch 13 train loss: 1.2578 val_cls: 1.2637 val_acc: 0.5102\n",
            "tensor(1.2637) tensor(1.2778)\n",
            "=========\n",
            "epoch 14 train loss: 1.2441 val_cls: 1.2518 val_acc: 0.5214\n",
            "tensor(1.2518) tensor(1.2637)\n",
            "=========\n",
            "epoch 15 train loss: 1.2323 val_cls: 1.2417 val_acc: 0.5255\n",
            "tensor(1.2417) tensor(1.2518)\n",
            "=========\n",
            "epoch 16 train loss: 1.2222 val_cls: 1.2329 val_acc: 0.5235\n",
            "tensor(1.2329) tensor(1.2417)\n",
            "=========\n",
            "epoch 17 train loss: 1.2133 val_cls: 1.2252 val_acc: 0.5276\n",
            "tensor(1.2252) tensor(1.2329)\n",
            "=========\n",
            "epoch 18 train loss: 1.2053 val_cls: 1.2182 val_acc: 0.5245\n",
            "tensor(1.2182) tensor(1.2252)\n",
            "=========\n",
            "epoch 19 train loss: 1.1981 val_cls: 1.2119 val_acc: 0.5235\n",
            "tensor(1.2119) tensor(1.2182)\n",
            "=========\n",
            "epoch 20 train loss: 1.1915 val_cls: 1.2062 val_acc: 0.5194\n",
            "tensor(1.2062) tensor(1.2119)\n",
            "=========\n",
            "epoch 21 train loss: 1.1855 val_cls: 1.2009 val_acc: 0.5224\n",
            "tensor(1.2009) tensor(1.2062)\n",
            "=========\n",
            "epoch 22 train loss: 1.1798 val_cls: 1.1959 val_acc: 0.5255\n",
            "tensor(1.1959) tensor(1.2009)\n",
            "=========\n",
            "epoch 23 train loss: 1.1746 val_cls: 1.1914 val_acc: 0.5255\n",
            "tensor(1.1914) tensor(1.1959)\n",
            "=========\n",
            "epoch 24 train loss: 1.1697 val_cls: 1.1870 val_acc: 0.5306\n",
            "tensor(1.1870) tensor(1.1914)\n",
            "=========\n",
            "epoch 25 train loss: 1.1652 val_cls: 1.1829 val_acc: 0.5316\n",
            "tensor(1.1829) tensor(1.1870)\n",
            "=========\n",
            "epoch 26 train loss: 1.1607 val_cls: 1.1791 val_acc: 0.5306\n",
            "tensor(1.1791) tensor(1.1829)\n",
            "=========\n",
            "epoch 27 train loss: 1.1568 val_cls: 1.1754 val_acc: 0.5296\n",
            "tensor(1.1754) tensor(1.1791)\n",
            "=========\n",
            "epoch 28 train loss: 1.1527 val_cls: 1.1719 val_acc: 0.5327\n",
            "tensor(1.1719) tensor(1.1754)\n",
            "=========\n",
            "epoch 29 train loss: 1.1491 val_cls: 1.1686 val_acc: 0.5357\n",
            "tensor(1.1686) tensor(1.1719)\n",
            "=========\n",
            "epoch 30 train loss: 1.1456 val_cls: 1.1653 val_acc: 0.5398\n",
            "tensor(1.1653) tensor(1.1686)\n",
            "=========\n",
            "epoch 31 train loss: 1.1422 val_cls: 1.1624 val_acc: 0.5388\n",
            "tensor(1.1624) tensor(1.1653)\n",
            "=========\n",
            "epoch 32 train loss: 1.1390 val_cls: 1.1594 val_acc: 0.5367\n",
            "tensor(1.1594) tensor(1.1624)\n",
            "=========\n",
            "epoch 33 train loss: 1.1361 val_cls: 1.1566 val_acc: 0.5378\n",
            "tensor(1.1566) tensor(1.1594)\n",
            "=========\n",
            "epoch 34 train loss: 1.1331 val_cls: 1.1540 val_acc: 0.5378\n",
            "tensor(1.1540) tensor(1.1566)\n",
            "=========\n",
            "epoch 35 train loss: 1.1302 val_cls: 1.1515 val_acc: 0.5398\n",
            "tensor(1.1515) tensor(1.1540)\n",
            "=========\n",
            "epoch 36 train loss: 1.1277 val_cls: 1.1491 val_acc: 0.5398\n",
            "tensor(1.1491) tensor(1.1515)\n",
            "=========\n",
            "epoch 37 train loss: 1.1251 val_cls: 1.1468 val_acc: 0.5408\n",
            "tensor(1.1468) tensor(1.1491)\n",
            "=========\n",
            "epoch 38 train loss: 1.1225 val_cls: 1.1448 val_acc: 0.5398\n",
            "tensor(1.1448) tensor(1.1468)\n",
            "=========\n",
            "epoch 39 train loss: 1.1202 val_cls: 1.1427 val_acc: 0.5388\n",
            "tensor(1.1427) tensor(1.1448)\n",
            "=========\n",
            "epoch 40 train loss: 1.1180 val_cls: 1.1406 val_acc: 0.5378\n",
            "tensor(1.1406) tensor(1.1427)\n",
            "=========\n",
            "epoch 41 train loss: 1.1158 val_cls: 1.1386 val_acc: 0.5429\n",
            "tensor(1.1386) tensor(1.1406)\n",
            "=========\n",
            "epoch 42 train loss: 1.1137 val_cls: 1.1367 val_acc: 0.5408\n",
            "tensor(1.1367) tensor(1.1386)\n",
            "=========\n",
            "epoch 43 train loss: 1.1117 val_cls: 1.1351 val_acc: 0.5388\n",
            "tensor(1.1351) tensor(1.1367)\n",
            "=========\n",
            "epoch 44 train loss: 1.1098 val_cls: 1.1335 val_acc: 0.5408\n",
            "tensor(1.1335) tensor(1.1351)\n",
            "=========\n",
            "epoch 45 train loss: 1.1079 val_cls: 1.1319 val_acc: 0.5398\n",
            "tensor(1.1319) tensor(1.1335)\n",
            "=========\n",
            "epoch 46 train loss: 1.1062 val_cls: 1.1304 val_acc: 0.5398\n",
            "tensor(1.1304) tensor(1.1319)\n",
            "=========\n",
            "epoch 47 train loss: 1.1044 val_cls: 1.1290 val_acc: 0.5418\n",
            "tensor(1.1290) tensor(1.1304)\n",
            "=========\n",
            "epoch 48 train loss: 1.1027 val_cls: 1.1275 val_acc: 0.5408\n",
            "tensor(1.1275) tensor(1.1290)\n",
            "=========\n",
            "epoch 49 train loss: 1.1012 val_cls: 1.1261 val_acc: 0.5398\n",
            "tensor(1.1261) tensor(1.1275)\n",
            "=========\n",
            "epoch 50 train loss: 1.0996 val_cls: 1.1249 val_acc: 0.5388\n",
            "tensor(1.1249) tensor(1.1261)\n",
            "=========\n",
            "epoch 51 train loss: 1.0981 val_cls: 1.1237 val_acc: 0.5378\n",
            "tensor(1.1237) tensor(1.1249)\n",
            "=========\n",
            "epoch 52 train loss: 1.0966 val_cls: 1.1226 val_acc: 0.5378\n",
            "tensor(1.1226) tensor(1.1237)\n",
            "=========\n",
            "epoch 53 train loss: 1.0954 val_cls: 1.1215 val_acc: 0.5367\n",
            "tensor(1.1215) tensor(1.1226)\n",
            "=========\n",
            "epoch 54 train loss: 1.0940 val_cls: 1.1204 val_acc: 0.5388\n",
            "tensor(1.1204) tensor(1.1215)\n",
            "=========\n",
            "epoch 55 train loss: 1.0927 val_cls: 1.1193 val_acc: 0.5388\n",
            "tensor(1.1193) tensor(1.1204)\n",
            "=========\n",
            "epoch 56 train loss: 1.0915 val_cls: 1.1185 val_acc: 0.5408\n",
            "tensor(1.1185) tensor(1.1193)\n",
            "=========\n",
            "epoch 57 train loss: 1.0902 val_cls: 1.1174 val_acc: 0.5408\n",
            "tensor(1.1174) tensor(1.1185)\n",
            "=========\n",
            "epoch 58 train loss: 1.0892 val_cls: 1.1167 val_acc: 0.5429\n",
            "tensor(1.1167) tensor(1.1174)\n",
            "=========\n",
            "epoch 59 train loss: 1.0880 val_cls: 1.1155 val_acc: 0.5398\n",
            "tensor(1.1155) tensor(1.1167)\n",
            "=========\n",
            "epoch 60 train loss: 1.0869 val_cls: 1.1147 val_acc: 0.5398\n",
            "tensor(1.1147) tensor(1.1155)\n",
            "=========\n",
            "epoch 61 train loss: 1.0858 val_cls: 1.1139 val_acc: 0.5439\n",
            "tensor(1.1139) tensor(1.1147)\n",
            "=========\n",
            "epoch 62 train loss: 1.0847 val_cls: 1.1132 val_acc: 0.5408\n",
            "tensor(1.1132) tensor(1.1139)\n",
            "=========\n",
            "epoch 63 train loss: 1.0837 val_cls: 1.1124 val_acc: 0.5398\n",
            "tensor(1.1124) tensor(1.1132)\n",
            "=========\n",
            "epoch 64 train loss: 1.0828 val_cls: 1.1117 val_acc: 0.5398\n",
            "tensor(1.1117) tensor(1.1124)\n",
            "=========\n",
            "epoch 65 train loss: 1.0819 val_cls: 1.1110 val_acc: 0.5388\n",
            "tensor(1.1110) tensor(1.1117)\n",
            "=========\n",
            "epoch 66 train loss: 1.0809 val_cls: 1.1104 val_acc: 0.5398\n",
            "tensor(1.1104) tensor(1.1110)\n",
            "=========\n",
            "epoch 67 train loss: 1.0801 val_cls: 1.1097 val_acc: 0.5418\n",
            "tensor(1.1097) tensor(1.1104)\n",
            "=========\n",
            "epoch 68 train loss: 1.0792 val_cls: 1.1091 val_acc: 0.5388\n",
            "tensor(1.1091) tensor(1.1097)\n",
            "=========\n",
            "epoch 69 train loss: 1.0783 val_cls: 1.1084 val_acc: 0.5418\n",
            "tensor(1.1084) tensor(1.1091)\n",
            "=========\n",
            "epoch 70 train loss: 1.0774 val_cls: 1.1079 val_acc: 0.5408\n",
            "tensor(1.1079) tensor(1.1084)\n",
            "=========\n",
            "epoch 71 train loss: 1.0766 val_cls: 1.1073 val_acc: 0.5439\n",
            "tensor(1.1073) tensor(1.1079)\n",
            "=========\n",
            "epoch 72 train loss: 1.0759 val_cls: 1.1067 val_acc: 0.5429\n",
            "tensor(1.1067) tensor(1.1073)\n",
            "=========\n",
            "epoch 73 train loss: 1.0751 val_cls: 1.1062 val_acc: 0.5408\n",
            "tensor(1.1062) tensor(1.1067)\n",
            "=========\n",
            "epoch 74 train loss: 1.0743 val_cls: 1.1057 val_acc: 0.5429\n",
            "tensor(1.1057) tensor(1.1062)\n",
            "=========\n",
            "epoch 75 train loss: 1.0735 val_cls: 1.1051 val_acc: 0.5459\n",
            "tensor(1.1051) tensor(1.1057)\n",
            "=========\n",
            "epoch 76 train loss: 1.0728 val_cls: 1.1046 val_acc: 0.5429\n",
            "tensor(1.1046) tensor(1.1051)\n",
            "=========\n",
            "epoch 77 train loss: 1.0722 val_cls: 1.1042 val_acc: 0.5439\n",
            "tensor(1.1042) tensor(1.1046)\n",
            "=========\n",
            "epoch 78 train loss: 1.0714 val_cls: 1.1036 val_acc: 0.5469\n",
            "tensor(1.1036) tensor(1.1042)\n",
            "=========\n",
            "epoch 79 train loss: 1.0709 val_cls: 1.1033 val_acc: 0.5469\n",
            "tensor(1.1033) tensor(1.1036)\n",
            "=========\n",
            "epoch 80 train loss: 1.0701 val_cls: 1.1029 val_acc: 0.5490\n",
            "tensor(1.1029) tensor(1.1033)\n",
            "=========\n",
            "epoch 81 train loss: 1.0694 val_cls: 1.1023 val_acc: 0.5480\n",
            "tensor(1.1023) tensor(1.1029)\n",
            "=========\n",
            "epoch 82 train loss: 1.0687 val_cls: 1.1020 val_acc: 0.5490\n",
            "tensor(1.1020) tensor(1.1023)\n",
            "=========\n",
            "epoch 83 train loss: 1.0681 val_cls: 1.1015 val_acc: 0.5500\n",
            "tensor(1.1015) tensor(1.1020)\n",
            "=========\n",
            "epoch 84 train loss: 1.0675 val_cls: 1.1011 val_acc: 0.5490\n",
            "tensor(1.1011) tensor(1.1015)\n",
            "=========\n",
            "epoch 85 train loss: 1.0668 val_cls: 1.1008 val_acc: 0.5500\n",
            "tensor(1.1008) tensor(1.1011)\n",
            "=========\n",
            "epoch 86 train loss: 1.0662 val_cls: 1.1005 val_acc: 0.5500\n",
            "tensor(1.1005) tensor(1.1008)\n",
            "=========\n",
            "epoch 87 train loss: 1.0657 val_cls: 1.1001 val_acc: 0.5510\n",
            "tensor(1.1001) tensor(1.1005)\n",
            "=========\n",
            "epoch 88 train loss: 1.0651 val_cls: 1.0997 val_acc: 0.5459\n",
            "tensor(1.0997) tensor(1.1001)\n",
            "=========\n",
            "epoch 89 train loss: 1.0645 val_cls: 1.0994 val_acc: 0.5449\n",
            "tensor(1.0994) tensor(1.0997)\n",
            "=========\n",
            "epoch 90 train loss: 1.0640 val_cls: 1.0990 val_acc: 0.5459\n",
            "tensor(1.0990) tensor(1.0994)\n",
            "=========\n",
            "epoch 91 train loss: 1.0634 val_cls: 1.0986 val_acc: 0.5500\n",
            "tensor(1.0986) tensor(1.0990)\n",
            "=========\n",
            "epoch 92 train loss: 1.0628 val_cls: 1.0983 val_acc: 0.5469\n",
            "tensor(1.0983) tensor(1.0986)\n",
            "=========\n",
            "epoch 93 train loss: 1.0622 val_cls: 1.0981 val_acc: 0.5510\n",
            "tensor(1.0981) tensor(1.0983)\n",
            "=========\n",
            "epoch 94 train loss: 1.0617 val_cls: 1.0977 val_acc: 0.5459\n",
            "tensor(1.0977) tensor(1.0981)\n",
            "=========\n",
            "epoch 95 train loss: 1.0612 val_cls: 1.0973 val_acc: 0.5469\n",
            "tensor(1.0973) tensor(1.0977)\n",
            "=========\n",
            "epoch 96 train loss: 1.0606 val_cls: 1.0970 val_acc: 0.5500\n",
            "tensor(1.0970) tensor(1.0973)\n",
            "=========\n",
            "epoch 97 train loss: 1.0602 val_cls: 1.0967 val_acc: 0.5480\n",
            "tensor(1.0967) tensor(1.0970)\n",
            "=========\n",
            "epoch 98 train loss: 1.0597 val_cls: 1.0965 val_acc: 0.5490\n",
            "tensor(1.0965) tensor(1.0967)\n",
            "=========\n",
            "epoch 99 train loss: 1.0591 val_cls: 1.0962 val_acc: 0.5490\n",
            "tensor(1.0962) tensor(1.0965)\n",
            "=========\n",
            "epoch 100 train loss: 1.0587 val_cls: 1.0960 val_acc: 0.5480\n",
            "tensor(1.0960) tensor(1.0962)\n",
            "=========\n",
            "epoch 101 train loss: 1.0581 val_cls: 1.0958 val_acc: 0.5500\n",
            "tensor(1.0958) tensor(1.0960)\n",
            "=========\n",
            "epoch 102 train loss: 1.0577 val_cls: 1.0955 val_acc: 0.5480\n",
            "tensor(1.0955) tensor(1.0958)\n",
            "=========\n",
            "epoch 103 train loss: 1.0572 val_cls: 1.0952 val_acc: 0.5469\n",
            "tensor(1.0952) tensor(1.0955)\n",
            "=========\n",
            "epoch 104 train loss: 1.0566 val_cls: 1.0949 val_acc: 0.5469\n",
            "tensor(1.0949) tensor(1.0952)\n",
            "=========\n",
            "epoch 105 train loss: 1.0561 val_cls: 1.0947 val_acc: 0.5510\n",
            "tensor(1.0947) tensor(1.0949)\n",
            "=========\n",
            "epoch 106 train loss: 1.0557 val_cls: 1.0945 val_acc: 0.5459\n",
            "tensor(1.0945) tensor(1.0947)\n",
            "=========\n",
            "epoch 107 train loss: 1.0551 val_cls: 1.0941 val_acc: 0.5480\n",
            "tensor(1.0941) tensor(1.0945)\n",
            "=========\n",
            "epoch 108 train loss: 1.0547 val_cls: 1.0939 val_acc: 0.5490\n",
            "tensor(1.0939) tensor(1.0941)\n",
            "=========\n",
            "epoch 109 train loss: 1.0543 val_cls: 1.0937 val_acc: 0.5480\n",
            "tensor(1.0937) tensor(1.0939)\n",
            "=========\n",
            "epoch 110 train loss: 1.0537 val_cls: 1.0935 val_acc: 0.5500\n",
            "tensor(1.0935) tensor(1.0937)\n",
            "=========\n",
            "epoch 111 train loss: 1.0532 val_cls: 1.0933 val_acc: 0.5510\n",
            "tensor(1.0933) tensor(1.0935)\n",
            "=========\n",
            "epoch 112 train loss: 1.0528 val_cls: 1.0931 val_acc: 0.5459\n",
            "tensor(1.0931) tensor(1.0933)\n",
            "=========\n",
            "epoch 113 train loss: 1.0524 val_cls: 1.0928 val_acc: 0.5500\n",
            "tensor(1.0928) tensor(1.0931)\n",
            "=========\n",
            "epoch 114 train loss: 1.0519 val_cls: 1.0925 val_acc: 0.5520\n",
            "tensor(1.0925) tensor(1.0928)\n",
            "=========\n",
            "epoch 115 train loss: 1.0515 val_cls: 1.0925 val_acc: 0.5520\n",
            "tensor(1.0925) tensor(1.0925)\n",
            "=========\n",
            "epoch 116 train loss: 1.0511 val_cls: 1.0924 val_acc: 0.5500\n",
            "tensor(1.0924) tensor(1.0925)\n",
            "=========\n",
            "epoch 117 train loss: 1.0507 val_cls: 1.0921 val_acc: 0.5510\n",
            "tensor(1.0921) tensor(1.0924)\n",
            "=========\n",
            "epoch 118 train loss: 1.0502 val_cls: 1.0917 val_acc: 0.5551\n",
            "tensor(1.0917) tensor(1.0921)\n",
            "=========\n",
            "epoch 119 train loss: 1.0497 val_cls: 1.0915 val_acc: 0.5531\n",
            "tensor(1.0915) tensor(1.0917)\n",
            "=========\n",
            "epoch 120 train loss: 1.0493 val_cls: 1.0914 val_acc: 0.5500\n",
            "tensor(1.0914) tensor(1.0915)\n",
            "=========\n",
            "epoch 121 train loss: 1.0488 val_cls: 1.0912 val_acc: 0.5551\n",
            "tensor(1.0912) tensor(1.0914)\n",
            "=========\n",
            "epoch 122 train loss: 1.0485 val_cls: 1.0910 val_acc: 0.5571\n",
            "tensor(1.0910) tensor(1.0912)\n",
            "=========\n",
            "epoch 123 train loss: 1.0481 val_cls: 1.0908 val_acc: 0.5551\n",
            "tensor(1.0908) tensor(1.0910)\n",
            "=========\n",
            "epoch 124 train loss: 1.0477 val_cls: 1.0905 val_acc: 0.5531\n",
            "tensor(1.0905) tensor(1.0908)\n",
            "=========\n",
            "epoch 125 train loss: 1.0471 val_cls: 1.0903 val_acc: 0.5541\n",
            "tensor(1.0903) tensor(1.0905)\n",
            "=========\n",
            "epoch 126 train loss: 1.0468 val_cls: 1.0900 val_acc: 0.5541\n",
            "tensor(1.0900) tensor(1.0903)\n",
            "=========\n",
            "epoch 127 train loss: 1.0463 val_cls: 1.0899 val_acc: 0.5551\n",
            "tensor(1.0899) tensor(1.0900)\n",
            "=========\n",
            "epoch 128 train loss: 1.0460 val_cls: 1.0896 val_acc: 0.5531\n",
            "tensor(1.0896) tensor(1.0899)\n",
            "=========\n",
            "epoch 129 train loss: 1.0455 val_cls: 1.0895 val_acc: 0.5520\n",
            "tensor(1.0895) tensor(1.0896)\n",
            "=========\n",
            "epoch 130 train loss: 1.0452 val_cls: 1.0892 val_acc: 0.5520\n",
            "tensor(1.0892) tensor(1.0895)\n",
            "=========\n",
            "epoch 131 train loss: 1.0447 val_cls: 1.0890 val_acc: 0.5520\n",
            "tensor(1.0890) tensor(1.0892)\n",
            "=========\n",
            "epoch 132 train loss: 1.0443 val_cls: 1.0889 val_acc: 0.5571\n",
            "tensor(1.0889) tensor(1.0890)\n",
            "=========\n",
            "epoch 133 train loss: 1.0441 val_cls: 1.0886 val_acc: 0.5561\n",
            "tensor(1.0886) tensor(1.0889)\n",
            "=========\n",
            "epoch 134 train loss: 1.0436 val_cls: 1.0884 val_acc: 0.5520\n",
            "tensor(1.0884) tensor(1.0886)\n",
            "=========\n",
            "epoch 135 train loss: 1.0431 val_cls: 1.0883 val_acc: 0.5510\n",
            "tensor(1.0883) tensor(1.0884)\n",
            "=========\n",
            "epoch 136 train loss: 1.0429 val_cls: 1.0881 val_acc: 0.5510\n",
            "tensor(1.0881) tensor(1.0883)\n",
            "=========\n",
            "epoch 137 train loss: 1.0423 val_cls: 1.0878 val_acc: 0.5551\n",
            "tensor(1.0878) tensor(1.0881)\n",
            "=========\n",
            "epoch 138 train loss: 1.0420 val_cls: 1.0877 val_acc: 0.5551\n",
            "tensor(1.0877) tensor(1.0878)\n",
            "=========\n",
            "epoch 139 train loss: 1.0415 val_cls: 1.0875 val_acc: 0.5592\n",
            "tensor(1.0875) tensor(1.0877)\n",
            "=========\n",
            "epoch 140 train loss: 1.0414 val_cls: 1.0873 val_acc: 0.5551\n",
            "tensor(1.0873) tensor(1.0875)\n",
            "=========\n",
            "epoch 141 train loss: 1.0410 val_cls: 1.0871 val_acc: 0.5531\n",
            "tensor(1.0871) tensor(1.0873)\n",
            "=========\n",
            "epoch 142 train loss: 1.0405 val_cls: 1.0871 val_acc: 0.5520\n",
            "tensor(1.0871) tensor(1.0871)\n",
            "=========\n",
            "epoch 143 train loss: 1.0401 val_cls: 1.0869 val_acc: 0.5551\n",
            "tensor(1.0869) tensor(1.0871)\n",
            "=========\n",
            "epoch 144 train loss: 1.0398 val_cls: 1.0867 val_acc: 0.5582\n",
            "tensor(1.0867) tensor(1.0869)\n",
            "=========\n",
            "epoch 145 train loss: 1.0394 val_cls: 1.0865 val_acc: 0.5592\n",
            "tensor(1.0865) tensor(1.0867)\n",
            "=========\n",
            "epoch 146 train loss: 1.0389 val_cls: 1.0863 val_acc: 0.5592\n",
            "tensor(1.0863) tensor(1.0865)\n",
            "=========\n",
            "epoch 147 train loss: 1.0385 val_cls: 1.0862 val_acc: 0.5582\n",
            "tensor(1.0862) tensor(1.0863)\n",
            "=========\n",
            "epoch 148 train loss: 1.0382 val_cls: 1.0861 val_acc: 0.5541\n",
            "tensor(1.0861) tensor(1.0862)\n",
            "=========\n",
            "epoch 149 train loss: 1.0378 val_cls: 1.0861 val_acc: 0.5541\n",
            "tensor(1.0861) tensor(1.0861)\n",
            "=========\n",
            "epoch 150 train loss: 1.0375 val_cls: 1.0859 val_acc: 0.5551\n",
            "tensor(1.0859) tensor(1.0861)\n",
            "=========\n",
            "epoch 151 train loss: 1.0371 val_cls: 1.0858 val_acc: 0.5561\n",
            "tensor(1.0858) tensor(1.0859)\n",
            "=========\n",
            "epoch 152 train loss: 1.0368 val_cls: 1.0855 val_acc: 0.5571\n",
            "tensor(1.0855) tensor(1.0858)\n",
            "=========\n",
            "epoch 153 train loss: 1.0364 val_cls: 1.0852 val_acc: 0.5582\n",
            "tensor(1.0852) tensor(1.0855)\n",
            "=========\n",
            "epoch 154 train loss: 1.0361 val_cls: 1.0851 val_acc: 0.5612\n",
            "tensor(1.0851) tensor(1.0852)\n",
            "=========\n",
            "epoch 155 train loss: 1.0356 val_cls: 1.0850 val_acc: 0.5612\n",
            "tensor(1.0850) tensor(1.0851)\n",
            "=========\n",
            "epoch 156 train loss: 1.0351 val_cls: 1.0849 val_acc: 0.5612\n",
            "tensor(1.0849) tensor(1.0850)\n",
            "=========\n",
            "epoch 157 train loss: 1.0349 val_cls: 1.0848 val_acc: 0.5582\n",
            "tensor(1.0848) tensor(1.0849)\n",
            "=========\n",
            "epoch 158 train loss: 1.0346 val_cls: 1.0846 val_acc: 0.5612\n",
            "tensor(1.0846) tensor(1.0848)\n",
            "=========\n",
            "epoch 159 train loss: 1.0342 val_cls: 1.0846 val_acc: 0.5633\n",
            "tensor(1.0846) tensor(1.0846)\n",
            "=========\n",
            "epoch 160 train loss: 1.0339 val_cls: 1.0845 val_acc: 0.5592\n",
            "tensor(1.0845) tensor(1.0846)\n",
            "=========\n",
            "epoch 161 train loss: 1.0336 val_cls: 1.0843 val_acc: 0.5602\n",
            "tensor(1.0843) tensor(1.0845)\n",
            "=========\n",
            "epoch 162 train loss: 1.0332 val_cls: 1.0841 val_acc: 0.5612\n",
            "tensor(1.0841) tensor(1.0843)\n",
            "=========\n",
            "epoch 163 train loss: 1.0327 val_cls: 1.0840 val_acc: 0.5612\n",
            "tensor(1.0840) tensor(1.0841)\n",
            "=========\n",
            "epoch 164 train loss: 1.0326 val_cls: 1.0839 val_acc: 0.5602\n",
            "tensor(1.0839) tensor(1.0840)\n",
            "=========\n",
            "epoch 165 train loss: 1.0323 val_cls: 1.0837 val_acc: 0.5633\n",
            "tensor(1.0837) tensor(1.0839)\n",
            "=========\n",
            "epoch 166 train loss: 1.0318 val_cls: 1.0834 val_acc: 0.5643\n",
            "tensor(1.0834) tensor(1.0837)\n",
            "=========\n",
            "epoch 167 train loss: 1.0314 val_cls: 1.0833 val_acc: 0.5633\n",
            "tensor(1.0833) tensor(1.0834)\n",
            "=========\n",
            "epoch 168 train loss: 1.0311 val_cls: 1.0832 val_acc: 0.5633\n",
            "tensor(1.0832) tensor(1.0833)\n",
            "=========\n",
            "epoch 169 train loss: 1.0308 val_cls: 1.0831 val_acc: 0.5653\n",
            "tensor(1.0831) tensor(1.0832)\n",
            "=========\n",
            "epoch 170 train loss: 1.0305 val_cls: 1.0829 val_acc: 0.5633\n",
            "tensor(1.0829) tensor(1.0831)\n",
            "=========\n",
            "epoch 171 train loss: 1.0301 val_cls: 1.0828 val_acc: 0.5653\n",
            "tensor(1.0828) tensor(1.0829)\n",
            "=========\n",
            "epoch 172 train loss: 1.0296 val_cls: 1.0827 val_acc: 0.5653\n",
            "tensor(1.0827) tensor(1.0828)\n",
            "=========\n",
            "epoch 173 train loss: 1.0294 val_cls: 1.0826 val_acc: 0.5643\n",
            "tensor(1.0826) tensor(1.0827)\n",
            "=========\n",
            "epoch 174 train loss: 1.0292 val_cls: 1.0825 val_acc: 0.5602\n",
            "tensor(1.0825) tensor(1.0826)\n",
            "=========\n",
            "epoch 175 train loss: 1.0287 val_cls: 1.0822 val_acc: 0.5633\n",
            "tensor(1.0822) tensor(1.0825)\n",
            "=========\n",
            "epoch 176 train loss: 1.0285 val_cls: 1.0821 val_acc: 0.5643\n",
            "tensor(1.0821) tensor(1.0822)\n",
            "=========\n",
            "epoch 177 train loss: 1.0280 val_cls: 1.0820 val_acc: 0.5663\n",
            "tensor(1.0820) tensor(1.0821)\n",
            "=========\n",
            "epoch 178 train loss: 1.0279 val_cls: 1.0819 val_acc: 0.5673\n",
            "tensor(1.0819) tensor(1.0820)\n",
            "=========\n",
            "epoch 179 train loss: 1.0275 val_cls: 1.0818 val_acc: 0.5653\n",
            "tensor(1.0818) tensor(1.0819)\n",
            "=========\n",
            "epoch 180 train loss: 1.0272 val_cls: 1.0816 val_acc: 0.5653\n",
            "tensor(1.0816) tensor(1.0818)\n",
            "=========\n",
            "epoch 181 train loss: 1.0270 val_cls: 1.0815 val_acc: 0.5653\n",
            "tensor(1.0815) tensor(1.0816)\n",
            "=========\n",
            "epoch 182 train loss: 1.0266 val_cls: 1.0814 val_acc: 0.5653\n",
            "tensor(1.0814) tensor(1.0815)\n",
            "=========\n",
            "epoch 183 train loss: 1.0262 val_cls: 1.0811 val_acc: 0.5673\n",
            "tensor(1.0811) tensor(1.0814)\n",
            "=========\n",
            "epoch 184 train loss: 1.0258 val_cls: 1.0810 val_acc: 0.5633\n",
            "tensor(1.0810) tensor(1.0811)\n",
            "=========\n",
            "epoch 185 train loss: 1.0255 val_cls: 1.0809 val_acc: 0.5633\n",
            "tensor(1.0809) tensor(1.0810)\n",
            "=========\n",
            "epoch 186 train loss: 1.0253 val_cls: 1.0807 val_acc: 0.5663\n",
            "tensor(1.0807) tensor(1.0809)\n",
            "=========\n",
            "epoch 187 train loss: 1.0248 val_cls: 1.0806 val_acc: 0.5622\n",
            "tensor(1.0806) tensor(1.0807)\n",
            "=========\n",
            "epoch 188 train loss: 1.0248 val_cls: 1.0804 val_acc: 0.5673\n",
            "tensor(1.0804) tensor(1.0806)\n",
            "=========\n",
            "epoch 189 train loss: 1.0244 val_cls: 1.0802 val_acc: 0.5643\n",
            "tensor(1.0802) tensor(1.0804)\n",
            "=========\n",
            "epoch 190 train loss: 1.0240 val_cls: 1.0801 val_acc: 0.5663\n",
            "tensor(1.0801) tensor(1.0802)\n",
            "=========\n",
            "epoch 191 train loss: 1.0238 val_cls: 1.0799 val_acc: 0.5684\n",
            "tensor(1.0799) tensor(1.0801)\n",
            "=========\n",
            "epoch 192 train loss: 1.0235 val_cls: 1.0797 val_acc: 0.5673\n",
            "tensor(1.0797) tensor(1.0799)\n",
            "=========\n",
            "epoch 193 train loss: 1.0232 val_cls: 1.0797 val_acc: 0.5673\n",
            "tensor(1.0797) tensor(1.0797)\n",
            "=========\n",
            "epoch 194 train loss: 1.0230 val_cls: 1.0795 val_acc: 0.5684\n",
            "tensor(1.0795) tensor(1.0797)\n",
            "=========\n",
            "epoch 195 train loss: 1.0225 val_cls: 1.0795 val_acc: 0.5653\n",
            "tensor(1.0795) tensor(1.0795)\n",
            "=========\n",
            "epoch 196 train loss: 1.0224 val_cls: 1.0794 val_acc: 0.5714\n",
            "tensor(1.0794) tensor(1.0795)\n",
            "=========\n",
            "epoch 197 train loss: 1.0220 val_cls: 1.0793 val_acc: 0.5714\n",
            "tensor(1.0793) tensor(1.0794)\n",
            "=========\n",
            "epoch 198 train loss: 1.0218 val_cls: 1.0790 val_acc: 0.5714\n",
            "tensor(1.0790) tensor(1.0793)\n",
            "=========\n",
            "epoch 199 train loss: 1.0214 val_cls: 1.0789 val_acc: 0.5673\n",
            "tensor(1.0789) tensor(1.0790)\n",
            "=========\n",
            "epoch 200 train loss: 1.0211 val_cls: 1.0788 val_acc: 0.5684\n",
            "tensor(1.0788) tensor(1.0789)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history['val_cls'],  label='val')\n",
        "plt.plot(history['train_loss'], label='train')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "HCiOfoYfrHa_",
        "outputId": "9465818c-8026-492a-c88e-42ea514425c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ikh_tmzAkT80",
        "outputId": "beedf46e-c8ab-4dc1-8bdf-4138d4d2b1e8"
      },
      "source": [
        "\n",
        "class_predictions = utils.test(classification_model, c_test_iter)\n",
        "class_predictions = np.argmax(class_predictions, axis=1)\n",
        "\n",
        "print(\"Accuracy: \",  accuracy_score(y_true=y_test, y_pred=class_predictions),\n",
        "      \", MSE: \",mse(y_true=y_test, y_pred=class_predictions),\n",
        "      \", MAE\", mae(y_true=y_test, y_pred=class_predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.5510204081632653 , MSE:  0.6183673469387755 , MAE 0.5040816326530613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVacqcxW9ClC"
      },
      "source": [
        "**Task 8.** Add an additional dense layer (a second hidden layer) with the ReLU activation and input/output 30 neurons. Is the result better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaSkyNu1mtIc",
        "outputId": "ee79acc3-e231-4f3a-8dc5-afaa7fb1a62e"
      },
      "source": [
        "class ClassModel3(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ClassModel3, self).__init__()\n",
        "    self.layer =nn.Sequential(nn.Linear(in_features=11, out_features=30),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(in_features=30, out_features=30),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(in_features=30, out_features=10))\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "classification_model = ClassModel3()\n",
        "classification_model = classification_model.cuda()\n",
        "\n",
        "history = utils.train(model=classification_model,\n",
        "                            loss=nn.CrossEntropyLoss(),\n",
        "                            val_metrics={\"cls\": nn.CrossEntropyLoss(), \"acc\": val_acc},\n",
        "                            optimizer=torch.optim.SGD(classification_model.parameters(), lr=0.01),\n",
        "                            train_ds=c_train_iter,\n",
        "                            dev_ds=c_dev_iter,\n",
        "                            num_epochs=200,\n",
        "                            early_stopper=utils.EarlyStopper(metric_name=\"cls\", patience=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========\n",
            "epoch 1 train loss: 2.1905 val_cls: 2.0965 val_acc: 0.4490\n",
            "tensor(2.0965) None\n",
            "=========\n",
            "epoch 2 train loss: 2.0149 val_cls: 1.9257 val_acc: 0.4490\n",
            "tensor(1.9257) tensor(2.0965)\n",
            "=========\n",
            "epoch 3 train loss: 1.8491 val_cls: 1.7662 val_acc: 0.4490\n",
            "tensor(1.7662) tensor(1.9257)\n",
            "=========\n",
            "epoch 4 train loss: 1.7001 val_cls: 1.6321 val_acc: 0.4490\n",
            "tensor(1.6321) tensor(1.7662)\n",
            "=========\n",
            "epoch 5 train loss: 1.5820 val_cls: 1.5337 val_acc: 0.4490\n",
            "tensor(1.5337) tensor(1.6321)\n",
            "=========\n",
            "epoch 6 train loss: 1.4976 val_cls: 1.4653 val_acc: 0.4490\n",
            "tensor(1.4653) tensor(1.5337)\n",
            "=========\n",
            "epoch 7 train loss: 1.4383 val_cls: 1.4175 val_acc: 0.4490\n",
            "tensor(1.4175) tensor(1.4653)\n",
            "=========\n",
            "epoch 8 train loss: 1.3964 val_cls: 1.3835 val_acc: 0.4490\n",
            "tensor(1.3835) tensor(1.4175)\n",
            "=========\n",
            "epoch 9 train loss: 1.3662 val_cls: 1.3588 val_acc: 0.4510\n",
            "tensor(1.3588) tensor(1.3835)\n",
            "=========\n",
            "epoch 10 train loss: 1.3435 val_cls: 1.3400 val_acc: 0.4582\n",
            "tensor(1.3400) tensor(1.3588)\n",
            "=========\n",
            "epoch 11 train loss: 1.3258 val_cls: 1.3251 val_acc: 0.4663\n",
            "tensor(1.3251) tensor(1.3400)\n",
            "=========\n",
            "epoch 12 train loss: 1.3113 val_cls: 1.3128 val_acc: 0.4673\n",
            "tensor(1.3128) tensor(1.3251)\n",
            "=========\n",
            "epoch 13 train loss: 1.2991 val_cls: 1.3023 val_acc: 0.4633\n",
            "tensor(1.3023) tensor(1.3128)\n",
            "=========\n",
            "epoch 14 train loss: 1.2881 val_cls: 1.2930 val_acc: 0.4704\n",
            "tensor(1.2930) tensor(1.3023)\n",
            "=========\n",
            "epoch 15 train loss: 1.2784 val_cls: 1.2843 val_acc: 0.4684\n",
            "tensor(1.2843) tensor(1.2930)\n",
            "=========\n",
            "epoch 16 train loss: 1.2693 val_cls: 1.2763 val_acc: 0.4714\n",
            "tensor(1.2763) tensor(1.2843)\n",
            "=========\n",
            "epoch 17 train loss: 1.2606 val_cls: 1.2688 val_acc: 0.4776\n",
            "tensor(1.2688) tensor(1.2763)\n",
            "=========\n",
            "epoch 18 train loss: 1.2526 val_cls: 1.2615 val_acc: 0.4735\n",
            "tensor(1.2615) tensor(1.2688)\n",
            "=========\n",
            "epoch 19 train loss: 1.2445 val_cls: 1.2545 val_acc: 0.4796\n",
            "tensor(1.2545) tensor(1.2615)\n",
            "=========\n",
            "epoch 20 train loss: 1.2370 val_cls: 1.2479 val_acc: 0.4816\n",
            "tensor(1.2479) tensor(1.2545)\n",
            "=========\n",
            "epoch 21 train loss: 1.2298 val_cls: 1.2415 val_acc: 0.4827\n",
            "tensor(1.2415) tensor(1.2479)\n",
            "=========\n",
            "epoch 22 train loss: 1.2226 val_cls: 1.2352 val_acc: 0.4806\n",
            "tensor(1.2352) tensor(1.2415)\n",
            "=========\n",
            "epoch 23 train loss: 1.2158 val_cls: 1.2291 val_acc: 0.4816\n",
            "tensor(1.2291) tensor(1.2352)\n",
            "=========\n",
            "epoch 24 train loss: 1.2092 val_cls: 1.2236 val_acc: 0.4847\n",
            "tensor(1.2236) tensor(1.2291)\n",
            "=========\n",
            "epoch 25 train loss: 1.2030 val_cls: 1.2177 val_acc: 0.4847\n",
            "tensor(1.2177) tensor(1.2236)\n",
            "=========\n",
            "epoch 26 train loss: 1.1969 val_cls: 1.2123 val_acc: 0.4857\n",
            "tensor(1.2123) tensor(1.2177)\n",
            "=========\n",
            "epoch 27 train loss: 1.1910 val_cls: 1.2071 val_acc: 0.4918\n",
            "tensor(1.2071) tensor(1.2123)\n",
            "=========\n",
            "epoch 28 train loss: 1.1855 val_cls: 1.2020 val_acc: 0.4990\n",
            "tensor(1.2020) tensor(1.2071)\n",
            "=========\n",
            "epoch 29 train loss: 1.1800 val_cls: 1.1973 val_acc: 0.5020\n",
            "tensor(1.1973) tensor(1.2020)\n",
            "=========\n",
            "epoch 30 train loss: 1.1749 val_cls: 1.1925 val_acc: 0.4990\n",
            "tensor(1.1925) tensor(1.1973)\n",
            "=========\n",
            "epoch 31 train loss: 1.1697 val_cls: 1.1881 val_acc: 0.5041\n",
            "tensor(1.1881) tensor(1.1925)\n",
            "=========\n",
            "epoch 32 train loss: 1.1651 val_cls: 1.1838 val_acc: 0.5112\n",
            "tensor(1.1838) tensor(1.1881)\n",
            "=========\n",
            "epoch 33 train loss: 1.1606 val_cls: 1.1797 val_acc: 0.5061\n",
            "tensor(1.1797) tensor(1.1838)\n",
            "=========\n",
            "epoch 34 train loss: 1.1563 val_cls: 1.1757 val_acc: 0.5122\n",
            "tensor(1.1757) tensor(1.1797)\n",
            "=========\n",
            "epoch 35 train loss: 1.1524 val_cls: 1.1719 val_acc: 0.5112\n",
            "tensor(1.1719) tensor(1.1757)\n",
            "=========\n",
            "epoch 36 train loss: 1.1480 val_cls: 1.1683 val_acc: 0.5133\n",
            "tensor(1.1683) tensor(1.1719)\n",
            "=========\n",
            "epoch 37 train loss: 1.1445 val_cls: 1.1648 val_acc: 0.5173\n",
            "tensor(1.1648) tensor(1.1683)\n",
            "=========\n",
            "epoch 38 train loss: 1.1411 val_cls: 1.1614 val_acc: 0.5173\n",
            "tensor(1.1614) tensor(1.1648)\n",
            "=========\n",
            "epoch 39 train loss: 1.1378 val_cls: 1.1581 val_acc: 0.5133\n",
            "tensor(1.1581) tensor(1.1614)\n",
            "=========\n",
            "epoch 40 train loss: 1.1343 val_cls: 1.1552 val_acc: 0.5173\n",
            "tensor(1.1552) tensor(1.1581)\n",
            "=========\n",
            "epoch 41 train loss: 1.1313 val_cls: 1.1524 val_acc: 0.5184\n",
            "tensor(1.1524) tensor(1.1552)\n",
            "=========\n",
            "epoch 42 train loss: 1.1284 val_cls: 1.1495 val_acc: 0.5204\n",
            "tensor(1.1495) tensor(1.1524)\n",
            "=========\n",
            "epoch 43 train loss: 1.1254 val_cls: 1.1470 val_acc: 0.5163\n",
            "tensor(1.1470) tensor(1.1495)\n",
            "=========\n",
            "epoch 44 train loss: 1.1228 val_cls: 1.1446 val_acc: 0.5265\n",
            "tensor(1.1446) tensor(1.1470)\n",
            "=========\n",
            "epoch 45 train loss: 1.1203 val_cls: 1.1418 val_acc: 0.5245\n",
            "tensor(1.1418) tensor(1.1446)\n",
            "=========\n",
            "epoch 46 train loss: 1.1176 val_cls: 1.1397 val_acc: 0.5255\n",
            "tensor(1.1397) tensor(1.1418)\n",
            "=========\n",
            "epoch 47 train loss: 1.1154 val_cls: 1.1377 val_acc: 0.5214\n",
            "tensor(1.1377) tensor(1.1397)\n",
            "=========\n",
            "epoch 48 train loss: 1.1133 val_cls: 1.1351 val_acc: 0.5214\n",
            "tensor(1.1351) tensor(1.1377)\n",
            "=========\n",
            "epoch 49 train loss: 1.1111 val_cls: 1.1334 val_acc: 0.5235\n",
            "tensor(1.1334) tensor(1.1351)\n",
            "=========\n",
            "epoch 50 train loss: 1.1091 val_cls: 1.1312 val_acc: 0.5265\n",
            "tensor(1.1312) tensor(1.1334)\n",
            "=========\n",
            "epoch 51 train loss: 1.1068 val_cls: 1.1296 val_acc: 0.5276\n",
            "tensor(1.1296) tensor(1.1312)\n",
            "=========\n",
            "epoch 52 train loss: 1.1052 val_cls: 1.1275 val_acc: 0.5276\n",
            "tensor(1.1275) tensor(1.1296)\n",
            "=========\n",
            "epoch 53 train loss: 1.1031 val_cls: 1.1260 val_acc: 0.5296\n",
            "tensor(1.1260) tensor(1.1275)\n",
            "=========\n",
            "epoch 54 train loss: 1.1013 val_cls: 1.1242 val_acc: 0.5327\n",
            "tensor(1.1242) tensor(1.1260)\n",
            "=========\n",
            "epoch 55 train loss: 1.0995 val_cls: 1.1224 val_acc: 0.5337\n",
            "tensor(1.1224) tensor(1.1242)\n",
            "=========\n",
            "epoch 56 train loss: 1.0978 val_cls: 1.1210 val_acc: 0.5367\n",
            "tensor(1.1210) tensor(1.1224)\n",
            "=========\n",
            "epoch 57 train loss: 1.0959 val_cls: 1.1194 val_acc: 0.5367\n",
            "tensor(1.1194) tensor(1.1210)\n",
            "=========\n",
            "epoch 58 train loss: 1.0948 val_cls: 1.1180 val_acc: 0.5378\n",
            "tensor(1.1180) tensor(1.1194)\n",
            "=========\n",
            "epoch 59 train loss: 1.0933 val_cls: 1.1168 val_acc: 0.5378\n",
            "tensor(1.1168) tensor(1.1180)\n",
            "=========\n",
            "epoch 60 train loss: 1.0915 val_cls: 1.1153 val_acc: 0.5378\n",
            "tensor(1.1153) tensor(1.1168)\n",
            "=========\n",
            "epoch 61 train loss: 1.0899 val_cls: 1.1143 val_acc: 0.5378\n",
            "tensor(1.1143) tensor(1.1153)\n",
            "=========\n",
            "epoch 62 train loss: 1.0885 val_cls: 1.1133 val_acc: 0.5388\n",
            "tensor(1.1133) tensor(1.1143)\n",
            "=========\n",
            "epoch 63 train loss: 1.0875 val_cls: 1.1118 val_acc: 0.5408\n",
            "tensor(1.1118) tensor(1.1133)\n",
            "=========\n",
            "epoch 64 train loss: 1.0859 val_cls: 1.1107 val_acc: 0.5418\n",
            "tensor(1.1107) tensor(1.1118)\n",
            "=========\n",
            "epoch 65 train loss: 1.0847 val_cls: 1.1098 val_acc: 0.5398\n",
            "tensor(1.1098) tensor(1.1107)\n",
            "=========\n",
            "epoch 66 train loss: 1.0834 val_cls: 1.1092 val_acc: 0.5408\n",
            "tensor(1.1092) tensor(1.1098)\n",
            "=========\n",
            "epoch 67 train loss: 1.0824 val_cls: 1.1079 val_acc: 0.5398\n",
            "tensor(1.1079) tensor(1.1092)\n",
            "=========\n",
            "epoch 68 train loss: 1.0811 val_cls: 1.1070 val_acc: 0.5378\n",
            "tensor(1.1070) tensor(1.1079)\n",
            "=========\n",
            "epoch 69 train loss: 1.0799 val_cls: 1.1062 val_acc: 0.5367\n",
            "tensor(1.1062) tensor(1.1070)\n",
            "=========\n",
            "epoch 70 train loss: 1.0786 val_cls: 1.1054 val_acc: 0.5388\n",
            "tensor(1.1054) tensor(1.1062)\n",
            "=========\n",
            "epoch 71 train loss: 1.0775 val_cls: 1.1043 val_acc: 0.5408\n",
            "tensor(1.1043) tensor(1.1054)\n",
            "=========\n",
            "epoch 72 train loss: 1.0767 val_cls: 1.1033 val_acc: 0.5388\n",
            "tensor(1.1033) tensor(1.1043)\n",
            "=========\n",
            "epoch 73 train loss: 1.0755 val_cls: 1.1026 val_acc: 0.5408\n",
            "tensor(1.1026) tensor(1.1033)\n",
            "=========\n",
            "epoch 74 train loss: 1.0744 val_cls: 1.1022 val_acc: 0.5388\n",
            "tensor(1.1022) tensor(1.1026)\n",
            "=========\n",
            "epoch 75 train loss: 1.0735 val_cls: 1.1013 val_acc: 0.5378\n",
            "tensor(1.1013) tensor(1.1022)\n",
            "=========\n",
            "epoch 76 train loss: 1.0723 val_cls: 1.1010 val_acc: 0.5429\n",
            "tensor(1.1010) tensor(1.1013)\n",
            "=========\n",
            "epoch 77 train loss: 1.0715 val_cls: 1.0999 val_acc: 0.5490\n",
            "tensor(1.0999) tensor(1.1010)\n",
            "=========\n",
            "epoch 78 train loss: 1.0706 val_cls: 1.0991 val_acc: 0.5439\n",
            "tensor(1.0991) tensor(1.0999)\n",
            "=========\n",
            "epoch 79 train loss: 1.0698 val_cls: 1.0986 val_acc: 0.5388\n",
            "tensor(1.0986) tensor(1.0991)\n",
            "=========\n",
            "epoch 80 train loss: 1.0692 val_cls: 1.0980 val_acc: 0.5449\n",
            "tensor(1.0980) tensor(1.0986)\n",
            "=========\n",
            "epoch 81 train loss: 1.0680 val_cls: 1.0972 val_acc: 0.5500\n",
            "tensor(1.0972) tensor(1.0980)\n",
            "=========\n",
            "epoch 82 train loss: 1.0667 val_cls: 1.0969 val_acc: 0.5490\n",
            "tensor(1.0969) tensor(1.0972)\n",
            "=========\n",
            "epoch 83 train loss: 1.0661 val_cls: 1.0962 val_acc: 0.5490\n",
            "tensor(1.0962) tensor(1.0969)\n",
            "=========\n",
            "epoch 84 train loss: 1.0654 val_cls: 1.0956 val_acc: 0.5500\n",
            "tensor(1.0956) tensor(1.0962)\n",
            "=========\n",
            "epoch 85 train loss: 1.0644 val_cls: 1.0950 val_acc: 0.5398\n",
            "tensor(1.0950) tensor(1.0956)\n",
            "=========\n",
            "epoch 86 train loss: 1.0638 val_cls: 1.0944 val_acc: 0.5490\n",
            "tensor(1.0944) tensor(1.0950)\n",
            "=========\n",
            "epoch 87 train loss: 1.0629 val_cls: 1.0941 val_acc: 0.5398\n",
            "tensor(1.0941) tensor(1.0944)\n",
            "=========\n",
            "epoch 88 train loss: 1.0624 val_cls: 1.0937 val_acc: 0.5439\n",
            "tensor(1.0937) tensor(1.0941)\n",
            "=========\n",
            "epoch 89 train loss: 1.0615 val_cls: 1.0932 val_acc: 0.5449\n",
            "tensor(1.0932) tensor(1.0937)\n",
            "=========\n",
            "epoch 90 train loss: 1.0606 val_cls: 1.0926 val_acc: 0.5510\n",
            "tensor(1.0926) tensor(1.0932)\n",
            "=========\n",
            "epoch 91 train loss: 1.0601 val_cls: 1.0920 val_acc: 0.5510\n",
            "tensor(1.0920) tensor(1.0926)\n",
            "=========\n",
            "epoch 92 train loss: 1.0593 val_cls: 1.0919 val_acc: 0.5490\n",
            "tensor(1.0919) tensor(1.0920)\n",
            "=========\n",
            "epoch 93 train loss: 1.0584 val_cls: 1.0912 val_acc: 0.5510\n",
            "tensor(1.0912) tensor(1.0919)\n",
            "=========\n",
            "epoch 94 train loss: 1.0580 val_cls: 1.0908 val_acc: 0.5480\n",
            "tensor(1.0908) tensor(1.0912)\n",
            "=========\n",
            "epoch 95 train loss: 1.0571 val_cls: 1.0905 val_acc: 0.5541\n",
            "tensor(1.0905) tensor(1.0908)\n",
            "=========\n",
            "epoch 96 train loss: 1.0566 val_cls: 1.0899 val_acc: 0.5500\n",
            "tensor(1.0899) tensor(1.0905)\n",
            "=========\n",
            "epoch 97 train loss: 1.0559 val_cls: 1.0895 val_acc: 0.5520\n",
            "tensor(1.0895) tensor(1.0899)\n",
            "=========\n",
            "epoch 98 train loss: 1.0554 val_cls: 1.0891 val_acc: 0.5510\n",
            "tensor(1.0891) tensor(1.0895)\n",
            "=========\n",
            "epoch 99 train loss: 1.0544 val_cls: 1.0887 val_acc: 0.5520\n",
            "tensor(1.0887) tensor(1.0891)\n",
            "=========\n",
            "epoch 100 train loss: 1.0541 val_cls: 1.0883 val_acc: 0.5531\n",
            "tensor(1.0883) tensor(1.0887)\n",
            "=========\n",
            "epoch 101 train loss: 1.0533 val_cls: 1.0880 val_acc: 0.5541\n",
            "tensor(1.0880) tensor(1.0883)\n",
            "=========\n",
            "epoch 102 train loss: 1.0530 val_cls: 1.0876 val_acc: 0.5520\n",
            "tensor(1.0876) tensor(1.0880)\n",
            "=========\n",
            "epoch 103 train loss: 1.0522 val_cls: 1.0873 val_acc: 0.5510\n",
            "tensor(1.0873) tensor(1.0876)\n",
            "=========\n",
            "epoch 104 train loss: 1.0514 val_cls: 1.0870 val_acc: 0.5541\n",
            "tensor(1.0870) tensor(1.0873)\n",
            "=========\n",
            "epoch 105 train loss: 1.0509 val_cls: 1.0866 val_acc: 0.5531\n",
            "tensor(1.0866) tensor(1.0870)\n",
            "=========\n",
            "epoch 106 train loss: 1.0504 val_cls: 1.0863 val_acc: 0.5520\n",
            "tensor(1.0863) tensor(1.0866)\n",
            "=========\n",
            "epoch 107 train loss: 1.0497 val_cls: 1.0862 val_acc: 0.5561\n",
            "tensor(1.0862) tensor(1.0863)\n",
            "=========\n",
            "epoch 108 train loss: 1.0493 val_cls: 1.0856 val_acc: 0.5531\n",
            "tensor(1.0856) tensor(1.0862)\n",
            "=========\n",
            "epoch 109 train loss: 1.0485 val_cls: 1.0855 val_acc: 0.5551\n",
            "tensor(1.0855) tensor(1.0856)\n",
            "=========\n",
            "epoch 110 train loss: 1.0480 val_cls: 1.0852 val_acc: 0.5459\n",
            "tensor(1.0852) tensor(1.0855)\n",
            "=========\n",
            "epoch 111 train loss: 1.0475 val_cls: 1.0851 val_acc: 0.5418\n",
            "tensor(1.0851) tensor(1.0852)\n",
            "=========\n",
            "epoch 112 train loss: 1.0470 val_cls: 1.0847 val_acc: 0.5500\n",
            "tensor(1.0847) tensor(1.0851)\n",
            "=========\n",
            "epoch 113 train loss: 1.0462 val_cls: 1.0844 val_acc: 0.5439\n",
            "tensor(1.0844) tensor(1.0847)\n",
            "=========\n",
            "epoch 114 train loss: 1.0459 val_cls: 1.0839 val_acc: 0.5561\n",
            "tensor(1.0839) tensor(1.0844)\n",
            "=========\n",
            "epoch 115 train loss: 1.0453 val_cls: 1.0835 val_acc: 0.5541\n",
            "tensor(1.0835) tensor(1.0839)\n",
            "=========\n",
            "epoch 116 train loss: 1.0445 val_cls: 1.0834 val_acc: 0.5531\n",
            "tensor(1.0834) tensor(1.0835)\n",
            "=========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history['val_cls'],  label='val')\n",
        "plt.plot(history['train_loss'], label='train')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xpEb2bsJrLa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-C1qzxF7tIQ"
      },
      "source": [
        "\n",
        "class_predictions = utils.test(classification_model, c_test_iter)\n",
        "class_predictions = np.argmax(class_predictions, axis=1)\n",
        "\n",
        "print(\"Accuracy: \",  accuracy_score(y_true=y_test, y_pred=class_predictions),\n",
        "      \", MSE: \",mse(y_true=y_test, y_pred=class_predictions),\n",
        "      \", MAE\", mae(y_true=y_test, y_pred=class_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "967Kh67vzi8K"
      },
      "source": [
        "**Task 9** Write a short report about your findings and answer the following questions:\n",
        "\n",
        "\n",
        "1. Is the wine problem a classification task (or a regression task)? Can be both, but the classification setting considers the importance of the mistaken classes to be the same. If we predict rating 1 instead of 10,it is equivalent to predicting rating 9. The regression setting would consider rating 9 to be much better.\n",
        "\n",
        "2. What is the most suitable metric for our task (MSE, MAE, accuracy)? Not accuracy ()\n",
        "3. Which model is the best performer overall?\n",
        "4. Is 'deeper' always better? No, we might get overfitting.\n",
        "5. Add other your findings you want to share.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTnvBW_MlSis"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}